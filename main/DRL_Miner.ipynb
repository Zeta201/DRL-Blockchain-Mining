{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-nWnnHFjJAZ",
        "outputId": "f87eb7e9-5c0b-4275-be15-7e679054dbf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchrl in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: torch>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from torchrl) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchrl) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from torchrl) (24.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from torchrl) (3.1.0)\n",
            "Requirement already satisfied: tensordict>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from torchrl) (0.6.2)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from tensordict>=0.6.0->torchrl) (3.10.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.5.0->torchrl) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.5.0->torchrl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5.0->torchrl) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchrl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTAbkptTbGGp"
      },
      "source": [
        "# Mining Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRvLRKcZjNUy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import enum\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple\n",
        "from torchrl.data import ListStorage, ReplayBuffer\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Jkfteott9o-"
      },
      "outputs": [],
      "source": [
        "match_dict_inv = {0: 'irrelevant', 1: 'relevant', 2: 'active'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsc7WuVG8IRg"
      },
      "outputs": [],
      "source": [
        "class State:\n",
        "    def __init__(self, l_a, l_h, b_e, match=\"relevant\"):\n",
        "        self.length_a = l_a\n",
        "        self.length_h = l_h\n",
        "        self.blocks_e = b_e\n",
        "        self.match = match\n",
        "        self.match_dict = {'irrelevant': 0, 'relevant': 1, 'active': 2}\n",
        "\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash((self.length_a, self.length_h, self.blocks_e, self.match))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        try:\n",
        "            return (self.length_a, self.length_h, self.blocks_e, self.match) == (other.length_a, other.length_h, other.blocks_e, other.match)\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def __ne__(self, other):\n",
        "        return not (self == other)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"(%d, %d, %d, %s)\" % (self.length_a, self.length_h, self.blocks_e, self.match)\n",
        "\n",
        "    def to_numpy(self):\n",
        "      return np.array([self.length_a, self.length_h, self.blocks_e, self.match_dict[self.match]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVLwdBP00p9F"
      },
      "outputs": [],
      "source": [
        "class Fork(enum.IntEnum):\n",
        "  irrelevant = 0,\n",
        "  relevant = 1,\n",
        "  active = 2\n",
        "\n",
        "class Action(enum.IntEnum):\n",
        "  wait = 0,\n",
        "  adopt = 1,\n",
        "  override = 2,\n",
        "  match = 3\n",
        "\n",
        "class Miner():\n",
        "  def __init__(self, id, type, mines_on, mining_power, is_stale=False):\n",
        "    self.id = id\n",
        "    self.type = type\n",
        "    self.mines_on = mines_on\n",
        "    self.mining_power = mining_power\n",
        "    self.is_stale = is_stale\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"(%d, %s, %s, %.2f, %d)\" % (self.id, self.type, self.mines_on, self.mining_power, self.is_stale)\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    if isinstance(other, Miner):\n",
        "        return self.id == other.id\n",
        "    return False\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash(self.id)\n",
        "\n",
        "action_size = 4\n",
        "state_size = 4\n",
        "\n",
        "def to_state_obj(state):\n",
        "  return State(state[0], state[1], state[2], match_dict_inv[state[3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQEjcBWn0ujC"
      },
      "outputs": [],
      "source": [
        "def get_allowed_actions(state, cutoff):\n",
        "  state = to_state_obj(state.cpu().detach().view(-1).numpy())\n",
        "  allowed_actions = [Action.adopt, Action.wait]\n",
        "  if state.length_a == cutoff or state.length_h == cutoff:\n",
        "    allowed_actions.remove(Action.wait)\n",
        "  if state.length_a > state.length_h:\n",
        "    allowed_actions.append(Action.override)\n",
        "  if state.length_a >= state.length_h and state.length_h > 0 and state.match == 'relevant':\n",
        "    allowed_actions.append(Action.match)\n",
        "  return list(set(allowed_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btmk-9tOlJeL"
      },
      "outputs": [],
      "source": [
        "class BitcoinSimulator():\n",
        "  def __init__(self, p, stale, gamma, cost=0, cutoff=20, lam=0):\n",
        "    self.p = p\n",
        "    self.stale = stale\n",
        "    self.gamma = gamma\n",
        "    self.cost = cost\n",
        "    self.cutoff = cutoff\n",
        "    self.lam = lam\n",
        "    self.match_cases = [\"irrelevant\", \"relevant\", \"active\"]\n",
        "    self.a_cost = cost * p\n",
        "    self.h_cost = cost * (1 - p)\n",
        "    self.q = 1 - p - lam\n",
        "    state_count = 0\n",
        "    self.states = {}\n",
        "    self.states_inverted = {}\n",
        "    for l_a in range(self.cutoff + 1):\n",
        "      for l_h in range(self.cutoff + 1):\n",
        "        for b_e in range(l_a + 1):\n",
        "          if self.lam == 0 and b_e > 0:\n",
        "            continue\n",
        "          for match in self.match_cases:\n",
        "            state = State(l_a, l_h, b_e, match)\n",
        "            self.states[state_count] = state\n",
        "            self.states_inverted[state] = state_count\n",
        "            state_count += 1\n",
        "    self.states_counter = state_count\n",
        "\n",
        "    self.S = state_count\n",
        "    self.A = action_size\n",
        "\n",
        "  def get_curr_state():\n",
        "    return self.current_state.to_numpy()\n",
        "\n",
        "  def set_blockchain_state(self, state):\n",
        "    self.current_state = state\n",
        "    self.l_a = state.length_a\n",
        "    self.l_h = state.length_h\n",
        "    self.b_e = state.blocks_e\n",
        "    self.match = state.match\n",
        "\n",
        "  def reset(self):\n",
        "    # probs = [self.p, self.lam, self.q * (1 - self.stale), self.q * self.stale]\n",
        "    # states = [State(1, 0, 0, 'irrelevant'), State(1, 0, 1, 'irrelevant'), State(0, 1, 0, 'relevant'), ]\n",
        "    self.set_blockchain_state(State(0, 0, 0, 'irrelevant'))\n",
        "    return self.current_state.to_numpy()\n",
        "\n",
        "  def create_miners(self, num_miners):\n",
        "\n",
        "    if self.lam != 0:\n",
        "      mining_powers = np.random.random(num_miners-2)\n",
        "      mining_powers = mining_powers / mining_powers.sum() * (1-self.p-self.lam)\n",
        "      mining_powers = np.append(mining_powers, self.p)\n",
        "      mining_powers = np.append(mining_powers, self.lam)\n",
        "      h_mining_powers = mining_powers[:-2]\n",
        "      a_mining_powers = mining_powers[-2]\n",
        "      e_mining_powers = mining_powers[-1]\n",
        "    else:\n",
        "      mining_powers = np.random.random(num_miners-1)\n",
        "      mining_powers = mining_powers / mining_powers.sum() * (1-self.p)\n",
        "      mining_powers = np.append(mining_powers, self.p)\n",
        "      h_mining_powers = mining_powers[:-1]\n",
        "      a_mining_powers = mining_powers[-1]\n",
        "\n",
        "    # number of honest miners\n",
        "    n_h_miners = len(h_mining_powers)\n",
        "    h_miners = [Miner(idx, 'H', 'H', mi) for idx, mi in enumerate(h_mining_powers)]\n",
        "    if self.stale != 0:\n",
        "      # number of stale miners\n",
        "      n_h_s_miners = int(n_h_miners * self.stale)\n",
        "      # number of non stale miners\n",
        "      n_h_ns_miners = n_h_miners - n_h_s_miners\n",
        "\n",
        "\n",
        "      # stale miner list\n",
        "      h_s_miners = random.sample(h_miners, n_h_s_miners)\n",
        "      h_s_miners = list(map(lambda miner: Miner(miner.id, miner.type, miner.mines_on, miner.mining_power, True), h_s_miners))\n",
        "      # non stale miner list\n",
        "      h_ns_miners = list(set(h_miners) - set(h_s_miners))\n",
        "    else:\n",
        "      n_h_ns_miners = n_h_miners\n",
        "      h_ns_miners = h_miners\n",
        "\n",
        "    # gamma fraction of non stale miners mines on adversary chain\n",
        "    # number of honest miners mines on adversary chain\n",
        "    n_h_a_miners = int(n_h_ns_miners * self.gamma)\n",
        "\n",
        "    # number of honest miners mines on public chain\n",
        "    n_h_h_miners = n_h_ns_miners - n_h_a_miners\n",
        "\n",
        "    # list of honest miners mines on adversary chain\n",
        "    h_a_miners = random.sample(h_ns_miners, n_h_a_miners)\n",
        "    h_a_miners = list(map(lambda miner: Miner(miner.id, miner.type, 'A', miner.mining_power, miner.is_stale), h_a_miners))\n",
        "\n",
        "    # list of honest miners mines on honest chain\n",
        "    h_h_miners = list(set(h_ns_miners) - set(h_a_miners))\n",
        "\n",
        "    # all honest miners\n",
        "    if self.stale != 0:\n",
        "      miners = h_s_miners + h_a_miners + h_h_miners\n",
        "    else:\n",
        "      miners = h_a_miners + h_h_miners\n",
        "\n",
        "    a_miners = [Miner(len(miners), 'A', None, a_mining_powers, False)]\n",
        "    if self.lam != 0:\n",
        "      e_miners = [Miner(len(miners)+1, 'E', None, e_mining_powers, False)]\n",
        "      miners += a_miners + e_miners\n",
        "    else:\n",
        "      miners += a_miners\n",
        "    miners = sorted(miners, key=lambda miner: miner.id)\n",
        "    self.miners = miners\n",
        "    self.mining_powers = mining_powers\n",
        "\n",
        "  def mine_next_block(self):\n",
        "    # Generate block discovery times for each miner using exponential distribution\n",
        "    discovery_times = np.random.exponential(scale=1/self.mining_powers)\n",
        "    # Determine the miner who finds the first block\n",
        "    first_miner = self.miners[np.argmin(discovery_times)]\n",
        "    return first_miner\n",
        "\n",
        "  def step(self, action):\n",
        "    winner = self.mine_next_block()\n",
        "\n",
        "    if action == 0:\n",
        "      if self.match == 'active' and self.l_a >= self.l_h and self.l_h > 0:\n",
        "        payout = (self.l_h)*(self.l_a - self.b_e)//self.l_a\n",
        "        new_b_e = self.b_e - (self.l_h - payout)\n",
        "\n",
        "        # generate probability matrix\n",
        "        if winner.type == 'A':\n",
        "          wait_probs = [1, 0, 0, 0, 0]\n",
        "        elif winner.type == 'E':\n",
        "          wait_probs = [0, 1, 0, 0, 0]\n",
        "        elif winner.type == 'H' and winner.mines_on == 'A' and winner.is_stale == False:\n",
        "          wait_probs = [0, 0, 1, 0, 0]\n",
        "        elif winner.type == 'H' and winner.mines_on == 'H' and winner.is_stale == False:\n",
        "          wait_probs = [0, 0, 0, 1, 0]\n",
        "        elif winner.type == 'H' and winner.is_stale == True:\n",
        "          wait_probs = [0, 0, 0, 0, 1]\n",
        "        wait_reward_states = [((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e, \"active\")), ((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e+1, \"active\")), ((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a - self.l_h, 1, new_b_e, \"relevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h + 1, self.b_e, \"relevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h, self.b_e, \"active\"))]\n",
        "        next_reward_state_idx = np.random.choice(range(len(wait_reward_states)), p=wait_probs)\n",
        "        next_reward_state = wait_reward_states[next_reward_state_idx]\n",
        "        self.set_blockchain_state(next_reward_state[1])\n",
        "        return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n",
        "      else:\n",
        "        # generate probability matrix\n",
        "        if winner.type == 'A':\n",
        "          wait_probs = [1, 0, 0, 0]\n",
        "        elif winner.type == 'E':\n",
        "          wait_probs = [0, 1, 0, 0]\n",
        "        elif winner.type == 'H' and winner.is_stale == False:\n",
        "          wait_probs = [0, 0, 1, 0]\n",
        "        elif winner.type == 'H' and winner.is_stale == True:\n",
        "          wait_probs = [0, 0, 0, 1]\n",
        "        wait_reward_states = [((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e, \"irrelevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e+1, \"irrelevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h + 1, self.b_e, \"relevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h, self.b_e, \"irrelevant\"))]\n",
        "        next_reward_state_idx = np.random.choice(range(len(wait_reward_states)), p=wait_probs)\n",
        "        next_reward_state = wait_reward_states[next_reward_state_idx]\n",
        "\n",
        "        self.set_blockchain_state(next_reward_state[1])\n",
        "        return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n",
        "\n",
        "    elif action == 1:\n",
        "      # generate probability matrix\n",
        "      if winner.type == 'A':\n",
        "        adopt_probs = [1, 0, 0, 0]\n",
        "      elif winner.type == 'E':\n",
        "        adopt_probs = [0, 1, 0, 0]\n",
        "      elif winner.type == 'H' and winner.is_stale == False:\n",
        "        adopt_probs = [0, 0, 1, 0]\n",
        "      elif winner.type == 'H' and winner.is_stale == True:\n",
        "        adopt_probs = [0, 0, 0, 1]\n",
        "      adopt_reward_states = [((-self.a_cost, self.l_h - self.h_cost), State(1, 0, 0, \"irrelevant\")), ((-self.a_cost, self.l_h - self.h_cost), State(1, 0, 1, \"irrelevant\")), ((-self.a_cost, self.l_h - self.h_cost), State(0, 1, 0, \"relevant\")), ((-self.a_cost, self.l_h - self.h_cost), State(0, 0, 0, \"irrelevant\"))]\n",
        "      next_reward_state_idx = np.random.choice(range(len(adopt_reward_states)), p=adopt_probs)\n",
        "      next_reward_state = adopt_reward_states[next_reward_state_idx]\n",
        "      self.set_blockchain_state(next_reward_state[1])\n",
        "      return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n",
        "\n",
        "    elif action == 2:\n",
        "      payout = (self.l_h+1)*(self.l_a - self.b_e)//self.l_a\n",
        "      new_b_e = self.b_e - (self.l_h+1 - payout)\n",
        "      if winner.type == 'A':\n",
        "        override_probs = [1, 0, 0, 0]\n",
        "      elif winner.type == 'E':\n",
        "        override_probs = [0, 1, 0, 0]\n",
        "      elif winner.type == 'H' and winner.is_stale == False:\n",
        "        override_probs = [0, 0, 1, 0]\n",
        "      elif winner.type == 'H' and winner.is_stale == True:\n",
        "        override_probs = [0, 0, 0, 1]\n",
        "\n",
        "      override_reward_states = [((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a - self.l_h, 0, new_b_e, \"irrelevant\")),((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a - self.l_h, 0, new_b_e + 1, \"irrelevant\")), ((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a-self.l_h-1, 1, new_b_e, \"relevant\")), ((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a-self.l_h-1, 0, new_b_e, \"irrelevant\"))]\n",
        "      next_reward_state_idx = np.random.choice(range(len(override_reward_states)), p=override_probs)\n",
        "      next_reward_state = override_reward_states[next_reward_state_idx]\n",
        "      self.set_blockchain_state(next_reward_state[1])\n",
        "      return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n",
        "\n",
        "    elif action == 3:\n",
        "\n",
        "      payout = (self.l_h)*(self.l_a - self.b_e)//self.l_a\n",
        "      new_b_e = self.b_e - (self.l_h - payout)\n",
        "      if winner.type == 'A':\n",
        "        match_probs = [1, 0, 0, 0, 0]\n",
        "      elif winner.type == 'E':\n",
        "        match_probs = [0, 1, 0, 0, 0]\n",
        "      elif winner.type == 'H' and winner.mines_on == 'A' and winner.is_stale == False:\n",
        "        match_probs = [0, 0, 1, 0, 0]\n",
        "      elif winner.type == 'H' and winner.mines_on == 'H' and winner.is_stale == False:\n",
        "        match_probs = [0, 0, 0, 1, 0]\n",
        "      elif winner.type == 'H' and winner.is_stale == True:\n",
        "        match_probs = [0, 0, 0, 0, 1]\n",
        "\n",
        "      match_reward_states = [((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e, \"active\")),((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e+1, \"active\")), ((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a - self.l_h, 1, new_b_e, \"relevant\")),((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h + 1, self.b_e, \"relevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h, self.b_e, \"active\"))]\n",
        "      next_reward_state_idx = np.random.choice(range(len(match_reward_states)), p=match_probs)\n",
        "      next_reward_state = match_reward_states[next_reward_state_idx]\n",
        "      self.set_blockchain_state(next_reward_state[1])\n",
        "      return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6IFZ4NUDQhp"
      },
      "outputs": [],
      "source": [
        "# Creating the architecture of the network\n",
        "class Network(nn.Module):\n",
        "  # number of input neurons = size of the dimensions of the state (8)\n",
        "  # number of actions\n",
        "  # random seed\n",
        "  def __init__(self, state_size, action_size, seed = 42):\n",
        "    super(Network, self).__init__()\n",
        "    # Sets the seed for generating random numbers\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    # first full connection between the input layer and hidden layer\n",
        "    self.fc1 = nn.Linear(state_size, 2048)\n",
        "    # second full connection layer between first hidden layer and second layer\n",
        "    self.fc2 = nn.Linear(2048, 1024)\n",
        "    self.fc3 = nn.Linear(1024, 512)\n",
        "    self.fc4 = nn.Linear(512, 256)\n",
        "    # connetion between the second hidden layer and the output layer\n",
        "    self.fc5 = nn.Linear(256, action_size)\n",
        "\n",
        "  # forward propogation from input layer to the output layer\n",
        "  def forward(self, state):\n",
        "    # applying activation function (propogate the signal from input layer\n",
        "    # to the first hidden layer applying activation function)\n",
        "    x = self.fc1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc4(x)\n",
        "    x = F.relu(x)\n",
        "    return self.fc5(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8a2_QL3DWFW"
      },
      "outputs": [],
      "source": [
        "learning_rate = 5e-5\n",
        "# size of each batch where the model will be trained\n",
        "minibatch_size = 400\n",
        "discount_factor = 0.999\n",
        "# size of the replay buffer\n",
        "replay_buffer_size = int(1e12)\n",
        "interpolation_parameter = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMeHyFQzDY1F"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    # local q network of adversary\n",
        "    self.a_local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    # target q network of adversary\n",
        "    self.a_target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    # local q network of honest\n",
        "    self.h_local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    # target q network of honest\n",
        "    self.h_target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "\n",
        "    # parameters are the weights of the network\n",
        "    self.a_optimizer = optim.AdamW(self.a_local_qnetwork.parameters(), lr=learning_rate)\n",
        "    self.h_optimizer = optim.AdamW(self.h_local_qnetwork.parameters(), lr=learning_rate)\n",
        "\n",
        "    # replay memory\n",
        "    self.memory = ReplayBuffer(storage=ListStorage(900000))\n",
        "    # timestep to decide when to learn from the experirences\n",
        "    self.t_step = 0\n",
        "    self.temp = 1e-9\n",
        "\n",
        "# method to store exp and decide when to learn from them\n",
        "  def step(self, state, action, reward_a, reward_h, next_state):\n",
        "    self.memory.add((state, next_state, action, reward_a, reward_h))\n",
        "    # when timestep reaches 4 the model will learn by taking a minibatch from the\n",
        "    # replay buffer\n",
        "    self.t_step = (self.t_step + 1) % 20\n",
        "    if self.t_step == 0:\n",
        "      # check if there are at least 100 exp in the buffer\n",
        "      if len(self.memory) > minibatch_size:\n",
        "        experiences = self.memory.sample(400)\n",
        "        self.learn(experiences, discount_factor)\n",
        "\n",
        "\n",
        "  def act(self, state, eps_t):\n",
        "    # adding an extra dimension corresponding to the batch (it indicates to which batch this state belogns to)\n",
        "    # note that always the batch index should be at the beginning\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "    # set the local network to evaluation mode before forward pass\n",
        "    # because as this is the forward pass we are making predictions\n",
        "    self.a_local_qnetwork.eval()\n",
        "    self.h_local_qnetwork.eval()\n",
        "    # since we are in forward there is no need to calculate gradients\n",
        "    with torch.no_grad():\n",
        "      # predicting q values (forward pass)\n",
        "      a_action_values = self.a_local_qnetwork(state).cpu().data.squeeze(0)\n",
        "      h_action_values = self.h_local_qnetwork(state).cpu().data.squeeze(0)\n",
        "      # print(a_action_values, h_action_values)\n",
        "      # relative_rev = torch.max(a_action_values / (a_action_values + h_action_values)).item()\n",
        "    # resetting model to traning mode\n",
        "\n",
        "    self.a_local_qnetwork.train()\n",
        "    self.h_local_qnetwork.train()\n",
        "\n",
        "    # select an action based on epsilon greedy policy\n",
        "    # we generate a random number R and if R > epsilon ? we choose the maximum predicted q value\n",
        "    # : select a random aciton\n",
        "    rel_actions = a_action_values / (a_action_values + h_action_values + self.temp)\n",
        "\n",
        "    mask_value = float(\"-inf\")\n",
        "    # unallowed actions\n",
        "    diff = list(set([0, 1, 2, 3]) - set(get_allowed_actions(state, 20)))\n",
        "    if diff:\n",
        "      rel_actions[torch.tensor(diff)] = mask_value\n",
        "\n",
        "    tensor = rel_actions / eps_t\n",
        "    # replaced_tensor = torch.where(torch.isposinf(tensor), torch.tensor(1.0), tensor)\n",
        "    tensor -= torch.max(tensor)\n",
        "    probabilities = F.softmax(tensor, dim=-1)\n",
        "    act = torch.multinomial(probabilities, 1).item()\n",
        "    return act\n",
        "\n",
        "\n",
        "  # allows agent to learn based on the minibatch\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    states, next_states, actions, rewards_a, rewards_h = experiences\n",
        "\n",
        "    states = states.to(dtype=torch.float32, device=self.device)\n",
        "    next_states = next_states.to(dtype=torch.float32, device=self.device)\n",
        "    actions = actions.unsqueeze(1).to(dtype=torch.long, device=self.device)\n",
        "    rewards_a = rewards_a.unsqueeze(1).to(dtype=torch.float32, device=self.device)\n",
        "    rewards_h = rewards_h.unsqueeze(1).to(dtype=torch.float32, device=self.device)\n",
        "\n",
        "    # to compute the target q value we need the maxium q value for the next state\n",
        "    # use the target network to get the q values for all the actions from that next state\n",
        "    # next_q_targets = self.a_target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    # (100, 4)\n",
        "\n",
        "    next_a_q_targets = self.a_target_qnetwork(next_states).detach()\n",
        "    next_h_q_targets = self.h_target_qnetwork(next_states).detach()\n",
        "    # (100, 1)\n",
        "    a_ = torch.stack([next_a_q_targets[idx] / (next_a_q_targets[idx] + next_h_q_targets[idx] + self.temp) for idx, _ in enumerate(next_a_q_targets)])\n",
        "    # (100, 4\n",
        "    mask_value = float(\"-inf\")\n",
        "    # unallowed actions\n",
        "    for idx, state in enumerate(next_states):\n",
        "      diff = list(set([0, 1, 2, 3]) - set(get_allowed_actions(state, 20)))\n",
        "      if diff:\n",
        "        a_[idx][torch.tensor(diff)] = mask_value\n",
        "\n",
        "    a_ = a_.max(1)[1].unsqueeze(1).view(-1, 1)\n",
        "    # (100, 1)\n",
        "    q_a_targets = rewards_a + (discount_factor * next_a_q_targets.gather(1, a_))\n",
        "    q_h_targets = rewards_h + (discount_factor * next_h_q_targets.gather(1, a_))\n",
        "    # q_targets = rewards + (discount_factor * next_q_targets * (1 - dones))\n",
        "    # forward propogate the states to get the predicted q values\n",
        "\n",
        "    q_a_expected = self.a_local_qnetwork(states).gather(1, actions)\n",
        "    q_h_expected = self.h_local_qnetwork(states).gather(1, actions)\n",
        "    # loss (mean squared error)\n",
        "\n",
        "    loss_a = F.mse_loss(q_a_expected, q_a_targets)\n",
        "    loss_h = F.mse_loss(q_h_expected, q_h_targets)\n",
        "\n",
        "    # delta_a = q_a_expected - q_a_targets\n",
        "    # delta_h = q_h_expected - q_h_targets\n",
        "\n",
        "    # priorities_a = (delta_a.abs().cpu().detach().numpy().flatten())\n",
        "    # priorities_h = (delta_h.abs().cpu().detach().numpy().flatten())\n",
        "    # priorities = priorities_a / (priorities_a + priorities_h + self.temp)\n",
        "    # self.memory.update_priority(info['index'], priorities)\n",
        "\n",
        "    # backpropogating the error to update the weights\n",
        "    self.a_optimizer.zero_grad()\n",
        "    self.h_optimizer.zero_grad()\n",
        "\n",
        "    loss_a.backward()\n",
        "    loss_h.backward()\n",
        "\n",
        "    # max_grad_norm = 0.5\n",
        "    # clip_grad_norm_(self.a_local_qnetwork.parameters(), max_grad_norm)\n",
        "    # clip_grad_norm_(self.h_local_qnetwork.parameters(), max_grad_norm)\n",
        "\n",
        "    # single optimization step for updating the weights\n",
        "    self.a_optimizer.step()\n",
        "    self.h_optimizer.step()\n",
        "\n",
        "    # updating the target network weights\n",
        "    self.soft_update(self.a_local_qnetwork, self.a_target_qnetwork, interpolation_parameter)\n",
        "    self.soft_update(self.h_local_qnetwork, self.h_target_qnetwork, interpolation_parameter)\n",
        "\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    for target_param, local_param in zip(target_model.parameters(),local_model.parameters()):\n",
        "      # softly update the target model parameters with the weighted average of the local and target params\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "arR4RJr-DpoU",
        "outputId": "2f916c72-27ba-4dc3-97a8-b529542364e3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n[D]p = 0.45 stale = 0.1 gamma = 0.5 cost = 0 lam = 0.2\\np = 0.45 stale = 0.01 gamma = 0.5 cost = 0 lam = 0.3\\n'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p = 0.45\n",
        "stale = 0.2\n",
        "gamma = 0.5\n",
        "cost = 0\n",
        "lam = 0.2\n",
        "# setup up the environment\n",
        "env = BitcoinSimulator(p=p, stale=stale, gamma = gamma, cost = cost, cutoff=20, lam=lam)\n",
        "env.create_miners(1000)\n",
        "\n",
        "\"\"\"\n",
        "[D]p = 0.45 stale = 0.1 gamma = 0.5 cost = 0 lam = 0.2\n",
        "p = 0.45 stale = 0.01 gamma = 0.5 cost = 0 lam = 0.3\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0mgsTVSDbGU"
      },
      "outputs": [],
      "source": [
        "agent = Agent(state_size, action_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWWnRFYzDmLe",
        "outputId": "a580e14f-ee3e-4e0b-ad99-3a4fe4c629d5"
      },
      "outputs": [],
      "source": [
        "state = env.reset()\n",
        "a_score = 0\n",
        "h_score = 0\n",
        "temperature = 1\n",
        "temperature_decay = 0.9998\n",
        "min_temperature = 1e-5\n",
        "number_episodes = 600000\n",
        "window_size = 10000\n",
        "r_a = []\n",
        "r_h = []\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "    action = agent.act(state, temperature)\n",
        "    next_state, reward_a, reward_h = env.step(action)\n",
        "    agent.step(torch.tensor(state), torch.tensor(action), torch.tensor(reward_a), torch.tensor(reward_h), torch.tensor(next_state))\n",
        "    state = next_state\n",
        "    a_score += reward_a\n",
        "    h_score += reward_h\n",
        "    r_a.append(reward_a)\n",
        "    r_h.append(reward_h)\n",
        "\n",
        "    if episode >= window_size:\n",
        "        window_a = np.sum(r_a[episode - window_size:episode])\n",
        "        window_h = np.sum(r_h[episode - window_size:episode])\n",
        "        rel_gain = window_a / (window_a + window_h) if (window_a + window_h) != 0 else 0\n",
        "        print(rel_gain)\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        print(a_score, h_score)\n",
        "        rel = a_score / (a_score + h_score)\n",
        "        print('\\rEpisode {}\\tGain: {:.4f}'.format(episode, rel))\n",
        "\n",
        "    if episode % 10000 == 0:\n",
        "        np.save('r_a.npy', r_a)\n",
        "        np.save('r_h.npy', r_h)\n",
        "\n",
        "    temperature = max(min_temperature, temperature * temperature_decay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAduRi811L5q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "# Load rewards\n",
        "r_a = np.load('r_a.npy')  # Adversary rewards\n",
        "r_h = np.load('r_h.npy')  # Honest miner rewards\n",
        "\n",
        "optimal = 0.8118\n",
        "T_w = 100000  # Window size\n",
        "k = 10  # Sampling interval\n",
        "T = len(r_a)\n",
        "\n",
        "# Calculate RMG values\n",
        "rmg_values = []\n",
        "timesteps = []\n",
        "\n",
        "for t in range(0, T - T_w + 1, k):\n",
        "    r_a_window = np.sum(r_a[t:t + T_w])\n",
        "    r_h_window = np.sum(r_h[t:t + T_w])\n",
        "    rmg = r_a_window / (r_a_window + r_h_window)\n",
        "    rmg_values.append(rmg)\n",
        "    timesteps.append(t)\n",
        "\n",
        "# Apply Gaussian filter for appearance only\n",
        "smoothed_rmg_values = gaussian_filter1d(rmg_values, sigma=2)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(timesteps, rmg_values, linestyle='-', lw=3, label=\"Raw RMG\")\n",
        "# plt.plot(timesteps, smoothed_rmg_values, linestyle='-', lw=2, label=\"Smoothed RMG\", color='blue')\n",
        "\n",
        "# Highlight the optimal line\n",
        "plt.axhline(y=optimal, color='#ffcc00', linestyle='--', linewidth=2, label='Optimal Policy')\n",
        "\n",
        "plt.xlabel(\"Timesteps\")\n",
        "plt.ylabel(\"RMG\")\n",
        "plt.ylim([0.4, 0.9])\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.title(\"RMG vs Timesteps\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEUrpLc8WTkT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1 ---- 0.7750167846679688\n",
        "# 2 ---- 0.7699813842773438\n",
        "def plot_convergence(gamma, stale=0, lam=0):\n",
        "  p = 0.45\n",
        "  window_size=10\n",
        "  initial_window_size = 1\n",
        "  later_window_size = 10\n",
        "  switch_point = 6 # point to switch window size\n",
        "  fig, ax = plt.subplots()\n",
        "  optimal_dict = 0.77\n",
        "\n",
        "  rl_revenues = np.load('rl_revenues_%.3fhashrate%.2fgamma%.3fstale%.2flam.npy' % (p, gamma, stale, lam))\n",
        "  rl_initial_moving_avg = np.convolve(rl_revenues[:switch_point], np.ones(initial_window_size)/initial_window_size, mode='valid')\n",
        "  rl_later_moving_avg = np.convolve(rl_revenues[switch_point:], np.ones(later_window_size)/later_window_size, mode='valid')\n",
        "  rl_moving_averages = np.concatenate((rl_initial_moving_avg, rl_later_moving_avg))\n",
        "  print(rl_later_moving_avg)\n",
        "  ax.plot(rl_moving_averages, '-',linewidth='2', label='DRL Mining')\n",
        "  plt.axhline(y=optimal_dict, color='r', linestyle='--', label='optimal revenue')\n",
        "  ax.set_ylim([0.2, 0.9])\n",
        "  ax.set_xlim([0, len(rl_moving_averages)])\n",
        "  ax.set_xlabel(r'Timesteps $\\times 10^4$')\n",
        "  ax.set_ylabel('Mining Reward')\n",
        "  ax.legend(loc='lower right')\n",
        "  # plt.show()\n",
        "  plt.savefig('rl_revenues_%.3fhashrate%.2fgamma%.3fstale%.2flam.png' % (p, gamma, stale, lam))\n",
        "\n",
        "# plot_convergence(gamma=0.5, stale=0.1, lam=0.2)\n",
        "plot_convergence(gamma=0.5, stale=0.1, lam=0.3)\n",
        "\n",
        "# plot_convergence(gamma=0.5)\n",
        "# plot_convergence(gamma=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6zUVXsYv211"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
