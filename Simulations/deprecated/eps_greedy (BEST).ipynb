{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z-nWnnHFjJAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c3d0be-eeb3-47c6-c794-dae255dfa6e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymdptoolbox\n",
            "  Downloading pymdptoolbox-4.0-b3.zip (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.11.4)\n",
            "Building wheels for collected packages: pymdptoolbox\n",
            "  Building wheel for pymdptoolbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymdptoolbox: filename=pymdptoolbox-4.0b3-py3-none-any.whl size=25656 sha256=d2b6a71758457a91de40530982360615bbc5dfab3512976b260f7842cd5d7b07\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/e7/c7/d7abf9e309f3573a934fed2750c70bd75d9e9d901f7f16e183\n",
            "Successfully built pymdptoolbox\n",
            "Installing collected packages: pymdptoolbox\n",
            "Successfully installed pymdptoolbox-4.0b3\n",
            "Collecting torchrl\n",
            "  Downloading torchrl-0.4.0-cp310-cp310-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchrl) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchrl) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchrl) (24.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from torchrl) (2.2.1)\n",
            "Collecting tensordict>=0.4.0 (from torchrl)\n",
            "  Downloading tensordict-0.4.0-cp310-cp310-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchrl)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchrl) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchrl) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, tensordict, torchrl\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 tensordict-0.4.0 torchrl-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pymdptoolbox\n",
        "!pip install torchrl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mining Environment"
      ],
      "metadata": {
        "id": "cTAbkptTbGGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mdptoolbox, mdptoolbox.example\n",
        "import numpy as np\n",
        "import enum\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple\n",
        "from torchrl.data import ListStorage, PrioritizedReplayBuffer\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "VRvLRKcZjNUy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "match_dict_inv = {0: 'irrelevant', 1: 'relevant', 2: 'active'}"
      ],
      "metadata": {
        "id": "7Jkfteott9o-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b714e7af-fc7e-4958-da50-190c961f60de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class State:\n",
        "    def __init__(self, l_a, l_h, b_e, match=\"relevant\"):\n",
        "        self.length_a = l_a\n",
        "        self.length_h = l_h\n",
        "        self.blocks_e = b_e\n",
        "        self.match = match\n",
        "        self.match_dict = {'irrelevant': 0, 'relevant': 1, 'active': 2}\n",
        "\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash((self.length_a, self.length_h, self.blocks_e, self.match))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        try:\n",
        "            return (self.length_a, self.length_h, self.blocks_e, self.match) == (other.length_a, other.length_h, other.blocks_e, other.match)\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def __ne__(self, other):\n",
        "        return not (self == other)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"(%d, %d, %d, %s)\" % (self.length_a, self.length_h, self.blocks_e, self.match)\n",
        "\n",
        "    def to_numpy(self):\n",
        "      return np.array([self.length_a, self.length_h, self.blocks_e, self.match_dict[self.match]])\n"
      ],
      "metadata": {
        "id": "dsc7WuVG8IRg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiningEnv():\n",
        "\n",
        "  def __init__(self, p, stale, gamma, cost, cutoff=20, lam=0):\n",
        "    self.p = p\n",
        "    self.cutoff = cutoff\n",
        "    self.lam = lam\n",
        "    self.match_cases = [\"irrelevant\", \"relevant\", \"active\"]\n",
        "    self.a_cost = cost * p\n",
        "    self.h_cost = cost * (1 - p)\n",
        "    self.q = 1 - p - lam\n",
        "    self.stale = stale\n",
        "    self.gamma = gamma\n",
        "    self.current_state = State(0, 0, 0, 'irrelevant')\n",
        "    state_count = 0\n",
        "    self.states = {}\n",
        "    self.states_inverted = {}\n",
        "    for l_a in range(self.cutoff + 1):\n",
        "      for l_h in range(self.cutoff + 1):\n",
        "        for b_e in range(l_a + 1):\n",
        "          if self.lam == 0 and b_e > 0:\n",
        "            continue\n",
        "          for match in self.match_cases:\n",
        "            state = State(l_a, l_h, b_e, match)\n",
        "            self.states[state_count] = state\n",
        "            self.states_inverted[state] = state_count\n",
        "            state_count += 1\n",
        "    self.states_counter = state_count\n",
        "\n",
        "    # transition matrices\n",
        "    P_adopt = np.zeros(shape=(state_count, state_count))\n",
        "    P_override = np.zeros(shape=(state_count, state_count))\n",
        "    P_match = np.zeros(shape=(state_count, state_count))\n",
        "    P_wait = np.zeros(shape=(state_count, state_count))\n",
        "\n",
        "    # reward matrices\n",
        "    R_adopt = np.empty(shape=(state_count, state_count), dtype=object)\n",
        "    R_override = np.empty(shape=(state_count, state_count), dtype=object)\n",
        "    R_match = np.empty(shape=(state_count, state_count), dtype=object)\n",
        "    R_wait = np.empty(shape=(state_count, state_count), dtype=object)\n",
        "\n",
        "    R_adopt.fill((0,0))\n",
        "    R_override.fill((0,0))\n",
        "    R_match.fill((0,0))\n",
        "    R_wait.fill((0,0))\n",
        "\n",
        "    for state_idx, state in self.states.items():\n",
        "      l_a = state.length_a\n",
        "      l_h = state.length_h\n",
        "      b_e = state.blocks_e\n",
        "      match = state.match\n",
        "\n",
        "      # adopt action transition matrix\n",
        "      # attacker mines next block\n",
        "      P_adopt[state_idx, self.states_inverted[State(1, 0, 0, \"irrelevant\")]] = self.p\n",
        "      R_adopt[state_idx, self.states_inverted[State(1, 0, 0, \"irrelevant\")]] = (-self.a_cost, l_h - self.h_cost)\n",
        "\n",
        "      # eclipsed node mines next block\n",
        "      if self.lam != 0:\n",
        "        P_adopt[state_idx, self.states_inverted[State(1, 0, 1, \"irrelevant\")]] = self.lam\n",
        "        R_adopt[state_idx, self.states_inverted[State(1, 0, 1, \"irrelevant\")]] = (-self.a_cost, l_h - self.h_cost)\n",
        "\n",
        "      # honest network mines next block\n",
        "      P_adopt[state_idx, self.states_inverted[State(0, 1, 0, \"relevant\")]] = self.q*(1-self.stale)\n",
        "      R_adopt[state_idx, self.states_inverted[State(0, 1, 0, \"relevant\")]] = (-self.a_cost, l_h - self.h_cost)\n",
        "\n",
        "      # network mines state block\n",
        "      P_adopt[state_idx, self.states_inverted[State(0, 0, 0, \"irrelevant\")]] = self.q * self.stale\n",
        "      R_adopt[state_idx, self.states_inverted[State(0, 0, 0, \"irrelevant\")]] = (-self.a_cost, l_h - self.h_cost)\n",
        "\n",
        "      # override action transition matrix\n",
        "      if l_a > l_h:\n",
        "        payout = (l_h+1)*(l_a - b_e)//l_a\n",
        "        new_b_e = b_e - (l_h+1 - payout)\n",
        "      # attacker mines next block\n",
        "        P_override[state_idx, self.states_inverted[State(l_a - l_h, 0, new_b_e, \"irrelevant\")]] = self.p\n",
        "        R_override[state_idx, self.states_inverted[State(l_a - l_h, 0, new_b_e, \"irrelevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "      # eclipsed node mines next block\n",
        "        if self.lam != 0:\n",
        "          P_override[state_idx, self.states_inverted[State(l_a - l_h, 0, new_b_e + 1, \"irrelevant\")]] = self.lam\n",
        "          R_override[state_idx, self.states_inverted[State(l_a - l_h, 0, new_b_e + 1, \"irrelevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "      # network mines next block\n",
        "        P_override[state_idx, self.states_inverted[State(l_a-l_h-1, 1, new_b_e, \"relevant\")]] = self.q*(1 - self.stale)\n",
        "        R_override[state_idx, self.states_inverted[State(l_a-l_h-1, 1, new_b_e, \"relevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "      # network mines stale block\n",
        "        P_override[state_idx, self.states_inverted[State(l_a-l_h-1, 0, new_b_e, \"irrelevant\")]] = self.q*self.stale\n",
        "        R_override[state_idx, self.states_inverted[State(l_a-l_h-1, 0, new_b_e, \"irrelevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "      else:\n",
        "        P_override[state_idx, state_idx] = 1\n",
        "        R_override[state_idx, state_idx] = (-10, -10)\n",
        "\n",
        "      # perform adopt and override after cutoff reached\n",
        "      if l_a == self.cutoff or l_h == self.cutoff:\n",
        "        P_match[state_idx, state_idx] = 1\n",
        "        R_match[state_idx, state_idx] = (-10, -10)\n",
        "        P_wait[state_idx, state_idx] = 1\n",
        "        R_wait[state_idx, state_idx] = (-10, -10)\n",
        "        continue\n",
        "\n",
        "\n",
        "      # match action transition matrix\n",
        "      if match == 'relevant' and l_a >= l_h and l_h > 0:\n",
        "        payout = (l_h)*(l_a - b_e)//l_a\n",
        "        new_b_e = b_e - (l_h - payout)\n",
        "\n",
        "        # attacker mines next block\n",
        "        P_match[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"active\")]] = self.p\n",
        "        R_match[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"active\")]] =  (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # eclipsed node mines next block\n",
        "        if self.lam != 0:\n",
        "          P_match[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"active\")]] = self.lam\n",
        "          R_match[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines next block after pool's head\n",
        "        P_match[state_idx, self.states_inverted[State(l_a - l_h, 1, new_b_e, \"relevant\")]] = self.gamma * self.q * (1 - self.stale)\n",
        "        R_match[state_idx, self.states_inverted[State(l_a - l_h, 1, new_b_e, \"relevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "        # network mines next block after other's head\n",
        "        P_match[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (1-self.gamma) * self.q * (1 - self.stale)\n",
        "        R_match[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines stale block\n",
        "        P_match[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"active\")]] = self.q * self.stale\n",
        "        R_match[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "      else:\n",
        "        P_match[state_idx, state_idx] = 1\n",
        "        R_match[state_idx, state_idx] = (-10, -10)\n",
        "\n",
        "      # wait action transition matrix\n",
        "      if match == 'active' and l_a >= l_h and l_h > 0:\n",
        "        payout = (l_h)*(l_a - b_e)//l_a\n",
        "        new_b_e = b_e - (l_h - payout)\n",
        "\n",
        "        # attacker mines next block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"active\")]] = self.p\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # eclipsed node mines next block\n",
        "        if self.lam != 0:\n",
        "          P_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"active\")]] = self.lam\n",
        "          R_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines after the pool's head\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a - l_h, 1, new_b_e, \"relevant\")]] = self.gamma * self.q * (1 - self.stale)\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a - l_h, 1, new_b_e, \"relevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "        # network mines after other's head\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (1-self.gamma) * self.q * (1 - self.stale)\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines stale block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"active\")]] = self.q * self.stale\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "      else:\n",
        "        # attacker mines next block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"irrelevant\")]] = self.p\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"irrelevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # eclipsed node mines next block\n",
        "        if self.lam != 0:\n",
        "          P_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"irrelevant\")]] = self.lam\n",
        "          R_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"irrelevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines next block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = self.q * (1 - self.stale)\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines stale block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"irrelevant\")]] = self.q * self.stale\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"irrelevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "    self.P = np.array([P_wait, P_adopt, P_override, P_match])\n",
        "    self.R = np.array([R_wait, R_adopt, R_override, R_match])\n",
        "\n",
        "    # state dimension\n",
        "    self.S = self.P.shape[1]\n",
        "\n",
        "    # action dimension\n",
        "    self.A = len(self.P)\n",
        "\n",
        "  def reset(self):\n",
        "    probs = [self.p, self.lam, self.q * (1 - self.stale), self.q * self.stale]\n",
        "    states = [State(1, 0, 0, 'irrelevant'), State(1, 0, 1, 'irrelevant'), State(0, 1, 0, 'relevant'), State(0, 0, 0, 'irrelevant')]\n",
        "    self.current_state = np.random.choice(states, p=probs)\n",
        "    return self.current_state.to_numpy()\n",
        "\n",
        "  def rand_state(self):\n",
        "    return self.states[np.random.randint(0, self.S)].to_numpy()\n",
        "\n",
        "  def step(self, action):\n",
        "    # p_s_new = np.random.random()\n",
        "    # p = 0\n",
        "    # s_new = -1\n",
        "    s_new = np.random.choice(np.arange(self.states_counter),p=env.P[action][self.states_inverted[self.current_state]])\n",
        "    # while (p < p_s_new) and (s_new < (self.S - 1)):\n",
        "    #   s_new = s_new + 1\n",
        "    #   p = p + self.P[action][self.states_inverted[self.current_state], s_new]\n",
        "    # try:\n",
        "    r = self.R[action][self.states_inverted[self.current_state], s_new]\n",
        "    # except IndexError:\n",
        "    #   try:\n",
        "    #     r = self.R[self.states_inverted[self.current_state], action]\n",
        "    #   except IndexError:\n",
        "    #     r = self.R[self.states_inverted[self.current_state]]\n",
        "    self.current_state = self.states[s_new]\n",
        "\n",
        "    return (self.current_state.to_numpy(), r[0], r[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "M-CRaClLjUdz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN"
      ],
      "metadata": {
        "id": "-9cTz6jRVgp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Match(enum.IntEnum):\n",
        "  irrelevant = 0,\n",
        "  relevant = 1,\n",
        "  active = 2\n",
        "\n",
        "class Action(enum.IntEnum):\n",
        "  wait = 0,\n",
        "  adopt = 1,\n",
        "  override = 2,\n",
        "  match = 3\n",
        "\n",
        "action_size = 4\n",
        "state_size = 4\n",
        "\n",
        "def to_state_obj(state):\n",
        "  return State(state[0], state[1], state[2], match_dict_inv[state[3]])"
      ],
      "metadata": {
        "id": "HVLwdBP00p9F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_allowed_actions(state, cutoff):\n",
        "  state = to_state_obj(state.cpu().detach().view(-1).numpy())\n",
        "  allowed_actions = [Action.adopt, Action.wait]\n",
        "  if state.length_a >= cutoff or state.length_h >= cutoff:\n",
        "    allowed_actions.remove(Action.wait)\n",
        "    allowed_actions.append(Action.adopt)\n",
        "    if state.length_a > state.length_h:\n",
        "      allowed_actions.append(Action.override)\n",
        "    return list(set(allowed_actions))\n",
        "  if state.length_a > state.length_h:\n",
        "    allowed_actions.append(Action.override)\n",
        "  if state.length_a >= state.length_h and state.length_h > 0 and state.match == 'relevant':\n",
        "    allowed_actions.append(Action.match)\n",
        "  return list(set(allowed_actions))"
      ],
      "metadata": {
        "id": "TQEjcBWn0ujC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the architecture of the network\n",
        "class Network(nn.Module):\n",
        "  # number of input neurons = size of the dimensions of the state (8)\n",
        "  # number of actions\n",
        "  # random seed\n",
        "  def __init__(self, state_size, action_size, seed = 42):\n",
        "    super(Network, self).__init__()\n",
        "    # Sets the seed for generating random numbers\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    # first full connection between the input layer and hidden layer\n",
        "    self.fc1 = nn.Linear(state_size, 2048)\n",
        "    # second full connection layer between first hidden layer and second layer\n",
        "    self.fc2 = nn.Linear(2048, 1024)\n",
        "    self.fc3 = nn.Linear(1024, 512)\n",
        "    self.fc4 = nn.Linear(512, 256)\n",
        "    # connetion between the second hidden layer and the output layer\n",
        "    self.fc5 = nn.Linear(256, action_size)\n",
        "\n",
        "  # forward propogation from input layer to the output layer\n",
        "  def forward(self, state):\n",
        "    # applying activation function (propogate the signal from input layer\n",
        "    # to the first hidden layer applying activation function)\n",
        "    x = self.fc1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc4(x)\n",
        "    x = F.relu(x)\n",
        "    return self.fc5(x)"
      ],
      "metadata": {
        "id": "_BqOBOqS1Avm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the agent"
      ],
      "metadata": {
        "id": "bwfu4bQcb5SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize hyper parameters\n"
      ],
      "metadata": {
        "id": "CxcAenGqcUep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-5\n",
        "# size of each batch where the model will be trained\n",
        "minibatch_size = 400\n",
        "discount_factor = 0.999\n",
        "# size of the replay buffer\n",
        "replay_buffer_size = int(1e12)\n",
        "interpolation_parameter = 0.01"
      ],
      "metadata": {
        "id": "pTTRd79kb7VD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing experience replay buffer"
      ],
      "metadata": {
        "id": "wKO_RcQOcjkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing experience replay\n",
        "class ReplayMemory(object):\n",
        "  # capacity -> size of the buffer\n",
        "  def __init__(self, capacity):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "\n",
        "  # append a new transition in the memory and ensures that memory contain\n",
        "  # only 100000 transitions\n",
        "  def push(self, event):\n",
        "    self.memory.append(event)\n",
        "    # check if buffer does not exceed the capacity\n",
        "    if len(self.memory) > self.capacity:\n",
        "      del self.memory[0]\n",
        "\n",
        "  # batch_size -> number of experiences sampled in a batch\n",
        "  # each experience contains (state,action,reward,nextstate,boolean done)\n",
        "  def sample(self, batch_size):\n",
        "    experiences = random.sample(self.memory, k=batch_size)\n",
        "    # all the states corresponding to all experiences\n",
        "    # next converting to torch tensor\n",
        "    # finally move to the designated computing device\n",
        "    states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "    # actions\n",
        "    actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
        "    # rewards\n",
        "    reward_a = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "    reward_h = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "    # next states\n",
        "    next_states = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None])).float().to(self.device)\n",
        "\n",
        "    return states, next_states, actions, reward_a, reward_h"
      ],
      "metadata": {
        "id": "GfvTb2hLceWC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    # local q network of adversary\n",
        "    self.a_local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    # target q network of adversary\n",
        "    self.a_target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    # local q network of honest\n",
        "    self.h_local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    # target q network of honest\n",
        "    self.h_target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "\n",
        "    # parameters are the weights of the network\n",
        "    self.a_optimizer = optim.RMSprop(self.a_local_qnetwork.parameters(), lr=learning_rate)\n",
        "    self.h_optimizer = optim.RMSprop(self.h_local_qnetwork.parameters(), lr=learning_rate)\n",
        "\n",
        "    # replay memory\n",
        "    self.memory = PrioritizedReplayBuffer(alpha=0.6, beta=0.4, storage=ListStorage(400000))\n",
        "    # timestep to decide when to learn from the experirences\n",
        "    self.t_step = 0\n",
        "\n",
        "# method to store exp and decide when to learn from them\n",
        "  def step(self, state, action, reward_a, reward_h, next_state):\n",
        "    self.memory.add((state, next_state, action, reward_a, reward_h))\n",
        "    # when timestep reaches 4 the model will learn by taking a minibatch from the\n",
        "    # replay buffer\n",
        "    self.t_step = (self.t_step + 1) % 20\n",
        "    if self.t_step == 0:\n",
        "      # check if there are at least 100 exp in the buffer\n",
        "      if len(self.memory) > minibatch_size:\n",
        "        experiences, info = self.memory.sample(400, return_info=True)\n",
        "        self.learn(info, experiences, discount_factor)\n",
        "\n",
        "\n",
        "  def act(self, state, eps_t):\n",
        "    # adding an extra dimension corresponding to the batch (it indicates to which batch this state belogns to)\n",
        "    # note that always the batch index should be at the beginning\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "    # set the local network to evaluation mode before forward pass\n",
        "    # because as this is the forward pass we are making predictions\n",
        "    self.a_local_qnetwork.eval()\n",
        "    self.h_local_qnetwork.eval()\n",
        "    # since we are in forward there is no need to calculate gradients\n",
        "    with torch.no_grad():\n",
        "      # predicting q values (forward pass)\n",
        "      a_action_values = self.a_local_qnetwork(state).cpu().data.squeeze(0)\n",
        "      h_action_values = self.h_local_qnetwork(state).cpu().data.squeeze(0)\n",
        "      # print(a_action_values, h_action_values)\n",
        "      # relative_rev = torch.max(a_action_values / (a_action_values + h_action_values)).item()\n",
        "    # resetting model to traning mode\n",
        "\n",
        "    self.a_local_qnetwork.train()\n",
        "    self.h_local_qnetwork.train()\n",
        "\n",
        "    # select an action based on epsilon greedy policy\n",
        "    # we generate a random number R and if R > epsilon ? we choose the maximum predicted q value\n",
        "    # : select a random aciton\n",
        "    rel_actions = a_action_values / (a_action_values + h_action_values)\n",
        "    rel_actions -= torch.max(rel_actions)\n",
        "\n",
        "    mask_value = float(\"-inf\")\n",
        "    # unallowed actions\n",
        "    diff = list(set([0, 1, 2, 3]) - set(get_allowed_actions(state, 20)))\n",
        "    if diff:\n",
        "      rel_actions[torch.tensor(diff)] = mask_value\n",
        "\n",
        "    pn = np.random.random()\n",
        "\n",
        "    if pn < eps_t:\n",
        "      valid_actions_indices = torch.nonzero(rel_actions > float('-inf'), as_tuple=False).view(-1)\n",
        "      act = np.random.choice(valid_actions_indices).item()\n",
        "      return act\n",
        "    else:\n",
        "      act = np.argmax(rel_actions.cpu().data.numpy())\n",
        "      return act\n",
        "\n",
        "  # allows agent to learn based on the minibatch\n",
        "  def learn(self, info, experiences, discount_factor):\n",
        "    states, next_states, actions, rewards_a, rewards_h = experiences\n",
        "\n",
        "    states = states.to(dtype=torch.float32, device=self.device)\n",
        "    next_states = next_states.to(dtype=torch.float32, device=self.device)\n",
        "    actions = actions.unsqueeze(1).to(dtype=torch.long, device=self.device)\n",
        "    rewards_a = rewards_a.unsqueeze(1).to(dtype=torch.float32, device=self.device)\n",
        "    rewards_h = rewards_h.unsqueeze(1).to(dtype=torch.float32, device=self.device)\n",
        "\n",
        "    # to compute the target q value we need the maxium q value for the next state\n",
        "    # use the target network to get the q values for all the actions from that next state\n",
        "    # next_q_targets = self.a_target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    # (100, 4)\n",
        "\n",
        "    next_a_q_targets = self.a_target_qnetwork(next_states).detach()\n",
        "    next_h_q_targets = self.h_target_qnetwork(next_states).detach()\n",
        "    # (100, 1)\n",
        "    a_ = torch.stack([next_a_q_targets[idx] / (next_a_q_targets[idx] + next_h_q_targets[idx]) for idx, _ in enumerate(next_a_q_targets)])\n",
        "    # (100, 4\n",
        "    mask_value = float(\"-inf\")\n",
        "    # unallowed actions\n",
        "    for idx, state in enumerate(next_states):\n",
        "      diff = list(set([0, 1, 2, 3]) - set(get_allowed_actions(state, 20)))\n",
        "      if diff:\n",
        "        a_[idx][torch.tensor(diff)] = mask_value\n",
        "\n",
        "    a_ = a_.max(1)[1].unsqueeze(1).view(-1, 1)\n",
        "    # (100, 1)\n",
        "    q_a_targets = rewards_a + (discount_factor * next_a_q_targets.gather(1, a_))\n",
        "    q_h_targets = rewards_h + (discount_factor * next_h_q_targets.gather(1, a_))\n",
        "    # q_targets = rewards + (discount_factor * next_q_targets * (1 - dones))\n",
        "    # forward propogate the states to get the predicted q values\n",
        "\n",
        "    q_a_expected = self.a_local_qnetwork(states).gather(1, actions)\n",
        "    q_h_expected = self.h_local_qnetwork(states).gather(1, actions)\n",
        "    # loss (mean squared error)\n",
        "\n",
        "    loss_a = F.mse_loss(q_a_expected, q_a_targets)\n",
        "    loss_h = F.mse_loss(q_h_expected, q_h_targets)\n",
        "\n",
        "    delta_a = q_a_expected - q_a_targets\n",
        "    delta_h = q_h_expected - q_h_targets\n",
        "\n",
        "    priorities_a = (delta_a.abs().cpu().detach().numpy().flatten())\n",
        "    priorities_h = (delta_h.abs().cpu().detach().numpy().flatten())\n",
        "    priorities = priorities_a / (priorities_a + priorities_h)\n",
        "    self.memory.update_priority(info['index'], priorities)\n",
        "\n",
        "    # backpropogating the error to update the weights\n",
        "    self.a_optimizer.zero_grad()\n",
        "    self.h_optimizer.zero_grad()\n",
        "\n",
        "    loss_a.backward()\n",
        "    loss_h.backward()\n",
        "\n",
        "    max_grad_norm = 0.5\n",
        "    clip_grad_norm_(self.a_local_qnetwork.parameters(), max_grad_norm)\n",
        "    clip_grad_norm_(self.h_local_qnetwork.parameters(), max_grad_norm)\n",
        "\n",
        "    # single optimization step for updating the weights\n",
        "    self.a_optimizer.step()\n",
        "    self.h_optimizer.step()\n",
        "\n",
        "    # updating the target network weights\n",
        "    self.soft_update(self.a_local_qnetwork, self.a_target_qnetwork, interpolation_parameter)\n",
        "    self.soft_update(self.h_local_qnetwork, self.h_target_qnetwork, interpolation_parameter)\n",
        "\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    for target_param, local_param in zip(target_model.parameters(),local_model.parameters()):\n",
        "      # softly update the target model parameters with the weighted average of the local and target params\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
      ],
      "metadata": {
        "id": "5DKT_S_rHsBb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing DQN class\n",
        "\n"
      ],
      "metadata": {
        "id": "cdztmUgfdbmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(state_size, action_size)"
      ],
      "metadata": {
        "id": "9Y-aVh2VqHgD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = 0.2\n",
        "stale = 0\n",
        "gamma = 0.5\n",
        "cost = 0\n",
        "# setup up the environment\n",
        "env = MiningEnv(p=p, stale=stale, gamma = gamma, cost = cost, cutoff=20)"
      ],
      "metadata": {
        "id": "aQVwhmUgYnRR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_episodes = deque(maxlen=400)\n",
        "v_s = np.zeros(env.S)\n",
        "state = env.reset()\n",
        "v_s[env.states_inverted[to_state_obj(state)]] += 1\n",
        "a_score = 0\n",
        "h_score = 0\n",
        "number_episodes = 1000000\n",
        "\n",
        "performance_window = 5\n",
        "performance_scores = []\n",
        "evaluation_interval = 1000\n",
        "last_avg_reward = None\n",
        "T = 10\n",
        "improvement_threshold=0.0002\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "  eps_t = np.exp(-1 * v_s[env.states_inverted[to_state_obj(state)]] / T)\n",
        "  action = agent.act(state, eps_t)\n",
        "  next_state, reward_a, reward_h = env.step(action)\n",
        "  agent.step(state, action, reward_a, reward_h, next_state)\n",
        "  state = next_state\n",
        "  v_s[env.states_inverted[to_state_obj(state)]] += 1\n",
        "  a_score += reward_a\n",
        "  h_score += reward_h\n",
        "  if episode % 1000 == 0:\n",
        "    print(a_score, h_score)\n",
        "    rel = a_score/(a_score+h_score)\n",
        "    scores_episodes.append(rel)\n",
        "    # performance_scores.append(rel)\n",
        "    # if len(performance_scores) > performance_window:\n",
        "    #   performance_scores.pop(0)\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(episode,rel))\n",
        "    # print('T: ', T)\n",
        "\n",
        "  # if episode % evaluation_interval == 0:\n",
        "  #   if len(performance_scores) == performance_window:\n",
        "  #     avg_reward = np.mean(performance_scores)\n",
        "  #     if last_avg_reward is not None:\n",
        "  #       improvement = avg_reward - last_avg_reward\n",
        "  #       if improvement < improvement_threshold:\n",
        "  #         T = min(1000, T * 1.05)\n",
        "  #       else:\n",
        "  #         T = max(10, T * 0.95)\n",
        "  #     last_avg_reward = avg_reward\n",
        "  # temperature = max(min_temperature, temperature * temperature_decay)\n",
        "np.save(\"dqn_revenues_no_clip.npy\", np.array(scores_episodes))"
      ],
      "metadata": {
        "id": "t66n8I7NXk3i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "da46a839-795b-4e73-e24b-4b5b3d908e8c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71.0 771.0\n",
            "Episode 1000\tAverage Score: 0.0843\n",
            "254.0 1544.0\n",
            "Episode 2000\tAverage Score: 0.1413\n",
            "407.0 2312.0\n",
            "Episode 3000\tAverage Score: 0.1497\n",
            "588.0 3058.0\n",
            "Episode 4000\tAverage Score: 0.1613\n",
            "781.0 3853.0\n",
            "Episode 5000\tAverage Score: 0.1685\n",
            "970.0 4651.0\n",
            "Episode 6000\tAverage Score: 0.1726\n",
            "1145.0 5428.0\n",
            "Episode 7000\tAverage Score: 0.1742\n",
            "1334.0 6223.0\n",
            "Episode 8000\tAverage Score: 0.1765\n",
            "1506.0 7025.0\n",
            "Episode 9000\tAverage Score: 0.1765\n",
            "1698.0 7821.0\n",
            "Episode 10000\tAverage Score: 0.1784\n",
            "1873.0 8602.0\n",
            "Episode 11000\tAverage Score: 0.1788\n",
            "2029.0 9383.0\n",
            "Episode 12000\tAverage Score: 0.1778\n",
            "2219.0 10176.0\n",
            "Episode 13000\tAverage Score: 0.1790\n",
            "2412.0 10963.0\n",
            "Episode 14000\tAverage Score: 0.1803\n",
            "2609.0 11761.0\n",
            "Episode 15000\tAverage Score: 0.1816\n",
            "2778.0 12562.0\n",
            "Episode 16000\tAverage Score: 0.1811\n",
            "2984.0 13339.0\n",
            "Episode 17000\tAverage Score: 0.1828\n",
            "3167.0 14127.0\n",
            "Episode 18000\tAverage Score: 0.1831\n",
            "3330.0 14930.0\n",
            "Episode 19000\tAverage Score: 0.1824\n",
            "3523.0 15737.0\n",
            "Episode 20000\tAverage Score: 0.1829\n",
            "3741.0 16519.0\n",
            "Episode 21000\tAverage Score: 0.1846\n",
            "3948.0 17298.0\n",
            "Episode 22000\tAverage Score: 0.1858\n",
            "4131.0 18069.0\n",
            "Episode 23000\tAverage Score: 0.1861\n",
            "4331.0 18839.0\n",
            "Episode 24000\tAverage Score: 0.1869\n",
            "4511.0 19633.0\n",
            "Episode 25000\tAverage Score: 0.1868\n",
            "4698.0 20419.0\n",
            "Episode 26000\tAverage Score: 0.1870\n",
            "4883.0 21207.0\n",
            "Episode 27000\tAverage Score: 0.1872\n",
            "5073.0 21975.0\n",
            "Episode 28000\tAverage Score: 0.1876\n",
            "5264.0 22762.0\n",
            "Episode 29000\tAverage Score: 0.1878\n",
            "5464.0 23515.0\n",
            "Episode 30000\tAverage Score: 0.1886\n",
            "5655.0 24301.0\n",
            "Episode 31000\tAverage Score: 0.1888\n",
            "5850.0 25097.0\n",
            "Episode 32000\tAverage Score: 0.1890\n",
            "6059.0 25880.0\n",
            "Episode 33000\tAverage Score: 0.1897\n",
            "6249.0 26677.0\n",
            "Episode 34000\tAverage Score: 0.1898\n",
            "6447.0 27472.0\n",
            "Episode 35000\tAverage Score: 0.1901\n",
            "6646.0 28270.0\n",
            "Episode 36000\tAverage Score: 0.1903\n",
            "6812.0 29068.0\n",
            "Episode 37000\tAverage Score: 0.1899\n",
            "7002.0 29877.0\n",
            "Episode 38000\tAverage Score: 0.1899\n",
            "7188.0 30681.0\n",
            "Episode 39000\tAverage Score: 0.1898\n",
            "7395.0 31462.0\n",
            "Episode 40000\tAverage Score: 0.1903\n",
            "7597.0 32244.0\n",
            "Episode 41000\tAverage Score: 0.1907\n",
            "7788.0 33049.0\n",
            "Episode 42000\tAverage Score: 0.1907\n",
            "7981.0 33846.0\n",
            "Episode 43000\tAverage Score: 0.1908\n",
            "8177.0 34642.0\n",
            "Episode 44000\tAverage Score: 0.1910\n",
            "8368.0 35425.0\n",
            "Episode 45000\tAverage Score: 0.1911\n",
            "8547.0 36246.0\n",
            "Episode 46000\tAverage Score: 0.1908\n",
            "8736.0 37011.0\n",
            "Episode 47000\tAverage Score: 0.1910\n",
            "8944.0 37801.0\n",
            "Episode 48000\tAverage Score: 0.1913\n",
            "9135.0 38595.0\n",
            "Episode 49000\tAverage Score: 0.1914\n",
            "9305.0 39390.0\n",
            "Episode 50000\tAverage Score: 0.1911\n",
            "9447.0 40183.0\n",
            "Episode 51000\tAverage Score: 0.1903\n",
            "9642.0 40980.0\n",
            "Episode 52000\tAverage Score: 0.1905\n",
            "9808.0 41758.0\n",
            "Episode 53000\tAverage Score: 0.1902\n",
            "10009.0 42517.0\n",
            "Episode 54000\tAverage Score: 0.1906\n",
            "10203.0 43323.0\n",
            "Episode 55000\tAverage Score: 0.1906\n",
            "10389.0 44122.0\n",
            "Episode 56000\tAverage Score: 0.1906\n",
            "10576.0 44915.0\n",
            "Episode 57000\tAverage Score: 0.1906\n",
            "10764.0 45712.0\n",
            "Episode 58000\tAverage Score: 0.1906\n",
            "10961.0 46512.0\n",
            "Episode 59000\tAverage Score: 0.1907\n",
            "11169.0 47277.0\n",
            "Episode 60000\tAverage Score: 0.1911\n",
            "11357.0 48049.0\n",
            "Episode 61000\tAverage Score: 0.1912\n",
            "11559.0 48833.0\n",
            "Episode 62000\tAverage Score: 0.1914\n",
            "11746.0 49612.0\n",
            "Episode 63000\tAverage Score: 0.1914\n",
            "11949.0 50380.0\n",
            "Episode 64000\tAverage Score: 0.1917\n",
            "12120.0 51152.0\n",
            "Episode 65000\tAverage Score: 0.1916\n",
            "12336.0 51902.0\n",
            "Episode 66000\tAverage Score: 0.1920\n",
            "12540.0 52698.0\n",
            "Episode 67000\tAverage Score: 0.1922\n",
            "12745.0 53493.0\n",
            "Episode 68000\tAverage Score: 0.1924\n",
            "12936.0 54285.0\n",
            "Episode 69000\tAverage Score: 0.1924\n",
            "13129.0 55057.0\n",
            "Episode 70000\tAverage Score: 0.1925\n",
            "13321.0 55839.0\n",
            "Episode 71000\tAverage Score: 0.1926\n",
            "13521.0 56624.0\n",
            "Episode 72000\tAverage Score: 0.1928\n",
            "13700.0 57406.0\n",
            "Episode 73000\tAverage Score: 0.1927\n",
            "13893.0 58200.0\n",
            "Episode 74000\tAverage Score: 0.1927\n",
            "14059.0 58944.0\n",
            "Episode 75000\tAverage Score: 0.1926\n",
            "14240.0 59717.0\n",
            "Episode 76000\tAverage Score: 0.1925\n",
            "14445.0 60489.0\n",
            "Episode 77000\tAverage Score: 0.1928\n",
            "14643.0 61272.0\n",
            "Episode 78000\tAverage Score: 0.1929\n",
            "14798.0 62097.0\n",
            "Episode 79000\tAverage Score: 0.1924\n",
            "15006.0 62869.0\n",
            "Episode 80000\tAverage Score: 0.1927\n",
            "15184.0 63637.0\n",
            "Episode 81000\tAverage Score: 0.1926\n",
            "15366.0 64379.0\n",
            "Episode 82000\tAverage Score: 0.1927\n",
            "15573.0 65139.0\n",
            "Episode 83000\tAverage Score: 0.1929\n",
            "15746.0 65950.0\n",
            "Episode 84000\tAverage Score: 0.1927\n",
            "15939.0 66734.0\n",
            "Episode 85000\tAverage Score: 0.1928\n",
            "16136.0 67509.0\n",
            "Episode 86000\tAverage Score: 0.1929\n",
            "16357.0 68278.0\n",
            "Episode 87000\tAverage Score: 0.1933\n",
            "16534.0 69079.0\n",
            "Episode 88000\tAverage Score: 0.1931\n",
            "16699.0 69864.0\n",
            "Episode 89000\tAverage Score: 0.1929\n",
            "16891.0 70627.0\n",
            "Episode 90000\tAverage Score: 0.1930\n",
            "17087.0 71387.0\n",
            "Episode 91000\tAverage Score: 0.1931\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-29239504f16a>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_episodes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0meps_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates_inverted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_state_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ac389e88d315>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, eps_t)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mpn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0meps_t\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mvalid_actions_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_actions\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_actions_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tabular Q-Learning"
      ],
      "metadata": {
        "id": "PrHIv27oP2F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = 0.45\n",
        "stale = 0\n",
        "gamma = 1\n",
        "cost = 0\n",
        "# setup up the environment\n",
        "env = MiningEnv(p=p, stale=stale, gamma = gamma, cost = cost, cutoff=20)"
      ],
      "metadata": {
        "id": "Y2KLCrenRu1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearning():\n",
        "\n",
        "  def __init__(self, t_eps, learning_rate, discount_factor=0.999, n_iter=350000):\n",
        "    self.n_iter = int(n_iter)\n",
        "    self.S = env.S\n",
        "    self.A = env.A\n",
        "    self.learning_rate = learning_rate\n",
        "    self.discount_factor = discount_factor\n",
        "    self.t_eps = t_eps\n",
        "    self.Q_a = np.zeros((self.S, self.A))\n",
        "    self.Q_h = np.zeros((self.S, self.A))\n",
        "    self.V_s = np.zeros(self.S)\n",
        "\n",
        "\n",
        "  def run(self):\n",
        "    state = env.reset()\n",
        "    scores_episodes = deque(maxlen=350)\n",
        "    self.V_s[env.states_inverted[to_state_obj(state)]] += 1\n",
        "    a_score = 0\n",
        "    h_score = 0\n",
        "    for t in range(1, self.n_iter + 1):\n",
        "      eps = np.exp(-1 * self.V_s[env.states_inverted[to_state_obj(state)]] / self.t_eps)\n",
        "      pn = np.random.random()\n",
        "      a_t = self.Q_a[env.states_inverted[to_state_obj(state)]] / (self.Q_a[env.states_inverted[to_state_obj(state)]] + self.Q_h[env.states_inverted[to_state_obj(state)]])\n",
        "      a_t = np.nan_to_num(a_t, nan=0.0)\n",
        "      mask_value = float(\"-inf\")\n",
        "      diff = list(set([0, 1, 2, 3]) - set(get_allowed_actions(torch.tensor(state), 20)))\n",
        "      if diff:\n",
        "        a_t[diff] = mask_value\n",
        "      if pn < eps:\n",
        "        valid_actions_indices = np.where(a_t > float('-inf'))[0]\n",
        "        act = np.random.choice(valid_actions_indices)\n",
        "      else:\n",
        "        act = a_t.argmax()\n",
        "      next_state, reward_a, reward_h = env.step(act)\n",
        "      a_ = self.Q_a[env.states_inverted[to_state_obj(next_state)]] / (self.Q_a[env.states_inverted[to_state_obj(next_state)]] + self.Q_h[env.states_inverted[to_state_obj(next_state)]])\n",
        "      a_ = a_.argmax()\n",
        "      self.Q_a[env.states_inverted[to_state_obj(state)], act] = (1-self.learning_rate) * self.Q_a[env.states_inverted[to_state_obj(state)], act] + self.learning_rate * (reward_a + self.discount_factor * self.Q_a[env.states_inverted[to_state_obj(next_state)], a_])\n",
        "      self.Q_h[env.states_inverted[to_state_obj(state)], act] = (1-self.learning_rate) * self.Q_h[env.states_inverted[to_state_obj(state)], act] + self.learning_rate * (reward_h + self.discount_factor * self.Q_h[env.states_inverted[to_state_obj(next_state)], a_])\n",
        "      state = next_state\n",
        "      self.V_s[env.states_inverted[to_state_obj(state)]] += 1\n",
        "      a_score += reward_a\n",
        "      h_score += reward_h\n",
        "      if t % 1000 == 0:\n",
        "        print(a_score, h_score)\n",
        "        rel = a_score/(a_score+h_score)\n",
        "        scores_episodes.append(rel)\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(t,rel))\n",
        "        print('eps: ', eps)\n",
        "    np.save(\"tabular_revenues %.5flr%dteps.npy\" % (self.learning_rate, self.t_eps), np.array(scores_episodes))\n",
        "\n"
      ],
      "metadata": {
        "id": "SrYblD5K4f8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = QLearning(t_eps=50, learning_rate=5e-5)"
      ],
      "metadata": {
        "id": "C3NTtkHFX6-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run()"
      ],
      "metadata": {
        "id": "F10lnCeZZaIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tab_revenues = np.load('tab_revenues.npy')\n",
        "rl_revenues = np.load('dqn_revenues_no_clip.npy')\n",
        "plt.plot(tab_revenues[:len(rl_revenues)])\n",
        "plt.plot(rl_revenues)"
      ],
      "metadata": {
        "id": "2SP4clsqG_1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [5e-2, 5e-3, 5e-5]\n",
        "t_eps = [10, 100, 1000]\n",
        "dqn_revenues = np.load('dqn_revenues.npy')\n",
        "plt.plot(timesteps, dqn_revenues[:-1], label='DRL Mining')\n",
        "\n",
        "timesteps = np.arange(1, len(dqn_revenues[:-1]) + 1)\n",
        "\n",
        "\n",
        "for lr in learning_rates:\n",
        "  for t in t_eps:\n",
        "    f_name = 'tabular_revenues {:.5f}lr{}teps.npy'.format(lr, t)\n",
        "    try:\n",
        "      revenue = np.load(f_name)\n",
        "      plt.plot(timesteps, revenue, label=f'Original RL (lr={lr}, t_eps={t})')\n",
        "    except:\n",
        "      pass\n",
        "plt.legend(loc=0)\n",
        "plt.xlabel('Timesteps')\n",
        "plt.ylabel('Relative Gain')\n",
        "plt.title(r'Relative Mining Gain $\\alpha$=0.45, $\\gamma$=1, $c$=0')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZaLRjmePb00g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, initial_tau=1.0, min_tau=1e-4, max_tau=2.0, initial_decay_rate=0.999, performance_window=1000, improvement_threshold=0.005):\n",
        "        self.tau = initial_tau\n",
        "        self.min_tau = min_tau\n",
        "        self.max_tau = max_tau  # Maximum temperature for exploration\n",
        "        self.decay_rate = initial_decay_rate\n",
        "        self.performance_window = performance_window\n",
        "        self.improvement_threshold = improvement_threshold\n",
        "        self.rewards = []\n",
        "        self.last_avg_reward = None\n",
        "\n",
        "    def select_action(self, q_values):\n",
        "        exp_q = np.exp(q_values / self.tau)\n",
        "        probabilities = exp_q / np.sum(exp_q)\n",
        "        action = np.random.choice(len(q_values), p=probabilities)\n",
        "        return action\n",
        "\n",
        "    def update_tau(self):\n",
        "        # Ensure tau does not decay too aggressively\n",
        "        self.tau = max(self.min_tau, min(self.max_tau, self.tau * self.decay_rate))\n",
        "\n",
        "    def update_performance(self, reward):\n",
        "        self.rewards.append(reward)\n",
        "        if len(self.rewards) > self.performance_window:\n",
        "            self.rewards.pop(0)\n",
        "        if len(self.rewards) == self.performance_window:\n",
        "            avg_reward = np.mean(self.rewards)\n",
        "            if self.last_avg_reward is not None:\n",
        "                improvement = avg_reward - self.last_avg_reward\n",
        "                if improvement < self.improvement_threshold:\n",
        "                    # Increase tau directly to promote exploration\n",
        "                    self.tau = min(self.max_tau, self.tau * 1.05)  # Increase tau to promote exploration\n",
        "                    # Decrease decay rate to slow down temperature decay\n",
        "                    self.decay_rate = max(0.99, self.decay_rate * 0.95)\n",
        "                else:\n",
        "                    # Decrease tau to promote exploitation\n",
        "                    self.tau = max(self.min_tau, self.tau * 0.95)  # Decrease tau to promote exploitation\n",
        "                    # Increase decay rate to speed up temperature decay\n",
        "                    self.decay_rate = min(0.9995, self.decay_rate * 1.05)\n",
        "            self.last_avg_reward = avg_reward\n",
        "\n",
        "# Simulation of temperature decay with performance evaluation every 1000 steps\n",
        "num_steps = 300000\n",
        "agent = Agent(initial_tau=1.0, min_tau=1e-4, max_tau=2.0, initial_decay_rate=0.999, performance_window=1000, improvement_threshold=0.005)\n",
        "\n",
        "# Assuming there is no improvement in performance, we can simulate constant rewards\n",
        "constant_reward = 1.0\n",
        "\n",
        "for step in range(num_steps):\n",
        "    # Simulate constant reward without improvement\n",
        "    agent.update_performance(constant_reward)\n",
        "\n",
        "    if (step + 1) % 1000 == 0:\n",
        "        agent.update_tau()\n",
        "        # Print or log tau occasionally to monitor decay\n",
        "        print(f'Step {step + 1}, Tau: {agent.tau:.6f}, Decay Rate: {agent.decay_rate:.6f}')\n",
        "\n",
        "# Final values after simulation\n",
        "print(f'Final Tau: {agent.tau:.6f}')\n",
        "print(f'Final Decay Rate: {agent.decay_rate:.6f}')\n"
      ],
      "metadata": {
        "id": "1u24aSf4CWZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}