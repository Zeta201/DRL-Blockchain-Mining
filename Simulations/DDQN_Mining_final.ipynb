{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-nWnnHFjJAZ",
        "outputId": "d59dfff9-5840-4321-de9a-f387035171ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymdptoolbox\n",
            "  Downloading pymdptoolbox-4.0-b3.zip (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.11.4)\n",
            "Building wheels for collected packages: pymdptoolbox\n",
            "  Building wheel for pymdptoolbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymdptoolbox: filename=pymdptoolbox-4.0b3-py3-none-any.whl size=25656 sha256=60096ef03e951c4c2dca99f55b4d400b912bd28eaee64ce9bce2bd0ac701ce16\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/e7/c7/d7abf9e309f3573a934fed2750c70bd75d9e9d901f7f16e183\n",
            "Successfully built pymdptoolbox\n",
            "Installing collected packages: pymdptoolbox\n",
            "Successfully installed pymdptoolbox-4.0b3\n"
          ]
        }
      ],
      "source": [
        "!pip install pymdptoolbox\n",
        "# !pip install torchrl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTAbkptTbGGp"
      },
      "source": [
        "# Mining Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VRvLRKcZjNUy"
      },
      "outputs": [],
      "source": [
        "import mdptoolbox, mdptoolbox.example\n",
        "import numpy as np\n",
        "import enum\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple\n",
        "# from torchrl.data import ListStorage, PrioritizedReplayBuffer\n",
        "# from torch.nn.utils import clip_grad_norm_\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Jkfteott9o-"
      },
      "outputs": [],
      "source": [
        "match_dict_inv = {0: 'irrelevant', 1: 'relevant', 2: 'active'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dsc7WuVG8IRg"
      },
      "outputs": [],
      "source": [
        "class State:\n",
        "    def __init__(self, l_a, l_h, b_e, match=\"relevant\"):\n",
        "        self.length_a = l_a\n",
        "        self.length_h = l_h\n",
        "        self.blocks_e = b_e\n",
        "        self.match = match\n",
        "        self.match_dict = {'irrelevant': 0, 'relevant': 1, 'active': 2}\n",
        "\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash((self.length_a, self.length_h, self.blocks_e, self.match))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        try:\n",
        "            return (self.length_a, self.length_h, self.blocks_e, self.match) == (other.length_a, other.length_h, other.blocks_e, other.match)\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def __ne__(self, other):\n",
        "        return not (self == other)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"(%d, %d, %d, %s)\" % (self.length_a, self.length_h, self.blocks_e, self.match)\n",
        "\n",
        "    def to_numpy(self):\n",
        "      return np.array([self.length_a, self.length_h, self.blocks_e, self.match_dict[self.match]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HVLwdBP00p9F"
      },
      "outputs": [],
      "source": [
        "class Match(enum.IntEnum):\n",
        "  irrelevant = 0,\n",
        "  relevant = 1,\n",
        "  active = 2\n",
        "\n",
        "class Action(enum.IntEnum):\n",
        "  wait = 0,\n",
        "  adopt = 1,\n",
        "  override = 2,\n",
        "  match = 3\n",
        "\n",
        "class MinerType(enum.IntEnum):\n",
        "  honest = 0,\n",
        "  adversary = 1\n",
        "\n",
        "\n",
        "action_size = 4\n",
        "state_size = 4\n",
        "\n",
        "def to_state_obj(state):\n",
        "  return State(state[0], state[1], state[2], match_dict_inv[state[3]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class Miner():\n",
        "#   def __init__(self, id, type, mines_on):\n",
        "#     self.id = id\n",
        "#     self.type = type\n",
        "#     self.mines_on = mines_on\n",
        "#     self.eclipsed = eclipsed\n",
        "\n",
        "#   def __repr__(self):\n",
        "#         return \"(%d, %s, %s)\" % (self.id, self.type, self.mines_on)"
      ],
      "metadata": {
        "id": "l9OmB_4IeZAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class BitcoinSimulatorV2():\n",
        "#   def __init__(self, p, stale, gamma, cost=0, cutoff=20, lam=0):\n",
        "#     self.p = p\n",
        "#     self.stale = stale\n",
        "#     self.gamma = gamma\n",
        "#     self.cost = cost\n",
        "#     self.cutoff = cutoff\n",
        "#     self.lam = lam\n",
        "#     self.match_cases = [\"irrelevant\", \"relevant\", \"active\"]\n",
        "#     self.a_cost = cost * p\n",
        "#     self.h_cost = cost * (1 - p)\n",
        "#     self.q = 1 - p - lam\n",
        "#     self.l_a = 0\n",
        "#     self.l_h = 0\n",
        "#     self.b_e = 0\n",
        "#     self.fork = None\n",
        "#     self.agent = None\n",
        "\n",
        "#     state_count = 0\n",
        "#     self.states = {}\n",
        "#     self.states_inverted = {}\n",
        "#     for l_a in range(self.cutoff + 1):\n",
        "#       for l_h in range(self.cutoff + 1):\n",
        "#         for b_e in range(l_a + 1):\n",
        "#           if self.lam == 0 and b_e > 0:\n",
        "#             continue\n",
        "#           for match in self.match_cases:\n",
        "#             state = State(l_a, l_h, b_e, match)\n",
        "#             self.states[state_count] = state\n",
        "#             self.states_inverted[state] = state_count\n",
        "#             state_count += 1\n",
        "#     self.states_counter = state_count\n",
        "\n",
        "#   def update_blockchain_state(self, state):\n",
        "#     self.l_a = state.length_a\n",
        "#     self.l_h = state.length_h\n",
        "#     self.b_e = state.blocks_e\n",
        "#     self.fork = state.match\n",
        "\n",
        "#   def get_curr_state(self):\n",
        "#     return State(self.l_a, self.l_h, self.b_e, self.fork).to_numpy()\n",
        "\n",
        "#   def simulate(self, mining_events=1):\n",
        "#     self.reset()\n",
        "#     probs = [self.p, self.lam, self.q * (1 - self.stale), self.q * self.stale]\n",
        "#     for mining in range(1, mining_events+1):\n",
        "#       miners = ['adversary', 'honest']\n",
        "#       event = np.random.choice(miners, p=probs)\n",
        "#       if event == 'adversary':\n",
        "#         # get current state\n",
        "#         curr_state = self.get_curr_state()\n",
        "#         # perform the action\n",
        "#       elif event == 'honest':\n",
        "#         if self.l_a < self.l_h:\n",
        "#           action = Action.adopt\n",
        "#         elif self.l_a == self.l_h:\n",
        "#           action = Action.wait\n",
        "#         elif self.l_a > self.l_h:\n",
        "#           action = Action.override\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     # states = [State(1, 0, 0, 'irrelevant'), State(1, 0, 1, 'irrelevant'), State(0, 1, 0, 'relevant'), State(0, 0, 0, 'irrelevant')]\n",
        "#     # self.set_blockchain_state(np.random.choice(states, p=probs))\n",
        "#     # return self.current_state.to_numpy()\n",
        "\n",
        "\n",
        "#   # def get_curr_state():\n",
        "#   #   return self.current_state.to_numpy()\n",
        "\n",
        "#   # def set_blockchain_state(self, state):\n",
        "#   #   self.current_state = state\n",
        "#   #   self.l_a = state.length_a\n",
        "#   #   self.l_h = state.length_h\n",
        "#   #   self.b_e = state.blocks_e\n",
        "#   #   self.match = state.match\n",
        "\n",
        "#   def reset(self):\n",
        "#     self.update_blockchain_state(State(0, 0, 0, 'irrelevant'))\n",
        "\n",
        "#   # def step(self, action):\n",
        "#   #   if action == 0:\n",
        "#   #     if self.match == 'active' and self.l_a >= self.l_h and self.l_h > 0:\n",
        "#   #       payout = (self.l_h)*(self.l_a - self.b_e)//self.l_a\n",
        "#   #       new_b_e = self.b_e - (self.l_h - payout)\n",
        "#   #       wait_probs = [self.p, self.lam, self.gamma * self.q * (1 - self.stale), (1-self.gamma) * self.q * (1 - self.stale), self.q * self.stale]\n",
        "#   #       wait_reward_states = [((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e, \"active\")), ((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e+1, \"active\")), ((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a - self.l_h, 1, new_b_e, \"relevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h + 1, self.b_e, \"relevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h, self.b_e, \"active\"))]\n",
        "#   #       next_reward_state_idx = np.random.choice(range(len(wait_reward_states)), p=wait_probs)\n",
        "#   #       next_reward_state = wait_reward_states[next_reward_state_idx]\n",
        "#   #       self.set_blockchain_state(next_reward_state[1])\n",
        "#   #       return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n",
        "#   #     else:\n",
        "#   #       wait_probs = [self.p, self.lam, self.q * (1 - self.stale), self.q * self.stale]\n",
        "#   #       wait_reward_states = [((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e, \"irrelevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e+1, \"irrelevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h + 1, self.b_e, \"relevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h, self.b_e, \"irrelevant\"))]\n",
        "#   #       next_reward_state_idx = np.random.choice(range(len(wait_reward_states)), p=wait_probs)\n",
        "#   #       next_reward_state = wait_reward_states[next_reward_state_idx]\n",
        "#   #       self.set_blockchain_state(next_reward_state[1])\n",
        "#   #       return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n",
        "\n",
        "#   #   elif action == 1:\n",
        "#   #     adopt_probs = [self.p, self.lam, self.q * (1 - self.stale), self.q * self.stale]\n",
        "#   #     adopt_reward_states = [((-self.a_cost, self.l_h - self.h_cost), State(1, 0, 0, \"irrelevant\")), ((-self.a_cost, self.l_h - self.h_cost), State(1, 0, 1, \"irrelevant\")), ((-self.a_cost, self.l_h - self.h_cost), State(0, 1, 0, \"relevant\")), ((-self.a_cost, self.l_h - self.h_cost), State(0, 0, 0, \"irrelevant\"))]\n",
        "#   #     next_reward_state_idx = np.random.choice(range(len(adopt_reward_states)), p=adopt_probs)\n",
        "#   #     next_reward_state = adopt_reward_states[next_reward_state_idx]\n",
        "#   #     self.set_blockchain_state(next_reward_state[1])\n",
        "#   #     return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n",
        "\n",
        "#   #   elif action == 2:\n",
        "#   #     payout = (self.l_h+1)*(self.l_a - self.b_e)//self.l_a\n",
        "#   #     new_b_e = self.b_e - (self.l_h+1 - payout)\n",
        "#   #     override_probs = [self.p, self.lam, self.q * (1 - self.stale), self.q * self.stale]\n",
        "#   #     override_reward_states = [((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a - self.l_h, 0, new_b_e, \"irrelevant\")),((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a - self.l_h, 0, new_b_e + 1, \"irrelevant\")), ((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a-self.l_h-1, 1, new_b_e, \"relevant\")), ((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a-self.l_h-1, 0, new_b_e, \"irrelevant\"))]\n",
        "#   #     next_reward_state_idx = np.random.choice(range(len(override_reward_states)), p=override_probs)\n",
        "#   #     next_reward_state = override_reward_states[next_reward_state_idx]\n",
        "#   #     self.set_blockchain_state(next_reward_state[1])\n",
        "#   #     return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n",
        "\n",
        "#   #   elif action == 3:\n",
        "#   #     payout = (self.l_h)*(self.l_a - self.b_e)//self.l_a\n",
        "#   #     new_b_e = self.b_e - (self.l_h - payout)\n",
        "#   #     match_probs = [self.p, self.lam, self.gamma * self.q * (1 - self.stale), (1-self.gamma) * self.q * (1 - self.stale), self.q * self.stale]\n",
        "#   #     match_reward_states = [((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e, \"active\")),((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e+1, \"active\")), ((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a - self.l_h, 1, new_b_e, \"relevant\")),((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h + 1, self.b_e, \"relevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h, self.b_e, \"active\"))]\n",
        "#   #     next_reward_state_idx = np.random.choice(range(len(match_reward_states)), p=match_probs)\n",
        "#   #     next_reward_state = match_reward_states[next_reward_state_idx]\n",
        "#   #     self.set_blockchain_state(next_reward_state[1])\n",
        "#   #     return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n"
      ],
      "metadata": {
        "id": "xLFPM-7hEtQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# p = 0.45\n",
        "# stale = 0\n",
        "# gamma = 1\n",
        "# cost = 0\n",
        "# # setup up the environment\n",
        "# env = BitcoinSimulatorV2(p=p, stale=stale, gamma = gamma, cost = cost, cutoff=30)\n",
        "# env.simulate()"
      ],
      "metadata": {
        "id": "N7jjbVW5KnQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# block_discovery_rate = 1 / 600  # Average rate of block discovery (e.g., one block per 10 minutes)\n",
        "# honest_blocks_found = np.random.poisson(hash_rates[honest_miners].sum() * block_discovery_rate)\n"
      ],
      "metadata": {
        "id": "0oG3QuZxooIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class BitcoinSimulator():\n",
        "#   def __init__(self, p, stale, gamma, cost=0, cutoff=20, lam=0):\n",
        "#     self.p = p\n",
        "#     self.stale = stale\n",
        "#     self.gamma = gamma\n",
        "#     self.cost = cost\n",
        "#     self.cutoff = cutoff\n",
        "#     self.lam = lam\n",
        "#     self.match_cases = [\"irrelevant\", \"relevant\", \"active\"]\n",
        "#     self.a_cost = cost * p\n",
        "#     self.h_cost = cost * (1 - p)\n",
        "#     self.q = 1 - p - lam\n",
        "#     state_count = 0\n",
        "#     self.states = {}\n",
        "#     self.states_inverted = {}\n",
        "#     for l_a in range(self.cutoff + 1):\n",
        "#       for l_h in range(self.cutoff + 1):\n",
        "#         for b_e in range(l_a + 1):\n",
        "#           if self.lam == 0 and b_e > 0:\n",
        "#             continue\n",
        "#           for match in self.match_cases:\n",
        "#             state = State(l_a, l_h, b_e, match)\n",
        "#             self.states[state_count] = state\n",
        "#             self.states_inverted[state] = state_count\n",
        "#             state_count += 1\n",
        "#     self.states_counter = state_count\n",
        "\n",
        "#   def get_curr_state():\n",
        "#     return self.current_state.to_numpy()\n",
        "\n",
        "#   def set_blockchain_state(self, state):\n",
        "#     self.current_state = state\n",
        "#     self.l_a = state.length_a\n",
        "#     self.l_h = state.length_h\n",
        "#     self.b_e = state.blocks_e\n",
        "#     self.match = state.match\n",
        "\n",
        "#   def reset(self):\n",
        "#     probs = [self.p, self.lam, self.q * (1 - self.stale), self.q * self.stale]\n",
        "#     states = [State(1, 0, 0, 'irrelevant'), State(1, 0, 1, 'irrelevant'), State(0, 1, 0, 'relevant'), State(0, 0, 0, 'irrelevant')]\n",
        "#     self.set_blockchain_state(np.random.choice(states, p=probs))\n",
        "#     return self.current_state.to_numpy()\n",
        "\n",
        "#   def step(self, action):\n",
        "#     if action == 0:\n",
        "#       if self.match == 'active' and self.l_a >= self.l_h and self.l_h > 0:\n",
        "#         payout = (self.l_h)*(self.l_a - self.b_e)//self.l_a\n",
        "#         new_b_e = self.b_e - (self.l_h - payout)\n",
        "#         wait_probs = [self.p, self.lam, self.gamma * self.q * (1 - self.stale), (1-self.gamma) * self.q * (1 - self.stale), self.q * self.stale]\n",
        "#         wait_reward_states = [((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e, \"active\")), ((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e+1, \"active\")), ((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a - self.l_h, 1, new_b_e, \"relevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h + 1, self.b_e, \"relevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h, self.b_e, \"active\"))]\n",
        "#         next_reward_state_idx = np.random.choice(range(len(wait_reward_states)), p=wait_probs)\n",
        "#         next_reward_state = wait_reward_states[next_reward_state_idx]\n",
        "#         self.set_blockchain_state(next_reward_state[1])\n",
        "#         return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n",
        "#       else:\n",
        "#         wait_probs = [self.p, self.lam, self.q * (1 - self.stale), self.q * self.stale]\n",
        "#         wait_reward_states = [((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e, \"irrelevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e+1, \"irrelevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h + 1, self.b_e, \"relevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h, self.b_e, \"irrelevant\"))]\n",
        "#         next_reward_state_idx = np.random.choice(range(len(wait_reward_states)), p=wait_probs)\n",
        "#         next_reward_state = wait_reward_states[next_reward_state_idx]\n",
        "#         self.set_blockchain_state(next_reward_state[1])\n",
        "#         return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n",
        "\n",
        "#     elif action == 1:\n",
        "#       adopt_probs = [self.p, self.lam, self.q * (1 - self.stale), self.q * self.stale]\n",
        "#       adopt_reward_states = [((-self.a_cost, self.l_h - self.h_cost), State(1, 0, 0, \"irrelevant\")), ((-self.a_cost, self.l_h - self.h_cost), State(1, 0, 1, \"irrelevant\")), ((-self.a_cost, self.l_h - self.h_cost), State(0, 1, 0, \"relevant\")), ((-self.a_cost, self.l_h - self.h_cost), State(0, 0, 0, \"irrelevant\"))]\n",
        "#       next_reward_state_idx = np.random.choice(range(len(adopt_reward_states)), p=adopt_probs)\n",
        "#       next_reward_state = adopt_reward_states[next_reward_state_idx]\n",
        "#       self.set_blockchain_state(next_reward_state[1])\n",
        "#       return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n",
        "\n",
        "#     elif action == 2:\n",
        "#       payout = (self.l_h+1)*(self.l_a - self.b_e)//self.l_a\n",
        "#       new_b_e = self.b_e - (self.l_h+1 - payout)\n",
        "#       override_probs = [self.p, self.lam, self.q * (1 - self.stale), self.q * self.stale]\n",
        "#       override_reward_states = [((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a - self.l_h, 0, new_b_e, \"irrelevant\")),((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a - self.l_h, 0, new_b_e + 1, \"irrelevant\")), ((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a-self.l_h-1, 1, new_b_e, \"relevant\")), ((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a-self.l_h-1, 0, new_b_e, \"irrelevant\"))]\n",
        "#       next_reward_state_idx = np.random.choice(range(len(override_reward_states)), p=override_probs)\n",
        "#       next_reward_state = override_reward_states[next_reward_state_idx]\n",
        "#       self.set_blockchain_state(next_reward_state[1])\n",
        "#       return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n",
        "\n",
        "#     elif action == 3:\n",
        "#       payout = (self.l_h)*(self.l_a - self.b_e)//self.l_a\n",
        "#       new_b_e = self.b_e - (self.l_h - payout)\n",
        "#       match_probs = [self.p, self.lam, self.gamma * self.q * (1 - self.stale), (1-self.gamma) * self.q * (1 - self.stale), self.q * self.stale]\n",
        "#       match_reward_states = [((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e, \"active\")),((-self.a_cost, -self.h_cost), State(self.l_a + 1, self.l_h, self.b_e+1, \"active\")), ((payout - self.a_cost, self.b_e - new_b_e - self.h_cost), State(self.l_a - self.l_h, 1, new_b_e, \"relevant\")),((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h + 1, self.b_e, \"relevant\")), ((-self.a_cost, -self.h_cost), State(self.l_a, self.l_h, self.b_e, \"active\"))]\n",
        "#       next_reward_state_idx = np.random.choice(range(len(match_reward_states)), p=match_probs)\n",
        "#       next_reward_state = match_reward_states[next_reward_state_idx]\n",
        "#       self.set_blockchain_state(next_reward_state[1])\n",
        "#       return (self.current_state.to_numpy(), next_reward_state[0][0], next_reward_state[0][1])\n"
      ],
      "metadata": {
        "id": "Jlbso9QNxKyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M-CRaClLjUdz"
      },
      "outputs": [],
      "source": [
        "class MiningEnv():\n",
        "\n",
        "  def __init__(self, p, stale, gamma, cost, cutoff=20, lam=0):\n",
        "    self.p = p\n",
        "    self.cutoff = cutoff\n",
        "    self.lam = lam\n",
        "    self.match_cases = [\"irrelevant\", \"relevant\", \"active\"]\n",
        "    self.a_cost = cost * p\n",
        "    self.h_cost = cost * (1 - p)\n",
        "    self.q = 1 - p - lam\n",
        "    self.stale = stale\n",
        "    self.gamma = gamma\n",
        "    self.current_state = State(0, 0, 0, 'irrelevant')\n",
        "    state_count = 0\n",
        "    self.states = {}\n",
        "    self.states_inverted = {}\n",
        "    for l_a in range(self.cutoff + 1):\n",
        "      for l_h in range(self.cutoff + 1):\n",
        "        for b_e in range(l_a + 1):\n",
        "          if self.lam == 0 and b_e > 0:\n",
        "            continue\n",
        "          for match in self.match_cases:\n",
        "            state = State(l_a, l_h, b_e, match)\n",
        "            self.states[state_count] = state\n",
        "            self.states_inverted[state] = state_count\n",
        "            state_count += 1\n",
        "    self.states_counter = state_count\n",
        "\n",
        "    # transition matrices\n",
        "    P_adopt = np.zeros(shape=(state_count, state_count))\n",
        "    P_override = np.zeros(shape=(state_count, state_count))\n",
        "    P_match = np.zeros(shape=(state_count, state_count))\n",
        "    P_wait = np.zeros(shape=(state_count, state_count))\n",
        "\n",
        "    # reward matrices\n",
        "    R_adopt = np.empty(shape=(state_count, state_count), dtype=object)\n",
        "    R_override = np.empty(shape=(state_count, state_count), dtype=object)\n",
        "    R_match = np.empty(shape=(state_count, state_count), dtype=object)\n",
        "    R_wait = np.empty(shape=(state_count, state_count), dtype=object)\n",
        "\n",
        "    R_adopt.fill((0,0))\n",
        "    R_override.fill((0,0))\n",
        "    R_match.fill((0,0))\n",
        "    R_wait.fill((0,0))\n",
        "\n",
        "    for state_idx, state in self.states.items():\n",
        "      l_a = state.length_a\n",
        "      l_h = state.length_h\n",
        "      b_e = state.blocks_e\n",
        "      match = state.match\n",
        "\n",
        "      # adopt action transition matrix\n",
        "      # attacker mines next block\n",
        "      P_adopt[state_idx, self.states_inverted[State(1, 0, 0, \"irrelevant\")]] = self.p\n",
        "      R_adopt[state_idx, self.states_inverted[State(1, 0, 0, \"irrelevant\")]] = (-self.a_cost, l_h - self.h_cost)\n",
        "\n",
        "      # eclipsed node mines next block\n",
        "      if self.lam != 0:\n",
        "        P_adopt[state_idx, self.states_inverted[State(1, 0, 1, \"irrelevant\")]] = self.lam\n",
        "        R_adopt[state_idx, self.states_inverted[State(1, 0, 1, \"irrelevant\")]] = (-self.a_cost, l_h - self.h_cost)\n",
        "\n",
        "      # honest network mines next block\n",
        "      P_adopt[state_idx, self.states_inverted[State(0, 1, 0, \"relevant\")]] = self.q*(1-self.stale)\n",
        "      R_adopt[state_idx, self.states_inverted[State(0, 1, 0, \"relevant\")]] = (-self.a_cost, l_h - self.h_cost)\n",
        "\n",
        "      # network mines state block\n",
        "      P_adopt[state_idx, self.states_inverted[State(0, 0, 0, \"irrelevant\")]] = self.q * self.stale\n",
        "      R_adopt[state_idx, self.states_inverted[State(0, 0, 0, \"irrelevant\")]] = (-self.a_cost, l_h - self.h_cost)\n",
        "\n",
        "      # override action transition matrix\n",
        "      if l_a > l_h:\n",
        "        payout = (l_h+1)*(l_a - b_e)//l_a\n",
        "        new_b_e = b_e - (l_h+1 - payout)\n",
        "      # attacker mines next block\n",
        "        P_override[state_idx, self.states_inverted[State(l_a - l_h, 0, new_b_e, \"irrelevant\")]] = self.p\n",
        "        R_override[state_idx, self.states_inverted[State(l_a - l_h, 0, new_b_e, \"irrelevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "      # eclipsed node mines next block\n",
        "        if self.lam != 0:\n",
        "          P_override[state_idx, self.states_inverted[State(l_a - l_h, 0, new_b_e + 1, \"irrelevant\")]] = self.lam\n",
        "          R_override[state_idx, self.states_inverted[State(l_a - l_h, 0, new_b_e + 1, \"irrelevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "      # network mines next block\n",
        "        P_override[state_idx, self.states_inverted[State(l_a-l_h-1, 1, new_b_e, \"relevant\")]] = self.q*(1 - self.stale)\n",
        "        R_override[state_idx, self.states_inverted[State(l_a-l_h-1, 1, new_b_e, \"relevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "      # network mines stale block\n",
        "        P_override[state_idx, self.states_inverted[State(l_a-l_h-1, 0, new_b_e, \"irrelevant\")]] = self.q*self.stale\n",
        "        R_override[state_idx, self.states_inverted[State(l_a-l_h-1, 0, new_b_e, \"irrelevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "      else:\n",
        "        P_override[state_idx, state_idx] = 1\n",
        "        R_override[state_idx, state_idx] = (-10, -10)\n",
        "\n",
        "      # perform adopt and override after cutoff reached\n",
        "      if l_a == self.cutoff or l_h == self.cutoff:\n",
        "        P_match[state_idx, state_idx] = 1\n",
        "        R_match[state_idx, state_idx] = (-10, -10)\n",
        "        P_wait[state_idx, state_idx] = 1\n",
        "        R_wait[state_idx, state_idx] = (-10, -10)\n",
        "        continue\n",
        "\n",
        "\n",
        "      # match action transition matrix\n",
        "      if match == 'relevant' and l_a >= l_h and l_h > 0:\n",
        "        payout = (l_h)*(l_a - b_e)//l_a\n",
        "        new_b_e = b_e - (l_h - payout)\n",
        "\n",
        "        # attacker mines next block\n",
        "        P_match[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"active\")]] = self.p\n",
        "        R_match[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"active\")]] =  (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # eclipsed node mines next block\n",
        "        if self.lam != 0:\n",
        "          P_match[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"active\")]] = self.lam\n",
        "          R_match[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines next block after pool's head\n",
        "        P_match[state_idx, self.states_inverted[State(l_a - l_h, 1, new_b_e, \"relevant\")]] = self.gamma * self.q * (1 - self.stale)\n",
        "        R_match[state_idx, self.states_inverted[State(l_a - l_h, 1, new_b_e, \"relevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "        # network mines next block after other's head\n",
        "        P_match[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (1-self.gamma) * self.q * (1 - self.stale)\n",
        "        R_match[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines stale block\n",
        "        P_match[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"active\")]] = self.q * self.stale\n",
        "        R_match[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "      else:\n",
        "        P_match[state_idx, state_idx] = 1\n",
        "        R_match[state_idx, state_idx] = (-10, -10)\n",
        "\n",
        "      # wait action transition matrix\n",
        "      if match == 'active' and l_a >= l_h and l_h > 0:\n",
        "        payout = (l_h)*(l_a - b_e)//l_a\n",
        "        new_b_e = b_e - (l_h - payout)\n",
        "\n",
        "        # attacker mines next block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"active\")]] = self.p\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # eclipsed node mines next block\n",
        "        if self.lam != 0:\n",
        "          P_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"active\")]] = self.lam\n",
        "          R_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines after the pool's head\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a - l_h, 1, new_b_e, \"relevant\")]] = self.gamma * self.q * (1 - self.stale)\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a - l_h, 1, new_b_e, \"relevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "        # network mines after other's head\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (1-self.gamma) * self.q * (1 - self.stale)\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines stale block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"active\")]] = self.q * self.stale\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "      else:\n",
        "        # attacker mines next block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"irrelevant\")]] = self.p\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"irrelevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # eclipsed node mines next block\n",
        "        if self.lam != 0:\n",
        "          P_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"irrelevant\")]] = self.lam\n",
        "          R_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"irrelevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines next block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = self.q * (1 - self.stale)\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines stale block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"irrelevant\")]] = self.q * self.stale\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"irrelevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "    self.P = np.array([P_wait, P_adopt, P_override, P_match])\n",
        "    self.R = np.array([R_wait, R_adopt, R_override, R_match])\n",
        "\n",
        "    # state dimension\n",
        "    self.S = self.P.shape[1]\n",
        "\n",
        "    # action dimension\n",
        "    self.A = len(self.P)\n",
        "\n",
        "  def reset(self):\n",
        "    probs = [self.p, self.lam, self.q * (1 - self.stale), self.q * self.stale]\n",
        "    states = [State(1, 0, 0, 'irrelevant'), State(1, 0, 1, 'irrelevant'), State(0, 1, 0, 'relevant'), State(0, 0, 0, 'irrelevant')]\n",
        "    self.current_state = np.random.choice(states, p=probs)\n",
        "    return self.current_state.to_numpy()\n",
        "\n",
        "  def rand_state(self):\n",
        "    return self.states[np.random.randint(0, self.S)].to_numpy()\n",
        "\n",
        "  def step(self, action):\n",
        "    # p_s_new = np.random.random()\n",
        "    # p = 0\n",
        "    # s_new = -1\n",
        "    s_new = np.random.choice(np.arange(self.states_counter),p=self.P[action][self.states_inverted[self.current_state]])\n",
        "    # while (p < p_s_new) and (s_new < (self.S - 1)):\n",
        "    #   s_new = s_new + 1\n",
        "    #   p = p + self.P[action][self.states_inverted[self.current_state], s_new]\n",
        "    # try:\n",
        "    r = self.R[action][self.states_inverted[self.current_state], s_new]\n",
        "    # except IndexError:\n",
        "    #   try:\n",
        "    #     r = self.R[self.states_inverted[self.current_state], action]\n",
        "    #   except IndexError:\n",
        "    #     r = self.R[self.states_inverted[self.current_state]]\n",
        "    self.current_state = self.states[s_new]\n",
        "\n",
        "    return (self.current_state.to_numpy(), r[0], r[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9cTz6jRVgp5"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TQEjcBWn0ujC"
      },
      "outputs": [],
      "source": [
        "def get_allowed_actions(state, cutoff):\n",
        "  state = to_state_obj(state.cpu().detach().view(-1).numpy())\n",
        "  allowed_actions = [Action.adopt, Action.wait]\n",
        "  if state.length_a >= cutoff or state.length_h >= cutoff:\n",
        "    allowed_actions.remove(Action.wait)\n",
        "    allowed_actions.append(Action.adopt)\n",
        "    if state.length_a > state.length_h:\n",
        "      allowed_actions.append(Action.override)\n",
        "    return list(set(allowed_actions))\n",
        "  if state.length_a > state.length_h:\n",
        "    allowed_actions.append(Action.override)\n",
        "  if state.length_a >= state.length_h and state.length_h > 0 and state.match == 'relevant':\n",
        "    allowed_actions.append(Action.match)\n",
        "  return list(set(allowed_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_BqOBOqS1Avm"
      },
      "outputs": [],
      "source": [
        "# Creating the architecture of the network\n",
        "class Network(nn.Module):\n",
        "  # number of input neurons = size of the dimensions of the state (8)\n",
        "  # number of actions\n",
        "  # random seed\n",
        "  def __init__(self, state_size, action_size, seed = 42):\n",
        "    super(Network, self).__init__()\n",
        "    # Sets the seed for generating random numbers\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    # first full connection between the input layer and hidden layer\n",
        "    self.fc1 = nn.Linear(state_size, 2048)\n",
        "    # second full connection layer between first hidden layer and second layer\n",
        "    self.fc2 = nn.Linear(2048, 1024)\n",
        "    self.fc3 = nn.Linear(1024, 512)\n",
        "    self.fc4 = nn.Linear(512, 256)\n",
        "    # connetion between the second hidden layer and the output layer\n",
        "    self.fc5 = nn.Linear(256, action_size)\n",
        "\n",
        "  # forward propogation from input layer to the output layer\n",
        "  def forward(self, state):\n",
        "    # applying activation function (propogate the signal from input layer\n",
        "    # to the first hidden layer applying activation function)\n",
        "    x = self.fc1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc4(x)\n",
        "    x = F.relu(x)\n",
        "    return self.fc5(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwfu4bQcb5SL"
      },
      "source": [
        "## Training the agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxcAenGqcUep"
      },
      "source": [
        "## Initialize hyper parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pTTRd79kb7VD"
      },
      "outputs": [],
      "source": [
        "learning_rate = 5e-5\n",
        "# size of each batch where the model will be trained\n",
        "minibatch_size = 400\n",
        "discount_factor = 0.999\n",
        "# size of the replay buffer\n",
        "replay_buffer_size = int(1e12)\n",
        "interpolation_parameter = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKO_RcQOcjkw"
      },
      "source": [
        "## Implementing experience replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfvTb2hLceWC"
      },
      "outputs": [],
      "source": [
        "# Implementing experience replay\n",
        "class ReplayMemory(object):\n",
        "  # capacity -> size of the buffer\n",
        "  def __init__(self, capacity):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "\n",
        "  # append a new transition in the memory and ensures that memory contain\n",
        "  # only 100000 transitions\n",
        "  def push(self, event):\n",
        "    self.memory.append(event)\n",
        "    # check if buffer does not exceed the capacity\n",
        "    if len(self.memory) > self.capacity:\n",
        "      del self.memory[0]\n",
        "\n",
        "  # batch_size -> number of experiences sampled in a batch\n",
        "  # each experience contains (state,action,reward,nextstate,boolean done)\n",
        "  def sample(self, batch_size):\n",
        "    experiences = random.sample(self.memory, k=batch_size)\n",
        "    # all the states corresponding to all experiences\n",
        "    # next converting to torch tensor\n",
        "    # finally move to the designated computing device\n",
        "    states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "    # actions\n",
        "    actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
        "    # rewards\n",
        "    reward_a = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "    reward_h = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "    # next states\n",
        "    next_states = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None])).float().to(self.device)\n",
        "\n",
        "    return states, next_states, actions, reward_a, reward_h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5DKT_S_rHsBb"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    # local q network of adversary\n",
        "    self.a_local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    # target q network of adversary\n",
        "    self.a_target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    # local q network of honest\n",
        "    self.h_local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    # target q network of honest\n",
        "    self.h_target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "\n",
        "    # parameters are the weights of the network\n",
        "    self.a_optimizer = optim.AdamW(self.a_local_qnetwork.parameters(), lr=learning_rate)\n",
        "    self.h_optimizer = optim.AdamW(self.h_local_qnetwork.parameters(), lr=learning_rate)\n",
        "\n",
        "    # replay memory\n",
        "    self.memory = PrioritizedReplayBuffer(alpha=0.7, beta=0.6, storage=ListStorage(900000))\n",
        "    # timestep to decide when to learn from the experirences\n",
        "    self.t_step = 0\n",
        "    self.temp = 1e-9\n",
        "\n",
        "# method to store exp and decide when to learn from them\n",
        "  def step(self, state, action, reward_a, reward_h, next_state):\n",
        "    self.memory.add((state, next_state, action, reward_a, reward_h))\n",
        "    # when timestep reaches 4 the model will learn by taking a minibatch from the\n",
        "    # replay buffer\n",
        "    self.t_step = (self.t_step + 1) % 20\n",
        "    if self.t_step == 0:\n",
        "      # check if there are at least 100 exp in the buffer\n",
        "      if len(self.memory) > minibatch_size:\n",
        "        experiences, info = self.memory.sample(400, return_info=True)\n",
        "        self.learn(info, experiences, discount_factor)\n",
        "\n",
        "\n",
        "  def act(self, state, eps_t):\n",
        "    # adding an extra dimension corresponding to the batch (it indicates to which batch this state belogns to)\n",
        "    # note that always the batch index should be at the beginning\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "    # set the local network to evaluation mode before forward pass\n",
        "    # because as this is the forward pass we are making predictions\n",
        "    self.a_local_qnetwork.eval()\n",
        "    self.h_local_qnetwork.eval()\n",
        "    # since we are in forward there is no need to calculate gradients\n",
        "    with torch.no_grad():\n",
        "      # predicting q values (forward pass)\n",
        "      a_action_values = self.a_local_qnetwork(state).cpu().data.squeeze(0)\n",
        "      h_action_values = self.h_local_qnetwork(state).cpu().data.squeeze(0)\n",
        "      # print(a_action_values, h_action_values)\n",
        "      # relative_rev = torch.max(a_action_values / (a_action_values + h_action_values)).item()\n",
        "    # resetting model to traning mode\n",
        "\n",
        "    self.a_local_qnetwork.train()\n",
        "    self.h_local_qnetwork.train()\n",
        "\n",
        "    # select an action based on epsilon greedy policy\n",
        "    # we generate a random number R and if R > epsilon ? we choose the maximum predicted q value\n",
        "    # : select a random aciton\n",
        "    rel_actions = a_action_values / (a_action_values + h_action_values + self.temp)\n",
        "\n",
        "    mask_value = float(\"-inf\")\n",
        "    # unallowed actions\n",
        "    diff = list(set([0, 1, 2, 3]) - set(get_allowed_actions(state, 30)))\n",
        "    if diff:\n",
        "      rel_actions[torch.tensor(diff)] = mask_value\n",
        "\n",
        "    tensor = rel_actions / eps_t\n",
        "    # replaced_tensor = torch.where(torch.isposinf(tensor), torch.tensor(1.0), tensor)\n",
        "    tensor -= torch.max(tensor)\n",
        "    probabilities = F.softmax(tensor, dim=-1)\n",
        "    act = torch.multinomial(probabilities, 1).item()\n",
        "    return act\n",
        "\n",
        "\n",
        "  # allows agent to learn based on the minibatch\n",
        "  def learn(self, info, experiences, discount_factor):\n",
        "    states, next_states, actions, rewards_a, rewards_h = experiences\n",
        "\n",
        "    states = states.to(dtype=torch.float32, device=self.device)\n",
        "    next_states = next_states.to(dtype=torch.float32, device=self.device)\n",
        "    actions = actions.unsqueeze(1).to(dtype=torch.long, device=self.device)\n",
        "    rewards_a = rewards_a.unsqueeze(1).to(dtype=torch.float32, device=self.device)\n",
        "    rewards_h = rewards_h.unsqueeze(1).to(dtype=torch.float32, device=self.device)\n",
        "\n",
        "    # to compute the target q value we need the maxium q value for the next state\n",
        "    # use the target network to get the q values for all the actions from that next state\n",
        "    # next_q_targets = self.a_target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    # (100, 4)\n",
        "\n",
        "    next_a_q_targets = self.a_target_qnetwork(next_states).detach()\n",
        "    next_h_q_targets = self.h_target_qnetwork(next_states).detach()\n",
        "    # (100, 1)\n",
        "    a_ = torch.stack([next_a_q_targets[idx] / (next_a_q_targets[idx] + next_h_q_targets[idx] + self.temp) for idx, _ in enumerate(next_a_q_targets)])\n",
        "    # (100, 4\n",
        "    mask_value = float(\"-inf\")\n",
        "    # unallowed actions\n",
        "    for idx, state in enumerate(next_states):\n",
        "      diff = list(set([0, 1, 2, 3]) - set(get_allowed_actions(state, 30)))\n",
        "      if diff:\n",
        "        a_[idx][torch.tensor(diff)] = mask_value\n",
        "\n",
        "    a_ = a_.max(1)[1].unsqueeze(1).view(-1, 1)\n",
        "    # (100, 1)\n",
        "    q_a_targets = rewards_a + (discount_factor * next_a_q_targets.gather(1, a_))\n",
        "    q_h_targets = rewards_h + (discount_factor * next_h_q_targets.gather(1, a_))\n",
        "    # q_targets = rewards + (discount_factor * next_q_targets * (1 - dones))\n",
        "    # forward propogate the states to get the predicted q values\n",
        "\n",
        "    q_a_expected = self.a_local_qnetwork(states).gather(1, actions)\n",
        "    q_h_expected = self.h_local_qnetwork(states).gather(1, actions)\n",
        "    # loss (mean squared error)\n",
        "\n",
        "    loss_a = F.mse_loss(q_a_expected, q_a_targets)\n",
        "    loss_h = F.mse_loss(q_h_expected, q_h_targets)\n",
        "\n",
        "    delta_a = q_a_expected - q_a_targets\n",
        "    delta_h = q_h_expected - q_h_targets\n",
        "\n",
        "    priorities_a = (delta_a.abs().cpu().detach().numpy().flatten())\n",
        "    priorities_h = (delta_h.abs().cpu().detach().numpy().flatten())\n",
        "    priorities = priorities_a / (priorities_a + priorities_h + self.temp)\n",
        "    self.memory.update_priority(info['index'], priorities)\n",
        "\n",
        "    # backpropogating the error to update the weights\n",
        "    self.a_optimizer.zero_grad()\n",
        "    self.h_optimizer.zero_grad()\n",
        "\n",
        "    loss_a.backward()\n",
        "    loss_h.backward()\n",
        "\n",
        "    # max_grad_norm = 0.5\n",
        "    # clip_grad_norm_(self.a_local_qnetwork.parameters(), max_grad_norm)\n",
        "    # clip_grad_norm_(self.h_local_qnetwork.parameters(), max_grad_norm)\n",
        "\n",
        "    # single optimization step for updating the weights\n",
        "    self.a_optimizer.step()\n",
        "    self.h_optimizer.step()\n",
        "\n",
        "    # updating the target network weights\n",
        "    self.soft_update(self.a_local_qnetwork, self.a_target_qnetwork, interpolation_parameter)\n",
        "    self.soft_update(self.h_local_qnetwork, self.h_target_qnetwork, interpolation_parameter)\n",
        "\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    for target_param, local_param in zip(target_model.parameters(),local_model.parameters()):\n",
        "      # softly update the target model parameters with the weighted average of the local and target params\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdztmUgfdbmJ"
      },
      "source": [
        "## Implementing DQN class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y-aVh2VqHgD"
      },
      "outputs": [],
      "source": [
        "agent = Agent(state_size, action_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aQVwhmUgYnRR"
      },
      "outputs": [],
      "source": [
        "p = 0.45\n",
        "stale = 0\n",
        "gamma = 1\n",
        "cost = 0\n",
        "# setup up the environment\n",
        "env = MiningEnv(p=p, stale=stale, gamma = gamma, cost = cost, cutoff=30)\n",
        "# 0.6414794921875"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t66n8I7NXk3i"
      },
      "outputs": [],
      "source": [
        "# scores_episodes = deque(maxlen=150)\n",
        "# state = env.reset()\n",
        "# a_score = 0\n",
        "# h_score = 0\n",
        "# temperature = 1\n",
        "# temperature_decay = 0.9992 # Temperature decay factor\n",
        "# # try 0.9998, 0.9999\n",
        "# min_temperature = 1e-5\n",
        "# number_episodes = 1000000\n",
        "# for episode in range(1, number_episodes + 1):\n",
        "#   action = agent.act(state, temperature)\n",
        "#   next_state, reward_a, reward_h = env.step(action)\n",
        "#   agent.step(state, action, reward_a, reward_h, next_state)\n",
        "#   state = next_state\n",
        "#   a_score += reward_a\n",
        "#   h_score += reward_h\n",
        "#   if episode % 10000 == 0:\n",
        "#     print(a_score, h_score)\n",
        "#     rel = a_score/(a_score+h_score)\n",
        "#     scores_episodes.append(rel)\n",
        "#     print('\\rEpisode {}\\tAverage Score: {:.4f} Gain: {:.4f}'.format(episode,np.mean(scores_episodes), rel))\n",
        "#     a_score = 0\n",
        "#     h_score = 0\n",
        "#   temperature = max(min_temperature, temperature * temperature_decay)\n",
        "#   np.save('drl_revenues_%.3fhashrate%.2fgamma%.3fstale' % (p, gamma, stale), np.array(scores_episodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gVaufY6jLulQ"
      },
      "outputs": [],
      "source": [
        "def plot_dynamic_env(change_tuple):\n",
        "  number_episodes = 700000\n",
        "  # change_interval = 100000\n",
        "  temperature_decay = 0.9992\n",
        "  min_temperature = 1e-5\n",
        "  agent = Agent(state_size, action_size)\n",
        "\n",
        "  scores_episodes = deque(maxlen=300)\n",
        "  temperature = 1\n",
        "  for env_tuple in change_tuple:\n",
        "    p = env_tuple[0]\n",
        "    gamma = env_tuple[1]\n",
        "    # setup up the environment\n",
        "    env = MiningEnv(p=p, stale=0, gamma = gamma, cost = 0, cutoff=30)\n",
        "    print(f'Evaluating environment gamma: {env.gamma}, p: {env.p}')\n",
        "    # training loop\n",
        "    state = env.reset()\n",
        "    a_score = 0\n",
        "    h_score = 0\n",
        "    agent.memory = PrioritizedReplayBuffer(alpha=0.7, beta=0.6, storage=ListStorage(900000))\n",
        "    for episode in range(1, number_episodes + 1):\n",
        "\n",
        "      action = agent.act(state, temperature)\n",
        "      next_state, reward_a, reward_h = env.step(action)\n",
        "      agent.step(state, action, reward_a, reward_h, next_state)\n",
        "      state = next_state\n",
        "      a_score += reward_a\n",
        "      h_score += reward_h\n",
        "      if episode % 10000 == 0:\n",
        "        print(a_score, h_score)\n",
        "        rel = a_score/(a_score+h_score)\n",
        "        scores_episodes.append(rel)\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.4f} Gain: {:.4f}'.format(episode,np.mean(scores_episodes), rel))\n",
        "        a_score = 0\n",
        "        h_score = 0\n",
        "      temperature = max(min_temperature, temperature * temperature_decay)\n",
        "    np.save('dynamic_rl_gain_%.3fhashrate%.2fgamma' % (p, gamma), np.array(scores_episodes))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_dynamic_env([(0.45, 1), (0.3, 1), (0.2, 1)])"
      ],
      "metadata": {
        "id": "ZETae46Vk27U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrHIv27oP2F8"
      },
      "source": [
        "# Tabular Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2KLCrenRu1n"
      },
      "outputs": [],
      "source": [
        "p = 0.45\n",
        "stale = 0\n",
        "gamma = 1\n",
        "cost = 0\n",
        "# setup up the environment\n",
        "env = MiningEnv(p=p, stale=stale, gamma = gamma, cost = cost, cutoff=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SrYblD5K4f8U"
      },
      "outputs": [],
      "source": [
        "class QLearning():\n",
        "\n",
        "  def __init__(self, t_eps, learning_rate, discount_factor=0.999, n_iter=700000):\n",
        "    self.n_iter = int(n_iter)\n",
        "    self.S = env.S\n",
        "    self.A = env.A\n",
        "    self.learning_rate = learning_rate\n",
        "    self.discount_factor = discount_factor\n",
        "    self.t_eps = t_eps\n",
        "    self.Q_a = np.zeros((self.S, self.A))\n",
        "    self.Q_h = np.zeros((self.S, self.A))\n",
        "    self.V_s = np.zeros(self.S)\n",
        "    self.temp = 1e-10\n",
        "\n",
        "\n",
        "  def run(self):\n",
        "    state = env.reset()\n",
        "    scores_episodes = deque(maxlen=150)\n",
        "    self.V_s[env.states_inverted[to_state_obj(state)]] += 1\n",
        "    a_score = 0\n",
        "    h_score = 0\n",
        "    for t in range(1, self.n_iter + 1):\n",
        "      eps = np.exp(-1 * self.V_s[env.states_inverted[to_state_obj(state)]] / self.t_eps)\n",
        "      pn = np.random.random()\n",
        "      a_t = self.Q_a[env.states_inverted[to_state_obj(state)]] / (self.Q_a[env.states_inverted[to_state_obj(state)]] + self.Q_h[env.states_inverted[to_state_obj(state)]]+self.temp)\n",
        "      a_t = np.nan_to_num(a_t, nan=0.0)\n",
        "      mask_value = float(\"-inf\")\n",
        "      diff = list(set([0, 1, 2, 3]) - set(get_allowed_actions(torch.tensor(state), 30)))\n",
        "      if diff:\n",
        "        a_t[diff] = mask_value\n",
        "      if pn < eps:\n",
        "        valid_actions_indices = np.where(a_t > float('-inf'))[0]\n",
        "        act = np.random.choice(valid_actions_indices)\n",
        "      else:\n",
        "        act = a_t.argmax()\n",
        "      next_state, reward_a, reward_h = env.step(act)\n",
        "      a_ = self.Q_a[env.states_inverted[to_state_obj(next_state)]] / (self.Q_a[env.states_inverted[to_state_obj(next_state)]] + self.Q_h[env.states_inverted[to_state_obj(next_state)]]+self.temp)\n",
        "      a_ = a_.argmax()\n",
        "      self.Q_a[env.states_inverted[to_state_obj(state)], act] = (1-self.learning_rate) * self.Q_a[env.states_inverted[to_state_obj(state)], act] + self.learning_rate * (reward_a + self.discount_factor * self.Q_a[env.states_inverted[to_state_obj(next_state)], a_])\n",
        "      self.Q_h[env.states_inverted[to_state_obj(state)], act] = (1-self.learning_rate) * self.Q_h[env.states_inverted[to_state_obj(state)], act] + self.learning_rate * (reward_h + self.discount_factor * self.Q_h[env.states_inverted[to_state_obj(next_state)], a_])\n",
        "      state = next_state\n",
        "      self.V_s[env.states_inverted[to_state_obj(state)]] += 1\n",
        "      a_score += reward_a\n",
        "      h_score += reward_h\n",
        "      if t % 10000 == 0:\n",
        "        print(a_score, h_score)\n",
        "        rel = a_score/(a_score+h_score)\n",
        "        scores_episodes.append(rel)\n",
        "        state = env.reset()\n",
        "        a_score = 0\n",
        "        h_score = 0\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.4f} Gain: {:.4f}'.format(t,np.mean(scores_episodes), rel))\n",
        "    np.save(\"tabular_revenues %.5flr%dteps.npy\" % (self.learning_rate, self.t_eps), np.array(scores_episodes))\n",
        "\n",
        "  def run_dynamic_env(envs):\n",
        "    scores_episodes = deque(maxlen=300)\n",
        "    for env_tuple in envs:\n",
        "      p = env_tuple[0]\n",
        "      gamma = env_tuple[1]\n",
        "      # setup up the environment\n",
        "      env = MiningEnv(p=p, stale=0, gamma = gamma, cost = 0, cutoff=30)\n",
        "      print(f'Evaluating environment gamma: {env.gamma}, p: {env.p}')\n",
        "      state = env.reset()\n",
        "\n",
        "      self.V_s[env.states_inverted[to_state_obj(state)]] += 1\n",
        "      a_score = 0\n",
        "      h_score = 0\n",
        "\n",
        "      for t in range(1, self.n_iter + 1):\n",
        "        eps = np.exp(-1 * self.V_s[env.states_inverted[to_state_obj(state)]] / self.t_eps)\n",
        "        pn = np.random.random()\n",
        "        a_t = self.Q_a[env.states_inverted[to_state_obj(state)]] / (self.Q_a[env.states_inverted[to_state_obj(state)]] + self.Q_h[env.states_inverted[to_state_obj(state)]]+self.temp)\n",
        "        a_t = np.nan_to_num(a_t, nan=0.0)\n",
        "        mask_value = float(\"-inf\")\n",
        "        diff = list(set([0, 1, 2, 3]) - set(get_allowed_actions(torch.tensor(state), 30)))\n",
        "        if diff:\n",
        "          a_t[diff] = mask_value\n",
        "        if pn < eps:\n",
        "          valid_actions_indices = np.where(a_t > float('-inf'))[0]\n",
        "          act = np.random.choice(valid_actions_indices)\n",
        "        else:\n",
        "          act = a_t.argmax()\n",
        "        next_state, reward_a, reward_h = env.step(act)\n",
        "        a_ = self.Q_a[env.states_inverted[to_state_obj(next_state)]] / (self.Q_a[env.states_inverted[to_state_obj(next_state)]] + self.Q_h[env.states_inverted[to_state_obj(next_state)]]+self.temp)\n",
        "        a_ = a_.argmax()\n",
        "        self.Q_a[env.states_inverted[to_state_obj(state)], act] = (1-self.learning_rate) * self.Q_a[env.states_inverted[to_state_obj(state)], act] + self.learning_rate * (reward_a + self.discount_factor * self.Q_a[env.states_inverted[to_state_obj(next_state)], a_])\n",
        "        self.Q_h[env.states_inverted[to_state_obj(state)], act] = (1-self.learning_rate) * self.Q_h[env.states_inverted[to_state_obj(state)], act] + self.learning_rate * (reward_h + self.discount_factor * self.Q_h[env.states_inverted[to_state_obj(next_state)], a_])\n",
        "        state = next_state\n",
        "        self.V_s[env.states_inverted[to_state_obj(state)]] += 1\n",
        "        a_score += reward_a\n",
        "        h_score += reward_h\n",
        "        if t % 10000 == 0:\n",
        "          print(a_score, h_score)\n",
        "          rel = a_score/(a_score+h_score)\n",
        "          scores_episodes.append(rel)\n",
        "          state = env.reset()\n",
        "          a_score = 0\n",
        "          h_score = 0\n",
        "          print('\\rEpisode {}\\tAverage Score: {:.4f} Gain: {:.4f}'.format(t,np.mean(scores_episodes), rel))\n",
        "      np.save(\"dynamic_tabular_revenues_1.npy\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3NTtkHFX6-t"
      },
      "outputs": [],
      "source": [
        "agent = QLearning(t_eps=1000, learning_rate=5e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F10lnCeZZaIS"
      },
      "outputs": [],
      "source": [
        "agent.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "ZI6VqjtbWRXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_convergence(gamma, stale=0):\n",
        "  p = 0.45\n",
        "  window_size=10\n",
        "  optimal_dict = {0:0.6523, 0.5:0.7496, 1:0.8100}\n",
        "  drl_revenues = np.load('drl_revenues_%.3fhashrate%.2fgamma%.3fstale.npy' % (p, gamma, stale))[:75]\n",
        "  tab_revenues = np.load('tabular_revenues_%.3fhashrate%.2fgamma%.3fstale.npy' % (p, gamma, stale))\n",
        "  initial_window_size = 1\n",
        "  later_window_size = 10\n",
        "  switch_point = 6 # point to switch window size\n",
        "\n",
        "  drl_initial_moving_avg = np.convolve(drl_revenues[:switch_point], np.ones(initial_window_size)/initial_window_size, mode='valid')\n",
        "  drl_later_moving_avg = np.convolve(drl_revenues[switch_point:], np.ones(later_window_size)/later_window_size, mode='valid')\n",
        "  drl_moving_averages = np.concatenate((drl_initial_moving_avg, drl_later_moving_avg))\n",
        "\n",
        "  tab_initial_moving_avg = np.convolve(tab_revenues[:switch_point], np.ones(initial_window_size)/initial_window_size, mode='valid')\n",
        "  tab_later_moving_avg = np.convolve(tab_revenues[switch_point:], np.ones(later_window_size)/later_window_size, mode='valid')\n",
        "  tab_moving_averages = np.concatenate((tab_initial_moving_avg, tab_later_moving_avg))\n",
        "  plt.ylim([0.2, 0.9])\n",
        "  plt.plot(drl_moving_averages, '-s',linewidth='4', label='DRL mining')\n",
        "  plt.plot(tab_moving_averages, '-s',linewidth='4', label='tabular Q-Learning')\n",
        "  plt.axhline(y=optimal_dict[gamma], color='r', linestyle='-', label='optimal revenue')\n",
        "  plt.title(r'DRL Convergence Comparison  $\\alpha$=0.45, $\\gamma$=%.1f' % (gamma))\n",
        "  plt.xlabel(r'Timesteps $\\times 10^4$')\n",
        "  plt.ylabel('Relative Gain')\n",
        "  plt.legend()\n",
        "  plt.savefig('DRL Convergence Comparison alpha=0.45_gamma=%.1f.png' % (gamma))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_convergence(gamma=1)\n",
        "plot_convergence(gamma=0.5)\n",
        "plot_convergence(gamma=0)\n"
      ],
      "metadata": {
        "id": "CEUrpLc8WTkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " agent = QLearning(t_eps=10000, learning_rate=5e-2)\n",
        " agent.run_dynamic_env()"
      ],
      "metadata": {
        "id": "8YEcFGDiacRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic environment test"
      ],
      "metadata": {
        "id": "DfDSCW1l-V_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('dynamic_rl_gain_1_log.txt') as f:\n",
        "  content = f.readlines()\n",
        "\n",
        "content = content[1::2]\n",
        "drl_revenues = [float(x.split(':')[2].strip()) for x in content]"
      ],
      "metadata": {
        "id": "hP--BaIA-QTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = 10\n",
        "a = drl_revenues[:70]\n",
        "b = np.convolve(drl_revenues[70:140], np.ones(w)/w, mode='valid')\n",
        "c = np.convolve(drl_revenues[140:], np.ones(w)/w, mode='valid')\n",
        "\n",
        "initial_window_size = 1\n",
        "later_window_size = 10\n",
        "switch_point = 6 # point to switch window size\n",
        "\n",
        "drl_initial_moving_avg = np.convolve(a[:switch_point], np.ones(initial_window_size)/initial_window_size, mode='valid')\n",
        "drl_later_moving_avg = np.convolve(a[switch_point:], np.ones(later_window_size)/later_window_size, mode='valid')\n",
        "drl_moving_averages = np.concatenate((drl_initial_moving_avg, drl_later_moving_avg))\n",
        "d = np.concatenate((drl_moving_averages,b,c))\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.set_ylim([0.2, 0.9])\n",
        "ax.set_xlim([0, 183])\n",
        "ax.plot(d, '-o',linewidth='3')\n",
        "ax.text(53, 0.4, r'($\\alpha$=%.2f, $\\gamma$=%.1f)' % (0.45, 1), fontsize=10, ha='right')\n",
        "ax.text(114, 0.4, r'($\\alpha$=%.2f, $\\gamma$=%.1f)' % (0.45, 0), fontsize=10, ha='right')\n",
        "ax.text(175, 0.4, r'($\\alpha$=%.2f, $\\gamma$=%.1f)' % (0.45, 0.5), fontsize=10, ha='right')\n",
        "ax.axvline(x=61, color='black', linestyle='dashdot')\n",
        "ax.axvline(x=122, color='black', linestyle='dashdot')\n",
        "\n",
        "ax.set_title(r'Performance of DRL Mining in dynamic environments')\n",
        "ax.set_xlabel(r'Timesteps $\\times 10^4$')\n",
        "ax.set_ylabel('Relative Gain')\n",
        "plt.savefig('DRL dynamic env_1.png')\n"
      ],
      "metadata": {
        "id": "yrrW64gh-dXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('results.txt') as f:\n",
        "  content = f.readlines()\n",
        "\n",
        "content = content[1::2]\n",
        "drl_revenues = [float(x.split(':')[2].strip()) for x in content]\n",
        "\n",
        "\n",
        "w = 10\n",
        "a = drl_revenues[:70]\n",
        "b = np.convolve(drl_revenues[70:140], np.ones(w)/w, mode='valid')\n",
        "c = np.convolve(drl_revenues[140:], np.ones(w)/w, mode='valid')\n",
        "\n",
        "initial_window_size = 1\n",
        "later_window_size = 10\n",
        "switch_point = 6 # point to switch window size\n",
        "\n",
        "drl_initial_moving_avg = np.convolve(a[:switch_point], np.ones(initial_window_size)/initial_window_size, mode='valid')\n",
        "drl_later_moving_avg = np.convolve(a[switch_point:], np.ones(later_window_size)/later_window_size, mode='valid')\n",
        "drl_moving_averages = np.concatenate((drl_initial_moving_avg, drl_later_moving_avg))\n",
        "d = np.concatenate((drl_moving_averages,b,c))\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.set_ylim([0.2, 0.9])\n",
        "ax.set_xlim([0, 183])\n",
        "ax.plot(d, '-o',linewidth='3')\n",
        "ax.text(53, 0.55, r'($\\alpha$=%.2f, $\\gamma$=%.1f)' % (0.45, 1), fontsize=10, ha='right')\n",
        "ax.text(114, 0.55, r'($\\alpha$=%.2f, $\\gamma$=%.1f)' % (0.3, 1), fontsize=10, ha='right')\n",
        "ax.text(175, 0.55, r'($\\alpha$=%.2f, $\\gamma$=%.1f)' % (0.2, 1), fontsize=10, ha='right')\n",
        "ax.axvline(x=61, color='black', linestyle='dashdot')\n",
        "ax.axvline(x=122, color='black', linestyle='dashdot')\n",
        "\n",
        "ax.set_title(r'Performance of DRL Mining in dynamic environments')\n",
        "ax.set_xlabel(r'Timesteps $\\times 10^4$')\n",
        "ax.set_ylabel('Relative Gain')\n",
        "plt.savefig('DRL dynamic env_2.png')\n"
      ],
      "metadata": {
        "id": "SPnJFGs4-gLG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}