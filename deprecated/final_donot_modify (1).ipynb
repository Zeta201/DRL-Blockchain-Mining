{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z-nWnnHFjJAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5cc22a4-a555-4eda-f0b8-ca8d98420029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymdptoolbox\n",
            "  Downloading pymdptoolbox-4.0-b3.zip (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.11.4)\n",
            "Building wheels for collected packages: pymdptoolbox\n",
            "  Building wheel for pymdptoolbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymdptoolbox: filename=pymdptoolbox-4.0b3-py3-none-any.whl size=25656 sha256=5cd32d91b4c5683a6e61de89b9cd87d6a6739046c093c6507ee58a1d033c742e\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/e7/c7/d7abf9e309f3573a934fed2750c70bd75d9e9d901f7f16e183\n",
            "Successfully built pymdptoolbox\n",
            "Installing collected packages: pymdptoolbox\n",
            "Successfully installed pymdptoolbox-4.0b3\n",
            "Collecting torchrl\n",
            "  Downloading torchrl-0.4.0-cp310-cp310-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchrl) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchrl) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchrl) (24.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from torchrl) (2.2.1)\n",
            "Collecting tensordict>=0.4.0 (from torchrl)\n",
            "  Downloading tensordict-0.4.0-cp310-cp310-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.3.0->torchrl)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchrl)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchrl) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchrl) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, tensordict, torchrl\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 tensordict-0.4.0 torchrl-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pymdptoolbox\n",
        "!pip install torchrl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mining Environment"
      ],
      "metadata": {
        "id": "cTAbkptTbGGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mdptoolbox, mdptoolbox.example\n",
        "import numpy as np\n",
        "import enum\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple\n",
        "from torchrl.data import ListStorage, PrioritizedReplayBuffer\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "VRvLRKcZjNUy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "match_dict_inv = {0: 'irrelevant', 1: 'relevant', 2: 'active'}"
      ],
      "metadata": {
        "id": "7Jkfteott9o-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2070e64f-3f75-4dd7-f266-33350e9a958c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class State:\n",
        "    def __init__(self, l_a, l_h, b_e, match=\"relevant\"):\n",
        "        self.length_a = l_a\n",
        "        self.length_h = l_h\n",
        "        self.blocks_e = b_e\n",
        "        self.match = match\n",
        "        self.match_dict = {'irrelevant': 0, 'relevant': 1, 'active': 2}\n",
        "\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash((self.length_a, self.length_h, self.blocks_e, self.match))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        try:\n",
        "            return (self.length_a, self.length_h, self.blocks_e, self.match) == (other.length_a, other.length_h, other.blocks_e, other.match)\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def __ne__(self, other):\n",
        "        return not (self == other)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"(%d, %d, %d, %s)\" % (self.length_a, self.length_h, self.blocks_e, self.match)\n",
        "\n",
        "    def to_numpy(self):\n",
        "      return np.array([self.length_a, self.length_h, self.blocks_e, self.match_dict[self.match]])\n"
      ],
      "metadata": {
        "id": "dsc7WuVG8IRg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiningEnv():\n",
        "\n",
        "  def __init__(self, p, stale, gamma, cost, cutoff=20, lam=0):\n",
        "    self.p = p\n",
        "    self.cutoff = cutoff\n",
        "    self.lam = lam\n",
        "    self.match_cases = [\"irrelevant\", \"relevant\", \"active\"]\n",
        "    self.a_cost = cost * p\n",
        "    self.h_cost = cost * (1 - p)\n",
        "    self.q = 1 - p - lam\n",
        "    self.stale = stale\n",
        "    self.gamma = gamma\n",
        "    self.current_state = State(0, 0, 0, 'irrelevant')\n",
        "    state_count = 0\n",
        "    self.states = {}\n",
        "    self.states_inverted = {}\n",
        "    for l_a in range(self.cutoff + 1):\n",
        "      for l_h in range(self.cutoff + 1):\n",
        "        for b_e in range(l_a + 1):\n",
        "          if self.lam == 0 and b_e > 0:\n",
        "            continue\n",
        "          for match in self.match_cases:\n",
        "            state = State(l_a, l_h, b_e, match)\n",
        "            self.states[state_count] = state\n",
        "            self.states_inverted[state] = state_count\n",
        "            state_count += 1\n",
        "    self.states_counter = state_count\n",
        "\n",
        "    # transition matrices\n",
        "    P_adopt = np.zeros(shape=(state_count, state_count))\n",
        "    P_override = np.zeros(shape=(state_count, state_count))\n",
        "    P_match = np.zeros(shape=(state_count, state_count))\n",
        "    P_wait = np.zeros(shape=(state_count, state_count))\n",
        "\n",
        "    # reward matrices\n",
        "    R_adopt = np.empty(shape=(state_count, state_count), dtype=object)\n",
        "    R_override = np.empty(shape=(state_count, state_count), dtype=object)\n",
        "    R_match = np.empty(shape=(state_count, state_count), dtype=object)\n",
        "    R_wait = np.empty(shape=(state_count, state_count), dtype=object)\n",
        "\n",
        "    R_adopt.fill((0,0))\n",
        "    R_override.fill((0,0))\n",
        "    R_match.fill((0,0))\n",
        "    R_wait.fill((0,0))\n",
        "\n",
        "    for state_idx, state in self.states.items():\n",
        "      l_a = state.length_a\n",
        "      l_h = state.length_h\n",
        "      b_e = state.blocks_e\n",
        "      match = state.match\n",
        "\n",
        "      # adopt action transition matrix\n",
        "      # attacker mines next block\n",
        "      P_adopt[state_idx, self.states_inverted[State(1, 0, 0, \"irrelevant\")]] = self.p\n",
        "      R_adopt[state_idx, self.states_inverted[State(1, 0, 0, \"irrelevant\")]] = (-self.a_cost, l_h - self.h_cost)\n",
        "\n",
        "      # eclipsed node mines next block\n",
        "      if self.lam != 0:\n",
        "        P_adopt[state_idx, self.states_inverted[State(1, 0, 1, \"irrelevant\")]] = self.lam\n",
        "        R_adopt[state_idx, self.states_inverted[State(1, 0, 1, \"irrelevant\")]] = (-self.a_cost, l_h - self.h_cost)\n",
        "\n",
        "      # honest network mines next block\n",
        "      P_adopt[state_idx, self.states_inverted[State(0, 1, 0, \"relevant\")]] = self.q*(1-self.stale)\n",
        "      R_adopt[state_idx, self.states_inverted[State(0, 1, 0, \"relevant\")]] = (-self.a_cost, l_h - self.h_cost)\n",
        "\n",
        "      # network mines state block\n",
        "      P_adopt[state_idx, self.states_inverted[State(0, 0, 0, \"irrelevant\")]] = self.q * self.stale\n",
        "      R_adopt[state_idx, self.states_inverted[State(0, 0, 0, \"irrelevant\")]] = (-self.a_cost, l_h - self.h_cost)\n",
        "\n",
        "      # override action transition matrix\n",
        "      if l_a > l_h:\n",
        "        payout = (l_h+1)*(l_a - b_e)//l_a\n",
        "        new_b_e = b_e - (l_h+1 - payout)\n",
        "      # attacker mines next block\n",
        "        P_override[state_idx, self.states_inverted[State(l_a - l_h, 0, new_b_e, \"irrelevant\")]] = self.p\n",
        "        R_override[state_idx, self.states_inverted[State(l_a - l_h, 0, new_b_e, \"irrelevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "      # eclipsed node mines next block\n",
        "        if self.lam != 0:\n",
        "          P_override[state_idx, self.states_inverted[State(l_a - l_h, 0, new_b_e + 1, \"irrelevant\")]] = self.lam\n",
        "          R_override[state_idx, self.states_inverted[State(l_a - l_h, 0, new_b_e + 1, \"irrelevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "      # network mines next block\n",
        "        P_override[state_idx, self.states_inverted[State(l_a-l_h-1, 1, new_b_e, \"relevant\")]] = self.q*(1 - self.stale)\n",
        "        R_override[state_idx, self.states_inverted[State(l_a-l_h-1, 1, new_b_e, \"relevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "      # network mines stale block\n",
        "        P_override[state_idx, self.states_inverted[State(l_a-l_h-1, 0, new_b_e, \"irrelevant\")]] = self.q*self.stale\n",
        "        R_override[state_idx, self.states_inverted[State(l_a-l_h-1, 0, new_b_e, \"irrelevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "      else:\n",
        "        P_override[state_idx, state_idx] = 1\n",
        "        R_override[state_idx, state_idx] = (-10, -10)\n",
        "\n",
        "      # perform adopt and override after cutoff reached\n",
        "      if l_a == self.cutoff or l_h == self.cutoff:\n",
        "        P_match[state_idx, state_idx] = 1\n",
        "        R_match[state_idx, state_idx] = (-10, -10)\n",
        "        P_wait[state_idx, state_idx] = 1\n",
        "        R_wait[state_idx, state_idx] = (-10, -10)\n",
        "        continue\n",
        "\n",
        "\n",
        "      # match action transition matrix\n",
        "      if match == 'relevant' and l_a >= l_h and l_h > 0:\n",
        "        payout = (l_h)*(l_a - b_e)//l_a\n",
        "        new_b_e = b_e - (l_h - payout)\n",
        "\n",
        "        # attacker mines next block\n",
        "        P_match[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"active\")]] = self.p\n",
        "        R_match[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"active\")]] =  (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # eclipsed node mines next block\n",
        "        if self.lam != 0:\n",
        "          P_match[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"active\")]] = self.lam\n",
        "          R_match[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines next block after pool's head\n",
        "        P_match[state_idx, self.states_inverted[State(l_a - l_h, 1, new_b_e, \"relevant\")]] = self.gamma * self.q * (1 - self.stale)\n",
        "        R_match[state_idx, self.states_inverted[State(l_a - l_h, 1, new_b_e, \"relevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "        # network mines next block after other's head\n",
        "        P_match[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (1-self.gamma) * self.q * (1 - self.stale)\n",
        "        R_match[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines stale block\n",
        "        P_match[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"active\")]] = self.q * self.stale\n",
        "        R_match[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "      else:\n",
        "        P_match[state_idx, state_idx] = 1\n",
        "        R_match[state_idx, state_idx] = (-10, -10)\n",
        "\n",
        "      # wait action transition matrix\n",
        "      if match == 'active' and l_a >= l_h and l_h > 0:\n",
        "        payout = (l_h)*(l_a - b_e)//l_a\n",
        "        new_b_e = b_e - (l_h - payout)\n",
        "\n",
        "        # attacker mines next block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"active\")]] = self.p\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # eclipsed node mines next block\n",
        "        if self.lam != 0:\n",
        "          P_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"active\")]] = self.lam\n",
        "          R_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines after the pool's head\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a - l_h, 1, new_b_e, \"relevant\")]] = self.gamma * self.q * (1 - self.stale)\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a - l_h, 1, new_b_e, \"relevant\")]] = (payout - self.a_cost, b_e - new_b_e - self.h_cost)\n",
        "\n",
        "        # network mines after other's head\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (1-self.gamma) * self.q * (1 - self.stale)\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines stale block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"active\")]] = self.q * self.stale\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"active\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "      else:\n",
        "        # attacker mines next block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"irrelevant\")]] = self.p\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e, \"irrelevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # eclipsed node mines next block\n",
        "        if self.lam != 0:\n",
        "          P_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"irrelevant\")]] = self.lam\n",
        "          R_wait[state_idx, self.states_inverted[State(l_a + 1, l_h, b_e+1, \"irrelevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines next block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = self.q * (1 - self.stale)\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a, l_h + 1, b_e, \"relevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "        # network mines stale block\n",
        "        P_wait[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"irrelevant\")]] = self.q * self.stale\n",
        "        R_wait[state_idx, self.states_inverted[State(l_a, l_h, b_e, \"irrelevant\")]] = (-self.a_cost, -self.h_cost)\n",
        "\n",
        "    self.P = np.array([P_wait, P_adopt, P_override, P_match])\n",
        "    self.R = np.array([R_wait, R_adopt, R_override, R_match])\n",
        "\n",
        "    # state dimension\n",
        "    self.S = self.P.shape[1]\n",
        "\n",
        "    # action dimension\n",
        "    self.A = len(self.P)\n",
        "\n",
        "  def reset(self):\n",
        "    probs = [self.p, self.lam, self.q * (1 - self.stale), self.q * self.stale]\n",
        "    states = [State(1, 0, 0, 'irrelevant'), State(1, 0, 1, 'irrelevant'), State(0, 1, 0, 'relevant'), State(0, 0, 0, 'irrelevant')]\n",
        "    self.current_state = np.random.choice(states, p=probs)\n",
        "    return self.current_state.to_numpy()\n",
        "\n",
        "  def rand_state(self):\n",
        "    return self.states[np.random.randint(0, self.S)].to_numpy()\n",
        "\n",
        "  def step(self, action):\n",
        "    # p_s_new = np.random.random()\n",
        "    # p = 0\n",
        "    # s_new = -1\n",
        "    s_new = np.random.choice(np.arange(self.states_counter),p=env.P[action][self.states_inverted[self.current_state]])\n",
        "    # while (p < p_s_new) and (s_new < (self.S - 1)):\n",
        "    #   s_new = s_new + 1\n",
        "    #   p = p + self.P[action][self.states_inverted[self.current_state], s_new]\n",
        "    # try:\n",
        "    r = self.R[action][self.states_inverted[self.current_state], s_new]\n",
        "    # except IndexError:\n",
        "    #   try:\n",
        "    #     r = self.R[self.states_inverted[self.current_state], action]\n",
        "    #   except IndexError:\n",
        "    #     r = self.R[self.states_inverted[self.current_state]]\n",
        "    self.current_state = self.states[s_new]\n",
        "\n",
        "    return (self.current_state.to_numpy(), r[0], r[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "M-CRaClLjUdz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN"
      ],
      "metadata": {
        "id": "-9cTz6jRVgp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Match(enum.IntEnum):\n",
        "  irrelevant = 0,\n",
        "  relevant = 1,\n",
        "  active = 2\n",
        "\n",
        "class Action(enum.IntEnum):\n",
        "  wait = 0,\n",
        "  adopt = 1,\n",
        "  override = 2,\n",
        "  match = 3\n",
        "\n",
        "action_size = 4\n",
        "state_size = 4\n",
        "\n",
        "def to_state_obj(state):\n",
        "  return State(state[0], state[1], state[2], match_dict_inv[state[3]])"
      ],
      "metadata": {
        "id": "HVLwdBP00p9F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_allowed_actions(state, cutoff):\n",
        "  state = to_state_obj(state.cpu().detach().view(-1).numpy())\n",
        "  allowed_actions = [Action.adopt, Action.wait]\n",
        "  if state.length_a >= cutoff or state.length_h >= cutoff:\n",
        "    allowed_actions.remove(Action.wait)\n",
        "    allowed_actions.append(Action.adopt)\n",
        "    if state.length_a > state.length_h:\n",
        "      allowed_actions.append(Action.override)\n",
        "    return list(set(allowed_actions))\n",
        "  if state.length_a > state.length_h:\n",
        "    allowed_actions.append(Action.override)\n",
        "  if state.length_a >= state.length_h and state.length_h > 0 and state.match == 'relevant':\n",
        "    allowed_actions.append(Action.match)\n",
        "  return list(set(allowed_actions))"
      ],
      "metadata": {
        "id": "TQEjcBWn0ujC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the architecture of the network\n",
        "class Network(nn.Module):\n",
        "  # number of input neurons = size of the dimensions of the state (8)\n",
        "  # number of actions\n",
        "  # random seed\n",
        "  def __init__(self, state_size, action_size, seed = 42):\n",
        "    super(Network, self).__init__()\n",
        "    # Sets the seed for generating random numbers\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    # first full connection between the input layer and hidden layer\n",
        "    self.fc1 = nn.Linear(state_size, 2048)\n",
        "    # second full connection layer between first hidden layer and second layer\n",
        "    self.fc2 = nn.Linear(2048, 1024)\n",
        "    self.fc3 = nn.Linear(1024, 512)\n",
        "    self.fc4 = nn.Linear(512, 256)\n",
        "    # connetion between the second hidden layer and the output layer\n",
        "    self.fc5 = nn.Linear(256, action_size)\n",
        "\n",
        "  # forward propogation from input layer to the output layer\n",
        "  def forward(self, state):\n",
        "    # applying activation function (propogate the signal from input layer\n",
        "    # to the first hidden layer applying activation function)\n",
        "    x = self.fc1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc4(x)\n",
        "    x = F.relu(x)\n",
        "    return self.fc5(x)"
      ],
      "metadata": {
        "id": "_BqOBOqS1Avm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the agent"
      ],
      "metadata": {
        "id": "bwfu4bQcb5SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize hyper parameters\n"
      ],
      "metadata": {
        "id": "CxcAenGqcUep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-5\n",
        "# size of each batch where the model will be trained\n",
        "minibatch_size = 400\n",
        "discount_factor = 0.999\n",
        "# size of the replay buffer\n",
        "replay_buffer_size = int(1e12)\n",
        "interpolation_parameter = 0.01"
      ],
      "metadata": {
        "id": "pTTRd79kb7VD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing experience replay buffer"
      ],
      "metadata": {
        "id": "wKO_RcQOcjkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing experience replay\n",
        "class ReplayMemory(object):\n",
        "  # capacity -> size of the buffer\n",
        "  def __init__(self, capacity):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "\n",
        "  # append a new transition in the memory and ensures that memory contain\n",
        "  # only 100000 transitions\n",
        "  def push(self, event):\n",
        "    self.memory.append(event)\n",
        "    # check if buffer does not exceed the capacity\n",
        "    if len(self.memory) > self.capacity:\n",
        "      del self.memory[0]\n",
        "\n",
        "  # batch_size -> number of experiences sampled in a batch\n",
        "  # each experience contains (state,action,reward,nextstate,boolean done)\n",
        "  def sample(self, batch_size):\n",
        "    experiences = random.sample(self.memory, k=batch_size)\n",
        "    # all the states corresponding to all experiences\n",
        "    # next converting to torch tensor\n",
        "    # finally move to the designated computing device\n",
        "    states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "    # actions\n",
        "    actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
        "    # rewards\n",
        "    reward_a = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "    reward_h = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "    # next states\n",
        "    next_states = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None])).float().to(self.device)\n",
        "\n",
        "    return states, next_states, actions, reward_a, reward_h"
      ],
      "metadata": {
        "id": "GfvTb2hLceWC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    # local q network of adversary\n",
        "    self.a_local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    # target q network of adversary\n",
        "    self.a_target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    # local q network of honest\n",
        "    self.h_local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    # target q network of honest\n",
        "    self.h_target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "\n",
        "    # parameters are the weights of the network\n",
        "    self.a_optimizer = optim.AdamW(self.a_local_qnetwork.parameters(), lr=learning_rate)\n",
        "    self.h_optimizer = optim.AdamW(self.h_local_qnetwork.parameters(), lr=learning_rate)\n",
        "\n",
        "    # replay memory\n",
        "    self.memory = PrioritizedReplayBuffer(alpha=0.6, beta=0.4, storage=ListStorage(900000))\n",
        "    # timestep to decide when to learn from the experirences\n",
        "    self.t_step = 0\n",
        "\n",
        "# method to store exp and decide when to learn from them\n",
        "  def step(self, state, action, reward_a, reward_h, next_state):\n",
        "    self.memory.add((state, next_state, action, reward_a, reward_h))\n",
        "    # when timestep reaches 4 the model will learn by taking a minibatch from the\n",
        "    # replay buffer\n",
        "    self.t_step = (self.t_step + 1) % 20\n",
        "    if self.t_step == 0:\n",
        "      # check if there are at least 100 exp in the buffer\n",
        "      if len(self.memory) > minibatch_size:\n",
        "        experiences, info = self.memory.sample(400, return_info=True)\n",
        "        self.learn(info, experiences, discount_factor)\n",
        "\n",
        "\n",
        "  def act(self, state, eps_t):\n",
        "    # adding an extra dimension corresponding to the batch (it indicates to which batch this state belogns to)\n",
        "    # note that always the batch index should be at the beginning\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "    # set the local network to evaluation mode before forward pass\n",
        "    # because as this is the forward pass we are making predictions\n",
        "    self.a_local_qnetwork.eval()\n",
        "    self.h_local_qnetwork.eval()\n",
        "    # since we are in forward there is no need to calculate gradients\n",
        "    with torch.no_grad():\n",
        "      # predicting q values (forward pass)\n",
        "      a_action_values = self.a_local_qnetwork(state).cpu().data.squeeze(0)\n",
        "      h_action_values = self.h_local_qnetwork(state).cpu().data.squeeze(0)\n",
        "      # print(a_action_values, h_action_values)\n",
        "      # relative_rev = torch.max(a_action_values / (a_action_values + h_action_values)).item()\n",
        "    # resetting model to traning mode\n",
        "\n",
        "    self.a_local_qnetwork.train()\n",
        "    self.h_local_qnetwork.train()\n",
        "\n",
        "    # select an action based on epsilon greedy policy\n",
        "    # we generate a random number R and if R > epsilon ? we choose the maximum predicted q value\n",
        "    # : select a random aciton\n",
        "    rel_actions = a_action_values / (a_action_values + h_action_values)\n",
        "\n",
        "    mask_value = float(\"-inf\")\n",
        "    # unallowed actions\n",
        "    diff = list(set([0, 1, 2, 3]) - set(get_allowed_actions(state, 20)))\n",
        "    if diff:\n",
        "      rel_actions[torch.tensor(diff)] = mask_value\n",
        "\n",
        "    tensor = rel_actions / eps_t\n",
        "    # replaced_tensor = torch.where(torch.isposinf(tensor), torch.tensor(1.0), tensor)\n",
        "    tensor -= torch.max(tensor)\n",
        "    probabilities = F.softmax(tensor, dim=-1)\n",
        "    act = torch.multinomial(probabilities, 1).item()\n",
        "    return act\n",
        "\n",
        "\n",
        "  # allows agent to learn based on the minibatch\n",
        "  def learn(self, info, experiences, discount_factor):\n",
        "    states, next_states, actions, rewards_a, rewards_h = experiences\n",
        "\n",
        "    states = states.to(dtype=torch.float32, device=self.device)\n",
        "    next_states = next_states.to(dtype=torch.float32, device=self.device)\n",
        "    actions = actions.unsqueeze(1).to(dtype=torch.long, device=self.device)\n",
        "    rewards_a = rewards_a.unsqueeze(1).to(dtype=torch.float32, device=self.device)\n",
        "    rewards_h = rewards_h.unsqueeze(1).to(dtype=torch.float32, device=self.device)\n",
        "\n",
        "    # to compute the target q value we need the maxium q value for the next state\n",
        "    # use the target network to get the q values for all the actions from that next state\n",
        "    # next_q_targets = self.a_target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    # (100, 4)\n",
        "\n",
        "    next_a_q_targets = self.a_target_qnetwork(next_states).detach()\n",
        "    next_h_q_targets = self.h_target_qnetwork(next_states).detach()\n",
        "    # (100, 1)\n",
        "    a_ = torch.stack([next_a_q_targets[idx] / (next_a_q_targets[idx] + next_h_q_targets[idx]) for idx, _ in enumerate(next_a_q_targets)])\n",
        "    # (100, 4\n",
        "    mask_value = float(\"-inf\")\n",
        "    # unallowed actions\n",
        "    for idx, state in enumerate(next_states):\n",
        "      diff = list(set([0, 1, 2, 3]) - set(get_allowed_actions(state, 20)))\n",
        "      if diff:\n",
        "        a_[idx][torch.tensor(diff)] = mask_value\n",
        "\n",
        "    a_ = a_.max(1)[1].unsqueeze(1).view(-1, 1)\n",
        "    # (100, 1)\n",
        "    q_a_targets = rewards_a + (discount_factor * next_a_q_targets.gather(1, a_))\n",
        "    q_h_targets = rewards_h + (discount_factor * next_h_q_targets.gather(1, a_))\n",
        "    # q_targets = rewards + (discount_factor * next_q_targets * (1 - dones))\n",
        "    # forward propogate the states to get the predicted q values\n",
        "\n",
        "    q_a_expected = self.a_local_qnetwork(states).gather(1, actions)\n",
        "    q_h_expected = self.h_local_qnetwork(states).gather(1, actions)\n",
        "    # loss (mean squared error)\n",
        "\n",
        "    loss_a = F.mse_loss(q_a_expected, q_a_targets)\n",
        "    loss_h = F.mse_loss(q_h_expected, q_h_targets)\n",
        "\n",
        "    delta_a = q_a_expected - q_a_targets\n",
        "    delta_h = q_h_expected - q_h_targets\n",
        "\n",
        "    priorities_a = (delta_a.abs().cpu().detach().numpy().flatten())\n",
        "    priorities_h = (delta_h.abs().cpu().detach().numpy().flatten())\n",
        "    priorities = priorities_a / (priorities_a + priorities_h)\n",
        "    self.memory.update_priority(info['index'], priorities)\n",
        "\n",
        "    # backpropogating the error to update the weights\n",
        "    self.a_optimizer.zero_grad()\n",
        "    self.h_optimizer.zero_grad()\n",
        "\n",
        "    loss_a.backward()\n",
        "    loss_h.backward()\n",
        "\n",
        "    # max_grad_norm = 0.5\n",
        "    # clip_grad_norm_(self.a_local_qnetwork.parameters(), max_grad_norm)\n",
        "    # clip_grad_norm_(self.h_local_qnetwork.parameters(), max_grad_norm)\n",
        "\n",
        "    # single optimization step for updating the weights\n",
        "    self.a_optimizer.step()\n",
        "    self.h_optimizer.step()\n",
        "\n",
        "    # updating the target network weights\n",
        "    self.soft_update(self.a_local_qnetwork, self.a_target_qnetwork, interpolation_parameter)\n",
        "    self.soft_update(self.h_local_qnetwork, self.h_target_qnetwork, interpolation_parameter)\n",
        "\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    for target_param, local_param in zip(target_model.parameters(),local_model.parameters()):\n",
        "      # softly update the target model parameters with the weighted average of the local and target params\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
      ],
      "metadata": {
        "id": "5DKT_S_rHsBb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing DQN class\n",
        "\n"
      ],
      "metadata": {
        "id": "cdztmUgfdbmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(state_size, action_size)"
      ],
      "metadata": {
        "id": "9Y-aVh2VqHgD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = 0.2\n",
        "stale = 0\n",
        "gamma = 0.5\n",
        "cost = 0\n",
        "# setup up the environment\n",
        "env = MiningEnv(p=p, stale=stale, gamma = gamma, cost = cost, cutoff=20)"
      ],
      "metadata": {
        "id": "aQVwhmUgYnRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c376c01-eb56-41c0-adb7-aad15a9ba04e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scores_episodes = deque(maxlen=1000)\n",
        "# state = env.reset()\n",
        "# a_score = 0\n",
        "# h_score = 0\n",
        "# tau = 1\n",
        "# decay_rate = 0.9999\n",
        "# min_tau = 1e-4\n",
        "# max_tau = 1.0\n",
        "# performance_window = 5\n",
        "# performance_scores = []\n",
        "# last_avg_reward = 0\n",
        "# improvement_threshold=0.0001\n",
        "# temp = True\n",
        "# evaluation_interval = 5000  # Steps between 9each parameter adjustment\n",
        "\n",
        "# for episode in range(1, number_episodes + 1):\n",
        "#   action = agent.act(state, tau)\n",
        "#   next_state, reward_a, reward_h = env.step(action)\n",
        "#   agent.step(state, action, reward_a, reward_h, next_state)\n",
        "#   state = next_state\n",
        "#   a_score += reward_a\n",
        "#   h_score += reward_h\n",
        "#   if episode % 1000 == 0:\n",
        "#     print(a_score, h_score)\n",
        "#     rel = a_score/(a_score+h_score)\n",
        "#     scores_episodes.append(rel)\n",
        "#     performance_scores.append(rel)\n",
        "#     if len(performance_scores) > performance_window:\n",
        "#       performance_scores.pop(0)\n",
        "#     print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(episode,rel))\n",
        "#     print('temperature: ',tau)\n",
        "#     print('decay: ', decay_rate)\n",
        "#   if episode % evaluation_interval == 0:\n",
        "#     if len(performance_scores) == performance_window:\n",
        "#       avg_reward = np.mean(performance_scores)\n",
        "#       if not temp:\n",
        "#         improvement = avg_reward - last_avg_reward\n",
        "#         if improvement < improvement_threshold:\n",
        "#           # 0.0001005\n",
        "#           tau = min(max_tau, tau / decay_rate)\n",
        "#           # 0.99\n",
        "#           decay_rate = max(0.99, decay_rate * 0.95)\n",
        "#         else:\n",
        "#           decay_rate = min(0.999, decay_rate * 1.05)\n",
        "\n",
        "#       last_avg_reward = avg_reward\n",
        "#       temp = False\n",
        "\n",
        "#     # decay the temperature\n",
        "#   # 0.000994\n",
        "#   #\n",
        "#   tau = max(min_tau, tau * decay_rate)\n",
        "#   tau = min(max_tau, tau)\n",
        "# np.save(\"rl_revenues_%.2fhashrate%.2fgamma%.2fstale.npy\" % (p, gamma, stale), np.array(scores_episodes))\n"
      ],
      "metadata": {
        "id": "t66n8I7NXk3i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_episodes = deque(maxlen=400)\n",
        "state = env.reset()\n",
        "a_score = 0\n",
        "h_score = 0\n",
        "temperature = 1\n",
        "temperature_decay = 0.9998  # Temperature decay factor\n",
        "# try 0.9998, 0.9999\n",
        "min_temperature = 1e-4\n",
        "number_episodes = 1000000\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "\n",
        "  action = agent.act(state, temperature)\n",
        "  next_state, reward_a, reward_h = env.step(action)\n",
        "  agent.step(state, action, reward_a, reward_h, next_state)\n",
        "  state = next_state\n",
        "  a_score += reward_a\n",
        "  h_score += reward_h\n",
        "  if episode % 1000 == 0:\n",
        "    print(a_score, h_score)\n",
        "    rel = a_score/(a_score+h_score)\n",
        "    scores_episodes.append(rel)\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(episode,rel))\n",
        "\n",
        "  temperature = max(min_temperature, temperature * temperature_decay)\n",
        "np.save(\"dqn_revenues_no_clip.npy\", np.array(scores_episodes))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wWVT5L386dyy",
        "outputId": "3a6ef866-9ca7-406d-80ae-0ec76e996d62"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63.0 777.0\n",
            "Episode 1000\tAverage Score: 0.0750\n",
            "142.0 1562.0\n",
            "Episode 2000\tAverage Score: 0.0833\n",
            "206.0 2359.0\n",
            "Episode 3000\tAverage Score: 0.0803\n",
            "284.0 3130.0\n",
            "Episode 4000\tAverage Score: 0.0832\n",
            "361.0 3902.0\n",
            "Episode 5000\tAverage Score: 0.0847\n",
            "459.0 4656.0\n",
            "Episode 6000\tAverage Score: 0.0897\n",
            "532.0 5443.0\n",
            "Episode 7000\tAverage Score: 0.0890\n",
            "610.0 6242.0\n",
            "Episode 8000\tAverage Score: 0.0890\n",
            "694.0 7015.0\n",
            "Episode 9000\tAverage Score: 0.0900\n",
            "789.0 7789.0\n",
            "Episode 10000\tAverage Score: 0.0920\n",
            "884.0 8575.0\n",
            "Episode 11000\tAverage Score: 0.0935\n",
            "986.0 9346.0\n",
            "Episode 12000\tAverage Score: 0.0954\n",
            "1103.0 10105.0\n",
            "Episode 13000\tAverage Score: 0.0984\n",
            "1192.0 10911.0\n",
            "Episode 14000\tAverage Score: 0.0985\n",
            "1307.0 11679.0\n",
            "Episode 15000\tAverage Score: 0.1006\n",
            "1417.0 12456.0\n",
            "Episode 16000\tAverage Score: 0.1021\n",
            "1553.0 13194.0\n",
            "Episode 17000\tAverage Score: 0.1053\n",
            "1691.0 13932.0\n",
            "Episode 18000\tAverage Score: 0.1082\n",
            "1834.0 14670.0\n",
            "Episode 19000\tAverage Score: 0.1111\n",
            "1982.0 15404.0\n",
            "Episode 20000\tAverage Score: 0.1140\n",
            "2106.0 16173.0\n",
            "Episode 21000\tAverage Score: 0.1152\n",
            "2234.0 16949.0\n",
            "Episode 22000\tAverage Score: 0.1165\n",
            "2388.0 17695.0\n",
            "Episode 23000\tAverage Score: 0.1189\n",
            "2539.0 18429.0\n",
            "Episode 24000\tAverage Score: 0.1211\n",
            "2694.0 19200.0\n",
            "Episode 25000\tAverage Score: 0.1230\n",
            "2854.0 19959.0\n",
            "Episode 26000\tAverage Score: 0.1251\n",
            "3025.0 20705.0\n",
            "Episode 27000\tAverage Score: 0.1275\n",
            "3185.0 21457.0\n",
            "Episode 28000\tAverage Score: 0.1293\n",
            "3373.0 22232.0\n",
            "Episode 29000\tAverage Score: 0.1317\n",
            "3552.0 22999.0\n",
            "Episode 30000\tAverage Score: 0.1338\n",
            "3738.0 23767.0\n",
            "Episode 31000\tAverage Score: 0.1359\n",
            "3891.0 24566.0\n",
            "Episode 32000\tAverage Score: 0.1367\n",
            "4095.0 25336.0\n",
            "Episode 33000\tAverage Score: 0.1391\n",
            "4311.0 26101.0\n",
            "Episode 34000\tAverage Score: 0.1418\n",
            "4507.0 26879.0\n",
            "Episode 35000\tAverage Score: 0.1436\n",
            "4714.0 27669.0\n",
            "Episode 36000\tAverage Score: 0.1456\n",
            "4888.0 28493.0\n",
            "Episode 37000\tAverage Score: 0.1464\n",
            "5108.0 29265.0\n",
            "Episode 38000\tAverage Score: 0.1486\n",
            "5301.0 30070.0\n",
            "Episode 39000\tAverage Score: 0.1499\n",
            "5484.0 30867.0\n",
            "Episode 40000\tAverage Score: 0.1509\n",
            "5675.0 31620.0\n",
            "Episode 41000\tAverage Score: 0.1522\n",
            "5876.0 32404.0\n",
            "Episode 42000\tAverage Score: 0.1535\n",
            "6089.0 33182.0\n",
            "Episode 43000\tAverage Score: 0.1551\n",
            "6292.0 33955.0\n",
            "Episode 44000\tAverage Score: 0.1563\n",
            "6477.0 34763.0\n",
            "Episode 45000\tAverage Score: 0.1571\n",
            "6683.0 35557.0\n",
            "Episode 46000\tAverage Score: 0.1582\n",
            "6879.0 36321.0\n",
            "Episode 47000\tAverage Score: 0.1592\n",
            "7084.0 37116.0\n",
            "Episode 48000\tAverage Score: 0.1603\n",
            "7295.0 37897.0\n",
            "Episode 49000\tAverage Score: 0.1614\n",
            "7469.0 38723.0\n",
            "Episode 50000\tAverage Score: 0.1617\n",
            "7657.0 39443.0\n",
            "Episode 51000\tAverage Score: 0.1626\n",
            "7856.0 40231.0\n",
            "Episode 52000\tAverage Score: 0.1634\n",
            "8036.0 41006.0\n",
            "Episode 53000\tAverage Score: 0.1639\n",
            "8208.0 41790.0\n",
            "Episode 54000\tAverage Score: 0.1642\n",
            "8382.0 42583.0\n",
            "Episode 55000\tAverage Score: 0.1645\n",
            "8583.0 43362.0\n",
            "Episode 56000\tAverage Score: 0.1652\n",
            "8775.0 44118.0\n",
            "Episode 57000\tAverage Score: 0.1659\n",
            "8984.0 44894.0\n",
            "Episode 58000\tAverage Score: 0.1667\n",
            "9201.0 45670.0\n",
            "Episode 59000\tAverage Score: 0.1677\n",
            "9410.0 46447.0\n",
            "Episode 60000\tAverage Score: 0.1685\n",
            "9600.0 47219.0\n",
            "Episode 61000\tAverage Score: 0.1690\n",
            "9791.0 48002.0\n",
            "Episode 62000\tAverage Score: 0.1694\n",
            "9965.0 48786.0\n",
            "Episode 63000\tAverage Score: 0.1696\n",
            "10157.0 49586.0\n",
            "Episode 64000\tAverage Score: 0.1700\n",
            "10370.0 50291.0\n",
            "Episode 65000\tAverage Score: 0.1710\n",
            "10556.0 51097.0\n",
            "Episode 66000\tAverage Score: 0.1712\n",
            "10742.0 51890.0\n",
            "Episode 67000\tAverage Score: 0.1715\n",
            "10953.0 52679.0\n",
            "Episode 68000\tAverage Score: 0.1721\n",
            "11156.0 53476.0\n",
            "Episode 69000\tAverage Score: 0.1726\n",
            "11328.0 54297.0\n",
            "Episode 70000\tAverage Score: 0.1726\n",
            "11519.0 55066.0\n",
            "Episode 71000\tAverage Score: 0.1730\n",
            "11699.0 55859.0\n",
            "Episode 72000\tAverage Score: 0.1732\n",
            "11870.0 56642.0\n",
            "Episode 73000\tAverage Score: 0.1733\n",
            "12063.0 57449.0\n",
            "Episode 74000\tAverage Score: 0.1735\n",
            "12247.0 58247.0\n",
            "Episode 75000\tAverage Score: 0.1737\n",
            "12470.0 59023.0\n",
            "Episode 76000\tAverage Score: 0.1744\n",
            "12678.0 59775.0\n",
            "Episode 77000\tAverage Score: 0.1750\n",
            "12859.0 60539.0\n",
            "Episode 78000\tAverage Score: 0.1752\n",
            "13019.0 61336.0\n",
            "Episode 79000\tAverage Score: 0.1751\n",
            "13208.0 62141.0\n",
            "Episode 80000\tAverage Score: 0.1753\n",
            "13402.0 62874.0\n",
            "Episode 81000\tAverage Score: 0.1757\n",
            "13580.0 63672.0\n",
            "Episode 82000\tAverage Score: 0.1758\n",
            "13782.0 64442.0\n",
            "Episode 83000\tAverage Score: 0.1762\n",
            "13976.0 65209.0\n",
            "Episode 84000\tAverage Score: 0.1765\n",
            "14200.0 65933.0\n",
            "Episode 85000\tAverage Score: 0.1772\n",
            "14382.0 66701.0\n",
            "Episode 86000\tAverage Score: 0.1774\n",
            "14574.0 67483.0\n",
            "Episode 87000\tAverage Score: 0.1776\n",
            "14770.0 68268.0\n",
            "Episode 88000\tAverage Score: 0.1779\n",
            "14961.0 69005.0\n",
            "Episode 89000\tAverage Score: 0.1782\n",
            "15132.0 69773.0\n",
            "Episode 90000\tAverage Score: 0.1782\n",
            "15323.0 70536.0\n",
            "Episode 91000\tAverage Score: 0.1785\n",
            "15518.0 71294.0\n",
            "Episode 92000\tAverage Score: 0.1788\n",
            "15692.0 72095.0\n",
            "Episode 93000\tAverage Score: 0.1788\n",
            "15858.0 72904.0\n",
            "Episode 94000\tAverage Score: 0.1787\n",
            "16065.0 73664.0\n",
            "Episode 95000\tAverage Score: 0.1790\n",
            "16238.0 74480.0\n",
            "Episode 96000\tAverage Score: 0.1790\n",
            "16443.0 75275.0\n",
            "Episode 97000\tAverage Score: 0.1793\n",
            "16658.0 76060.0\n",
            "Episode 98000\tAverage Score: 0.1797\n",
            "16829.0 76832.0\n",
            "Episode 99000\tAverage Score: 0.1797\n",
            "17030.0 77578.0\n",
            "Episode 100000\tAverage Score: 0.1800\n",
            "17236.0 78372.0\n",
            "Episode 101000\tAverage Score: 0.1803\n",
            "17435.0 79155.0\n",
            "Episode 102000\tAverage Score: 0.1805\n",
            "17585.0 79929.0\n",
            "Episode 103000\tAverage Score: 0.1803\n",
            "17754.0 80697.0\n",
            "Episode 104000\tAverage Score: 0.1803\n",
            "17944.0 81487.0\n",
            "Episode 105000\tAverage Score: 0.1805\n",
            "18115.0 82299.0\n",
            "Episode 106000\tAverage Score: 0.1804\n",
            "18275.0 83098.0\n",
            "Episode 107000\tAverage Score: 0.1803\n",
            "18473.0 83846.0\n",
            "Episode 108000\tAverage Score: 0.1805\n",
            "18653.0 84600.0\n",
            "Episode 109000\tAverage Score: 0.1807\n",
            "18849.0 85379.0\n",
            "Episode 110000\tAverage Score: 0.1808\n",
            "19024.0 86186.0\n",
            "Episode 111000\tAverage Score: 0.1808\n",
            "19203.0 86978.0\n",
            "Episode 112000\tAverage Score: 0.1809\n",
            "19392.0 87779.0\n",
            "Episode 113000\tAverage Score: 0.1809\n",
            "19592.0 88552.0\n",
            "Episode 114000\tAverage Score: 0.1812\n",
            "19753.0 89344.0\n",
            "Episode 115000\tAverage Score: 0.1811\n",
            "19912.0 90152.0\n",
            "Episode 116000\tAverage Score: 0.1809\n",
            "20107.0 90884.0\n",
            "Episode 117000\tAverage Score: 0.1812\n",
            "20326.0 91621.0\n",
            "Episode 118000\tAverage Score: 0.1816\n",
            "20545.0 92360.0\n",
            "Episode 119000\tAverage Score: 0.1820\n",
            "20686.0 93141.0\n",
            "Episode 120000\tAverage Score: 0.1817\n",
            "20878.0 93910.0\n",
            "Episode 121000\tAverage Score: 0.1819\n",
            "21070.0 94698.0\n",
            "Episode 122000\tAverage Score: 0.1820\n",
            "21262.0 95506.0\n",
            "Episode 123000\tAverage Score: 0.1821\n",
            "21439.0 96302.0\n",
            "Episode 124000\tAverage Score: 0.1821\n",
            "21623.0 97112.0\n",
            "Episode 125000\tAverage Score: 0.1821\n",
            "21820.0 97895.0\n",
            "Episode 126000\tAverage Score: 0.1823\n",
            "22006.0 98675.0\n",
            "Episode 127000\tAverage Score: 0.1823\n",
            "22199.0 99461.0\n",
            "Episode 128000\tAverage Score: 0.1825\n",
            "22383.0 100237.0\n",
            "Episode 129000\tAverage Score: 0.1825\n",
            "22581.0 101020.0\n",
            "Episode 130000\tAverage Score: 0.1827\n",
            "22774.0 101775.0\n",
            "Episode 131000\tAverage Score: 0.1829\n",
            "22953.0 102490.0\n",
            "Episode 132000\tAverage Score: 0.1830\n",
            "23118.0 103235.0\n",
            "Episode 133000\tAverage Score: 0.1830\n",
            "23300.0 104014.0\n",
            "Episode 134000\tAverage Score: 0.1830\n",
            "23466.0 104817.0\n",
            "Episode 135000\tAverage Score: 0.1829\n",
            "23647.0 105601.0\n",
            "Episode 136000\tAverage Score: 0.1830\n",
            "23858.0 106370.0\n",
            "Episode 137000\tAverage Score: 0.1832\n",
            "24057.0 107141.0\n",
            "Episode 138000\tAverage Score: 0.1834\n",
            "24229.0 107936.0\n",
            "Episode 139000\tAverage Score: 0.1833\n",
            "24421.0 108729.0\n",
            "Episode 140000\tAverage Score: 0.1834\n",
            "24623.0 109522.0\n",
            "Episode 141000\tAverage Score: 0.1836\n",
            "24816.0 110324.0\n",
            "Episode 142000\tAverage Score: 0.1836\n",
            "25010.0 111109.0\n",
            "Episode 143000\tAverage Score: 0.1837\n",
            "25205.0 111912.0\n",
            "Episode 144000\tAverage Score: 0.1838\n",
            "25401.0 112693.0\n",
            "Episode 145000\tAverage Score: 0.1839\n",
            "25597.0 113497.0\n",
            "Episode 146000\tAverage Score: 0.1840\n",
            "25818.0 114276.0\n",
            "Episode 147000\tAverage Score: 0.1843\n",
            "25983.0 115060.0\n",
            "Episode 148000\tAverage Score: 0.1842\n",
            "26166.0 115844.0\n",
            "Episode 149000\tAverage Score: 0.1843\n",
            "26380.0 116614.0\n",
            "Episode 150000\tAverage Score: 0.1845\n",
            "26572.0 117381.0\n",
            "Episode 151000\tAverage Score: 0.1846\n",
            "26757.0 118185.0\n",
            "Episode 152000\tAverage Score: 0.1846\n",
            "26921.0 118993.0\n",
            "Episode 153000\tAverage Score: 0.1845\n",
            "27098.0 119768.0\n",
            "Episode 154000\tAverage Score: 0.1845\n",
            "27258.0 120576.0\n",
            "Episode 155000\tAverage Score: 0.1844\n",
            "27459.0 121362.0\n",
            "Episode 156000\tAverage Score: 0.1845\n",
            "27645.0 122120.0\n",
            "Episode 157000\tAverage Score: 0.1846\n",
            "27817.0 122922.0\n",
            "Episode 158000\tAverage Score: 0.1845\n",
            "28016.0 123703.0\n",
            "Episode 159000\tAverage Score: 0.1847\n",
            "28197.0 124446.0\n",
            "Episode 160000\tAverage Score: 0.1847\n",
            "28395.0 125246.0\n",
            "Episode 161000\tAverage Score: 0.1848\n",
            "28596.0 126026.0\n",
            "Episode 162000\tAverage Score: 0.1849\n",
            "28738.0 126796.0\n",
            "Episode 163000\tAverage Score: 0.1848\n",
            "28914.0 127597.0\n",
            "Episode 164000\tAverage Score: 0.1847\n",
            "29088.0 128367.0\n",
            "Episode 165000\tAverage Score: 0.1847\n",
            "29281.0 129150.0\n",
            "Episode 166000\tAverage Score: 0.1848\n",
            "29472.0 129903.0\n",
            "Episode 167000\tAverage Score: 0.1849\n",
            "29657.0 130679.0\n",
            "Episode 168000\tAverage Score: 0.1850\n",
            "29840.0 131481.0\n",
            "Episode 169000\tAverage Score: 0.1850\n",
            "30018.0 132276.0\n",
            "Episode 170000\tAverage Score: 0.1850\n",
            "30208.0 133085.0\n",
            "Episode 171000\tAverage Score: 0.1850\n",
            "30371.0 133879.0\n",
            "Episode 172000\tAverage Score: 0.1849\n",
            "30572.0 134674.0\n",
            "Episode 173000\tAverage Score: 0.1850\n",
            "30757.0 135476.0\n",
            "Episode 174000\tAverage Score: 0.1850\n",
            "30967.0 136252.0\n",
            "Episode 175000\tAverage Score: 0.1852\n",
            "31181.0 137012.0\n",
            "Episode 176000\tAverage Score: 0.1854\n",
            "31397.0 137795.0\n",
            "Episode 177000\tAverage Score: 0.1856\n",
            "31593.0 138595.0\n",
            "Episode 178000\tAverage Score: 0.1856\n",
            "31772.0 139412.0\n",
            "Episode 179000\tAverage Score: 0.1856\n",
            "31964.0 140205.0\n",
            "Episode 180000\tAverage Score: 0.1857\n",
            "32154.0 140997.0\n",
            "Episode 181000\tAverage Score: 0.1857\n",
            "32346.0 141762.0\n",
            "Episode 182000\tAverage Score: 0.1858\n",
            "32536.0 142512.0\n",
            "Episode 183000\tAverage Score: 0.1859\n",
            "32741.0 143287.0\n",
            "Episode 184000\tAverage Score: 0.1860\n",
            "32955.0 144065.0\n",
            "Episode 185000\tAverage Score: 0.1862\n",
            "33131.0 144851.0\n",
            "Episode 186000\tAverage Score: 0.1861\n",
            "33333.0 145646.0\n",
            "Episode 187000\tAverage Score: 0.1862\n",
            "33506.0 146447.0\n",
            "Episode 188000\tAverage Score: 0.1862\n",
            "33689.0 147236.0\n",
            "Episode 189000\tAverage Score: 0.1862\n",
            "33878.0 148043.0\n",
            "Episode 190000\tAverage Score: 0.1862\n",
            "34113.0 148807.0\n",
            "Episode 191000\tAverage Score: 0.1865\n",
            "34271.0 149580.0\n",
            "Episode 192000\tAverage Score: 0.1864\n",
            "34475.0 150337.0\n",
            "Episode 193000\tAverage Score: 0.1865\n",
            "34674.0 151115.0\n",
            "Episode 194000\tAverage Score: 0.1866\n",
            "34878.0 151885.0\n",
            "Episode 195000\tAverage Score: 0.1868\n",
            "35070.0 152673.0\n",
            "Episode 196000\tAverage Score: 0.1868\n",
            "35242.0 153463.0\n",
            "Episode 197000\tAverage Score: 0.1868\n",
            "35429.0 154240.0\n",
            "Episode 198000\tAverage Score: 0.1868\n",
            "35618.0 155011.0\n",
            "Episode 199000\tAverage Score: 0.1868\n",
            "35793.0 155807.0\n",
            "Episode 200000\tAverage Score: 0.1868\n",
            "35976.0 156595.0\n",
            "Episode 201000\tAverage Score: 0.1868\n",
            "36177.0 157379.0\n",
            "Episode 202000\tAverage Score: 0.1869\n",
            "36342.0 158147.0\n",
            "Episode 203000\tAverage Score: 0.1869\n",
            "36535.0 158946.0\n",
            "Episode 204000\tAverage Score: 0.1869\n",
            "36695.0 159723.0\n",
            "Episode 205000\tAverage Score: 0.1868\n",
            "36878.0 160509.0\n",
            "Episode 206000\tAverage Score: 0.1868\n",
            "37068.0 161314.0\n",
            "Episode 207000\tAverage Score: 0.1869\n",
            "37264.0 162101.0\n",
            "Episode 208000\tAverage Score: 0.1869\n",
            "37474.0 162891.0\n",
            "Episode 209000\tAverage Score: 0.1870\n",
            "37663.0 163686.0\n",
            "Episode 210000\tAverage Score: 0.1871\n",
            "37878.0 164456.0\n",
            "Episode 211000\tAverage Score: 0.1872\n",
            "38072.0 165241.0\n",
            "Episode 212000\tAverage Score: 0.1873\n",
            "38261.0 166004.0\n",
            "Episode 213000\tAverage Score: 0.1873\n",
            "38469.0 166781.0\n",
            "Episode 214000\tAverage Score: 0.1874\n",
            "38654.0 167557.0\n",
            "Episode 215000\tAverage Score: 0.1874\n",
            "38850.0 168334.0\n",
            "Episode 216000\tAverage Score: 0.1875\n",
            "39027.0 169125.0\n",
            "Episode 217000\tAverage Score: 0.1875\n",
            "39230.0 169897.0\n",
            "Episode 218000\tAverage Score: 0.1876\n",
            "39435.0 170692.0\n",
            "Episode 219000\tAverage Score: 0.1877\n",
            "39621.0 171504.0\n",
            "Episode 220000\tAverage Score: 0.1877\n",
            "39777.0 172301.0\n",
            "Episode 221000\tAverage Score: 0.1876\n",
            "39964.0 173083.0\n",
            "Episode 222000\tAverage Score: 0.1876\n",
            "40159.0 173870.0\n",
            "Episode 223000\tAverage Score: 0.1876\n",
            "40360.0 174645.0\n",
            "Episode 224000\tAverage Score: 0.1877\n",
            "40518.0 175403.0\n",
            "Episode 225000\tAverage Score: 0.1877\n",
            "40725.0 176143.0\n",
            "Episode 226000\tAverage Score: 0.1878\n",
            "40911.0 176921.0\n",
            "Episode 227000\tAverage Score: 0.1878\n",
            "41133.0 177676.0\n",
            "Episode 228000\tAverage Score: 0.1880\n",
            "41333.0 178464.0\n",
            "Episode 229000\tAverage Score: 0.1881\n",
            "41517.0 179240.0\n",
            "Episode 230000\tAverage Score: 0.1881\n",
            "41702.0 180020.0\n",
            "Episode 231000\tAverage Score: 0.1881\n",
            "41908.0 180786.0\n",
            "Episode 232000\tAverage Score: 0.1882\n",
            "42099.0 181549.0\n",
            "Episode 233000\tAverage Score: 0.1882\n",
            "42290.0 182339.0\n",
            "Episode 234000\tAverage Score: 0.1883\n",
            "42461.0 183131.0\n",
            "Episode 235000\tAverage Score: 0.1882\n",
            "42639.0 183909.0\n",
            "Episode 236000\tAverage Score: 0.1882\n",
            "42787.0 184725.0\n",
            "Episode 237000\tAverage Score: 0.1881\n",
            "42969.0 185514.0\n",
            "Episode 238000\tAverage Score: 0.1881\n",
            "43171.0 186298.0\n",
            "Episode 239000\tAverage Score: 0.1881\n",
            "43369.0 187067.0\n",
            "Episode 240000\tAverage Score: 0.1882\n",
            "43573.0 187823.0\n",
            "Episode 241000\tAverage Score: 0.1883\n",
            "43768.0 188610.0\n",
            "Episode 242000\tAverage Score: 0.1883\n",
            "43950.0 189416.0\n",
            "Episode 243000\tAverage Score: 0.1883\n",
            "44132.0 190217.0\n",
            "Episode 244000\tAverage Score: 0.1883\n",
            "44301.0 191000.0\n",
            "Episode 245000\tAverage Score: 0.1883\n",
            "44486.0 191773.0\n",
            "Episode 246000\tAverage Score: 0.1883\n",
            "44677.0 192566.0\n",
            "Episode 247000\tAverage Score: 0.1883\n",
            "44865.0 193362.0\n",
            "Episode 248000\tAverage Score: 0.1883\n",
            "45067.0 194128.0\n",
            "Episode 249000\tAverage Score: 0.1884\n",
            "45234.0 194928.0\n",
            "Episode 250000\tAverage Score: 0.1883\n",
            "45406.0 195711.0\n",
            "Episode 251000\tAverage Score: 0.1883\n",
            "45598.0 196498.0\n",
            "Episode 252000\tAverage Score: 0.1883\n",
            "45772.0 197286.0\n",
            "Episode 253000\tAverage Score: 0.1883\n",
            "45966.0 198034.0\n",
            "Episode 254000\tAverage Score: 0.1884\n",
            "46141.0 198827.0\n",
            "Episode 255000\tAverage Score: 0.1884\n",
            "46320.0 199606.0\n",
            "Episode 256000\tAverage Score: 0.1883\n",
            "46506.0 200380.0\n",
            "Episode 257000\tAverage Score: 0.1884\n",
            "46680.0 201166.0\n",
            "Episode 258000\tAverage Score: 0.1883\n",
            "46865.0 201981.0\n",
            "Episode 259000\tAverage Score: 0.1883\n",
            "47083.0 202737.0\n",
            "Episode 260000\tAverage Score: 0.1885\n",
            "47295.0 203514.0\n",
            "Episode 261000\tAverage Score: 0.1886\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9632c2e03dee>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0ma_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward_a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0cce043477fc>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward_a, reward_h, next_state)\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0cce043477fc>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, info, experiences, discount_factor)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# updating the target network weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_local_qnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_target_qnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation_parameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_local_qnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_target_qnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation_parameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0cce043477fc>\u001b[0m in \u001b[0;36msoft_update\u001b[0;34m(self, local_model, target_model, interpolation_parameter)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msoft_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation_parameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtarget_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_param\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlocal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m       \u001b[0;31m# softly update the target model parameters with the weighted average of the local and target params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m       \u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpolation_parameter\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlocal_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minterpolation_parameter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         \"\"\"\n\u001b[0;32m-> 2229\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2230\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2260\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2261\u001b[0m             prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)\n\u001b[0;32m-> 2262\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2201\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m                     \u001b[0mmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m         \u001b[0;31m# Do NOT handle __torch_function__ here as user's default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0;31m# implementation that handle most functions will most likely do it wrong.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tabular Q-Learning"
      ],
      "metadata": {
        "id": "PrHIv27oP2F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = 0.45\n",
        "stale = 0\n",
        "gamma = 1\n",
        "cost = 0\n",
        "# setup up the environment\n",
        "env = MiningEnv(p=p, stale=stale, gamma = gamma, cost = cost, cutoff=30)"
      ],
      "metadata": {
        "id": "Y2KLCrenRu1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearning():\n",
        "\n",
        "  def __init__(self, t_eps, learning_rate, discount_factor=0.999, n_iter=350000):\n",
        "    self.n_iter = int(n_iter)\n",
        "    self.S = env.S\n",
        "    self.A = env.A\n",
        "    self.learning_rate = learning_rate\n",
        "    self.discount_factor = discount_factor\n",
        "    self.t_eps = t_eps\n",
        "    self.Q_a = np.zeros((self.S, self.A))\n",
        "    self.Q_h = np.zeros((self.S, self.A))\n",
        "    self.V_s = np.zeros(self.S)\n",
        "\n",
        "\n",
        "  def run(self):\n",
        "    state = env.reset()\n",
        "    scores_episodes = deque(maxlen=350)\n",
        "    self.V_s[env.states_inverted[to_state_obj(state)]] += 1\n",
        "    a_score = 0\n",
        "    h_score = 0\n",
        "    for t in range(1, self.n_iter + 1):\n",
        "      eps = np.exp(-1 * self.V_s[env.states_inverted[to_state_obj(state)]] / self.t_eps)\n",
        "      pn = np.random.random()\n",
        "      a_t = self.Q_a[env.states_inverted[to_state_obj(state)]] / (self.Q_a[env.states_inverted[to_state_obj(state)]] + self.Q_h[env.states_inverted[to_state_obj(state)]])\n",
        "      a_t = np.nan_to_num(a_t, nan=0.0)\n",
        "      mask_value = float(\"-inf\")\n",
        "      diff = list(set([0, 1, 2, 3]) - set(get_allowed_actions(torch.tensor(state), 30)))\n",
        "      if diff:\n",
        "        a_t[diff] = mask_value\n",
        "      if pn < eps:\n",
        "        valid_actions_indices = np.where(a_t > float('-inf'))[0]\n",
        "        act = np.random.choice(valid_actions_indices)\n",
        "      else:\n",
        "        act = a_t.argmax()\n",
        "      next_state, reward_a, reward_h = env.step(act)\n",
        "      a_ = self.Q_a[env.states_inverted[to_state_obj(next_state)]] / (self.Q_a[env.states_inverted[to_state_obj(next_state)]] + self.Q_h[env.states_inverted[to_state_obj(next_state)]])\n",
        "      a_ = a_.argmax()\n",
        "      self.Q_a[env.states_inverted[to_state_obj(state)], act] = (1-self.learning_rate) * self.Q_a[env.states_inverted[to_state_obj(state)], act] + self.learning_rate * (reward_a + self.discount_factor * self.Q_a[env.states_inverted[to_state_obj(next_state)], a_])\n",
        "      self.Q_h[env.states_inverted[to_state_obj(state)], act] = (1-self.learning_rate) * self.Q_h[env.states_inverted[to_state_obj(state)], act] + self.learning_rate * (reward_h + self.discount_factor * self.Q_h[env.states_inverted[to_state_obj(next_state)], a_])\n",
        "      state = next_state\n",
        "      self.V_s[env.states_inverted[to_state_obj(state)]] += 1\n",
        "      a_score += reward_a\n",
        "      h_score += reward_h\n",
        "      if t % 1000 == 0:\n",
        "        print(a_score, h_score)\n",
        "        rel = a_score/(a_score+h_score)\n",
        "        scores_episodes.append(rel)\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(t,rel))\n",
        "    np.save(\"tabular_revenues %.5flr%dteps.npy\" % (self.learning_rate, self.t_eps), np.array(scores_episodes))\n",
        "\n"
      ],
      "metadata": {
        "id": "SrYblD5K4f8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = QLearning(t_eps=1000, learning_rate=5e-5)"
      ],
      "metadata": {
        "id": "C3NTtkHFX6-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run()"
      ],
      "metadata": {
        "id": "F10lnCeZZaIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tab_revenues = np.load('tab_revenues.npy')\n",
        "rl_revenues = np.load('dqn_revenues_no_clip.npy')\n",
        "plt.plot(tab_revenues[:len(rl_revenues)])\n",
        "plt.plot(rl_revenues)"
      ],
      "metadata": {
        "id": "2SP4clsqG_1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [5e-2, 5e-3, 5e-5]\n",
        "t_eps = [10, 100, 1000]\n",
        "dqn_revenues = np.load('dqn_revenues.npy')\n",
        "plt.plot(timesteps, dqn_revenues[:-1], label='DRL Mining')\n",
        "\n",
        "timesteps = np.arange(1, len(dqn_revenues[:-1]) + 1)\n",
        "\n",
        "\n",
        "for lr in learning_rates:\n",
        "  for t in t_eps:\n",
        "    f_name = 'tabular_revenues {:.5f}lr{}teps.npy'.format(lr, t)\n",
        "    try:\n",
        "      revenue = np.load(f_name)\n",
        "      plt.plot(timesteps, revenue, label=f'Original RL (lr={lr}, t_eps={t})')\n",
        "    except:\n",
        "      pass\n",
        "plt.legend(loc=0)\n",
        "plt.xlabel('Timesteps')\n",
        "plt.ylabel('Relative Gain')\n",
        "plt.title(r'Relative Mining Gain $\\alpha$=0.45, $\\gamma$=1, $c$=0')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZaLRjmePb00g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, initial_tau=1.0, min_tau=1e-4, max_tau=2.0, initial_decay_rate=0.999, performance_window=1000, improvement_threshold=0.005):\n",
        "        self.tau = initial_tau\n",
        "        self.min_tau = min_tau\n",
        "        self.max_tau = max_tau  # Maximum temperature for exploration\n",
        "        self.decay_rate = initial_decay_rate\n",
        "        self.performance_window = performance_window\n",
        "        self.improvement_threshold = improvement_threshold\n",
        "        self.rewards = []\n",
        "        self.last_avg_reward = None\n",
        "\n",
        "    def select_action(self, q_values):\n",
        "        exp_q = np.exp(q_values / self.tau)\n",
        "        probabilities = exp_q / np.sum(exp_q)\n",
        "        action = np.random.choice(len(q_values), p=probabilities)\n",
        "        return action\n",
        "\n",
        "    def update_tau(self):\n",
        "        # Ensure tau does not decay too aggressively\n",
        "        self.tau = max(self.min_tau, min(self.max_tau, self.tau * self.decay_rate))\n",
        "\n",
        "    def update_performance(self, reward):\n",
        "        self.rewards.append(reward)\n",
        "        if len(self.rewards) > self.performance_window:\n",
        "            self.rewards.pop(0)\n",
        "        if len(self.rewards) == self.performance_window:\n",
        "            avg_reward = np.mean(self.rewards)\n",
        "            if self.last_avg_reward is not None:\n",
        "                improvement = avg_reward - self.last_avg_reward\n",
        "                if improvement < self.improvement_threshold:\n",
        "                    # Increase tau directly to promote exploration\n",
        "                    self.tau = min(self.max_tau, self.tau * 1.05)  # Increase tau to promote exploration\n",
        "                    # Decrease decay rate to slow down temperature decay\n",
        "                    self.decay_rate = max(0.99, self.decay_rate * 0.95)\n",
        "                else:\n",
        "                    # Decrease tau to promote exploitation\n",
        "                    self.tau = max(self.min_tau, self.tau * 0.95)  # Decrease tau to promote exploitation\n",
        "                    # Increase decay rate to speed up temperature decay\n",
        "                    self.decay_rate = min(0.9995, self.decay_rate * 1.05)\n",
        "            self.last_avg_reward = avg_reward\n",
        "\n",
        "# Simulation of temperature decay with performance evaluation every 1000 steps\n",
        "num_steps = 300000\n",
        "agent = Agent(initial_tau=1.0, min_tau=1e-4, max_tau=2.0, initial_decay_rate=0.999, performance_window=1000, improvement_threshold=0.005)\n",
        "\n",
        "# Assuming there is no improvement in performance, we can simulate constant rewards\n",
        "constant_reward = 1.0\n",
        "\n",
        "for step in range(num_steps):\n",
        "    # Simulate constant reward without improvement\n",
        "    agent.update_performance(constant_reward)\n",
        "\n",
        "    if (step + 1) % 1000 == 0:\n",
        "        agent.update_tau()\n",
        "        # Print or log tau occasionally to monitor decay\n",
        "        print(f'Step {step + 1}, Tau: {agent.tau:.6f}, Decay Rate: {agent.decay_rate:.6f}')\n",
        "\n",
        "# Final values after simulation\n",
        "print(f'Final Tau: {agent.tau:.6f}')\n",
        "print(f'Final Decay Rate: {agent.decay_rate:.6f}')\n"
      ],
      "metadata": {
        "id": "1u24aSf4CWZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}